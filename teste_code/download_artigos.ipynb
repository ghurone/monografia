{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## classe Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Request:\n",
    "    def __init__(self, n_retry: int = 10, timeout: float = 5):\n",
    "        self.retries = Retry(total=n_retry, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "        self.adapter = HTTPAdapter(max_retries=self.retries)\n",
    "        self.http = requests.Session()\n",
    "        self.http.mount(\"https://\", self.adapter)\n",
    "\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def get(self, url: str, params: dict = None) -> str:\n",
    "        if params is None:\n",
    "            params = {}\n",
    "\n",
    "        resp = self.http.get(url, params=params, timeout=self.timeout, headers={'accept-language':'pt-BR,pt;q=0.5'})\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_request = Request()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SciELO Artigos\n",
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scielo_url = lambda x: urljoin('https://www.scielo.br', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def volumes_por_ano() -> dict[str:list]:\n",
    "    req = my_request.get(scielo_url('j/rbef/grid'))\n",
    "    soup = BeautifulSoup(req.text, 'lxml', parse_only=SoupStrainer('tbody'))\n",
    "\n",
    "    links = {linha.td.text.strip(): [scielo_url(a['href']) for a in linha.find_all('a')] for linha in soup.find_all('tr')}\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def artigos_por_volume(url: str, formato: str = 'html', idioma: str = 'pt') -> list:\n",
    "    languages = {'pt': 'Português', 'en': 'Inglês', 'es': 'Espanhol'}\n",
    "    formatos = {'html': '/?lang=', 'pdf': '/?format=pdf&'}\n",
    "\n",
    "    lang = languages.get(idioma.lower())\n",
    "    form = formatos.get(formato.lower())\n",
    "\n",
    "    req = my_request.get(url)\n",
    "\n",
    "    strainer = SoupStrainer('a', attrs={'title': lang})\n",
    "    soup = BeautifulSoup(req.text, 'lxml', parse_only=strainer)\n",
    "\n",
    "    links = []\n",
    "    for a in soup.findAll('a'):\n",
    "        if 'abstract' not in a['href'] and form in a['href']:\n",
    "            links.append(scielo_url(a['href']))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def html_do_artigo(text: str) -> str:\n",
    "    strainer = SoupStrainer('div', attrs={'class': 'articleTxt'})\n",
    "    return BeautifulSoup(text, 'lxml', parse_only=strainer).prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Código\n",
    "Salva os artigos da scielo na pasta `artigos_brutos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta existente!\n",
      "Ano: 2001 | Volume: 4/4 | Artigo: 012/012 | Total: 1680\r"
     ]
    }
   ],
   "source": [
    "volumes = volumes_por_ano()\n",
    "del volumes['2023']  # remove 2023\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'artigos_brutos')) # cria a pasta\n",
    "except FileExistsError:\n",
    "    print('Pasta existente!')\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'artigos_brutos')\n",
    "\n",
    "n = 0  # Numeração dos artigos\n",
    "for ano, urls in volumes.items():\n",
    "    for i, link in enumerate(urls, start=1):  # itera nas urls dos volumes\n",
    "        all_ = artigos_por_volume(link)\n",
    "\n",
    "        for j, artigo in enumerate(all_, start=1):  # itera nas urls dos artigos contidos no volume\n",
    "            nome_arquivo = str(n).rjust(4, \"0\") + '.html'  # Nome do arquivo em função do n. ex: 0123.html\n",
    "            path_save = os.path.join(path, nome_arquivo)  # local onde salvar os arquivos\n",
    "\n",
    "            # print do progresso\n",
    "            print(f'Ano: {ano} | Volume: {i}/{len(urls)} | Artigo: {str(j).rjust(3, \"0\")}/{str(len(all_)).rjust(3, \"0\")} | Total: {str(n).rjust(4, \"0\")}', end='\\r')\n",
    "\n",
    "            # requesição da url do artigo\n",
    "            reque = my_request.get(artigo)\n",
    "\n",
    "            # salva o html artigo completo num arquivo\n",
    "            with open(path_save, 'w', encoding='utf8') as file:\n",
    "                file.write(html_do_artigo(reque.text))\n",
    "\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "804cd49afbe1d7becab313e2a1b0e2c2807237e5a7ce09f80bcc9c57badf60a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
