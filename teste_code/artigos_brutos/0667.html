<!DOCTYPE html>
<div class="articleTxt">
 <div class="articleBadge-editionMeta-doi-copyLink">
  <span class="_articleBadge">
   Artigos Gerais
  </span>
  <span class="_separator">
   ‚Ä¢
  </span>
  <span class="_editionMeta">
   Rev. Bras. Ensino F√≠s. 38 
                (1)
   <span class="_separator">
    ‚Ä¢
   </span>
   Mar¬†2016
  </span>
  <span class="_separator">
   ‚Ä¢
  </span>
  <span class="group-doi">
   <a class="_doi" href="https://doi.org/10.1590/S1806-11173812125" target="_blank">
    https://doi.org/10.1590/S1806-11173812125
   </a>
   <a class="copyLink" data-clipboard-text="https://doi.org/10.1590/S1806-11173812125">
    <span class="sci-ico-link">
    </span>
    copiar
   </a>
  </span>
 </div>
 <h1 class="article-title">
  <span class="sci-ico-openAccess showTooltip" data-original-title="by 4.0" data-toggle="tooltip">
  </span>
  Influ√™ncia da lei de Zipf na escolha de senhas
  <a class="short-link" href="#" id="shorten">
   <span class="sci-ico-link">
   </span>
  </a>
 </h1>
 <h2 class="article-title">
  Influence of Zipf's law on the password choices
 </h2>
 <div class="articleMeta">
 </div>
 <div class="contribGroup">
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor1">
    <span>
     Leonardo Carneiro de Ara√∫jo
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor1" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade Federal de S√£o Jo√£o del Rei, Campus Alto Paraopeba, S√£o Jo√£o del Rei, MG, Brasil
    <div class="corresp">
     * Endere√ßo de correspond√™ncia:
     <a href="mailto:leolca@ufsj.edu.br">
      leolca@ufsj.edu.br
     </a>
     .
    </div>
   </ul>
  </span>
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor2">
    <span>
     Jo√£o Pedro Hallack Sans√£o
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor2" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade Federal de S√£o Jo√£o del Rei, Campus Alto Paraopeba, S√£o Jo√£o del Rei, MG, Brasil
   </ul>
  </span>
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor3">
    <span>
     Hani Camille Yehia
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor3" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Departamento de Engenharia Eletr√¥nica, Universidade Federal de Minas Gerias, Belo Horizonte, MG, Brasil
   </ul>
  </span>
  <a class="outlineFadeLink" data-target="#ModalTutors" data-toggle="modal" href="">
   Sobre os autores
  </a>
 </div>
 <div class="row">
  <ul class="col-md-2 hidden-sm articleMenu">
  </ul>
  <article class="col-md-10 col-md-offset-2 col-sm-12 col-sm-offset-0" id="articleText">
   <div class="articleSection" data-anchor="Resumos">
    <h1 class="articleSectionTitle">
     Resumos
    </h1>
   </div>
   <div>
    <p>
     Este artigo apresenta uma an√°lise sob a √≥tica de teoria da informa√ß√£o de algumas maneiras de criar senhas para dificultar um ataque por for√ßa bruta. A lei de Zipf √© observada nas l√≠nguas naturais e, por conseguinte, a entropia √© reduzida quando as utilizamos ao criar uma senha. Muitas das empresas utilizam a pol√≠tica de restringir o n√∫mero de caracteres de uma senha. Al√©m disso, queremos criar senhas que n√£o sejam demasiadas longas, nem que sejam dif√≠ceis de se memorizar. Mostraremos que, dado este cen√°rio, a melhor estrat√©gia (rela√ß√£o de compromisso entre criar uma senha forte e uma senha de f√°cil memoriza√ß√£o e utiliza√ß√£o) √© a utiliza√ß√£o de acr√¥nimos, senhas formadas pela combina√ß√£o (aparentemente sem nexo) da primeira letra de palavras em uma frase. Com esta abordagem podemos aumentar em aproximadamente 80% a entropia por caractere.
    </p>
    <p>
     <strong>
      Palavras-chave:
     </strong>
     <br/>
     senhas; lei de Zipf; seguran√ßa; entropia
    </p>
   </div>
   <hr/>
   <div>
    <p>
     Under the perspective of information theory, the present work performs an analysis of some methods used to create passwords in order to harden the defense against brute force attacks. Zipf's law is ubiquitous in natural languages and therefore it implies an entropy reduction when any language is used to create a password. Many companies impose a length restriction on passwords. Also, we do not want to create long passwords (that would take longer time to type), nor we want passwords that are hard to remember. On those terms, the best approach to create a password (the best tradeoff between creating a strong and an easy to memorize and use password) is the acronym approach, selecting the first character of each word in a sentence and combining them to form a gibberish string. Using this approach we are able to increase in 80% the entropy per character.
    </p>
    <p>
     <strong>
      Keywords:
     </strong>
     <br/>
     passwords; Zipf's law; security; entropy
    </p>
   </div>
   <hr/>
   <div class="articleSection" data-anchor="Text">
    <h1 class="articleSectionTitle">
     1. Introdu√ß√£o
    </h1>
    <p>
     A preocupa√ß√£o com seguran√ßa da informa√ß√£o armazenada ou transmitida √© antiga. Muitas vezes criptografia √© confundida com estenografia. Esta possui como objetivo esconder a mensagem que ser√° enviada ou armazenada. Aquela possui como objetivo garantir que o significado da mensagem n√£o seja desvendado, mesmo que a mensagem caia em m√£os erradas. Criptografia √© a ci√™ncia que preocupase com a comunica√ß√£o segura e usualmente secreta. O nome criptografia √© composto de duas palavras gregas: kryptos, significando 'escondido' e graphein, significando 'escrita'
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       1
      </sup>
      <span class="refCtt closed">
       <span>
        [1] 
					Dictionary.com Unabridged, Online Dictionary (2015). disponivel em http://dictionary.reference.com/browse/cryptography.
       </span>
       <br/>
       <a href="Dictionary.com" target="_blank">
        Dictionary.com...
       </a>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Criptografia √© uma forma de transmitir uma mensagem de forma confidencial que ser√° incompreens√≠vel para algu√©m a intercepte sem conhecer previamente o segredo para desencript√°-la. Desta forma, a criptografia era utilizada usualmente por militares, espi√µes e diplomatas, para transmiss√£o de mensagens secretas. Podemos dizer que a hist√≥ria da criptografia √© quase t√£o antiga quanto a palavra escrita. Algumas escrituras eg√≠pcias feitas h√° 4 mil√™nios apresentam um elemento fundamental utilizado pela criptografia: a substitui√ß√£o
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       2
      </sup>
      <span class="refCtt closed">
       <span>
        [2] D. Kahn, The Codebreakers: The Comprehensive History of Secret Communication from Ancient Times to the Internet (Macmillan, New York, 1967).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . As primeiras formas de criptografia eram bem rudimentares, como por exemplo a cifra de C√©sar, que consistia na mera substitui√ß√£o dos caracteres realizando um deslocamento fixo nas letras do alfabeto.
    </p>
    <p>
     Se de um lado temos agentes que desejam realizar uma comunica√ß√£o secreta, de outro, temos aqueles que desejam interceptar estas mensagens e decodific√°-las. Destarte, surge a criptoan√°lise, ci√™ncia que estuda os sistemas de informa√ß√£o em busca de descobrir os aspectos ocultos de um sistema criptogr√°fico, visando quebr√°-lo e obter assim acesso √† mensagem, mesmo quando a chave √© desconhecida. At√© o in√≠cio do S√©culo XX, a criptoan√°lise baseava-se exclusivamente na an√°lise de padr√µes lingu√≠sticos e lexicogr√°ficos. Posteriormente, matem√°tica, estat√≠stica, simula√ß√µes e c√°lculos computacionais tornaram-se importantes ferramentas na criptoan√°lise moderna. Os desenvolvimentos realizados em
     <i>
      Bletchley Park
     </i>
     durante a Segunda Guerra Mundial s√£o not√≥rios, em especial o desenvolvimento das m√°quinas computacionais
     <i>
      Bombes
     </i>
     e
     <i>
      Colossus
     </i>
     importantes para quebrar os segredos da m√°quina electro-mec√¢nica de criptografia alem√£, intitulada Enigma.
    </p>
    <p>
     Recentemente, a utiliza√ß√£o de criptografia entrou na pauta da m√≠dia jornal√≠stica ap√≥s as revela√ß√µes de Edward Snowden sobre a abrang√™ncia das informa√ß√µes coletadas pela Ag√™ncia Nacional de Seguran√ßa Americana (em ingl√™s:
     <i>
      National Security Agency
     </i>
     - NSA), configurando uma situa√ß√£o de invas√£o de privacidade de milh√µes de cidad√£os e levantando questionamentos sobre os reais motivos para esta coleta, aparentemente indiscriminada, de informa√ß√µes. Como uma resposta, algumas empresas de inform√°tica, como Apple, Facebook, Google e Microsoft, passaram a disponibilizar, de forma mais acess√≠vel, criptografia de dados no armazenamento e transmiss√£o. Estes sistemas adotados s√£o categorizados como sistema de criptografia civil e baseiam-se no princ√≠pio de Kerckhoffs (veja sec√ß√£o 3.2), ou seja, a seguran√ßa deve depender inteiramente da senha secreta e, desta forma, o tamanho do espa√ßo de chaves
     <span class="ref footnote">
      <sup class="xref">
       1
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         1
        </strong>
        Espa√ßo de chaves √© o conjunto formado por todas as poss√≠veis chaves. O tamanho deste conjunto √© determinado pelo comprimento m√°ximo das chaves e pelo tamanho do alfabeto utilizado. Se C √© o espa√ßo de chaves, ùìß √© o alfabeto e n o comprimento m√°ximo das chaves, ent√£o |C| = |ùìß|n.
       </span>
      </span>
     </span>
     √© importante para garantir entropia ou aleatoriedade suficiente para manter o sistema seguro. V√°rios estudos mostram que o usu√°rio √© a liga√ß√£o fraca, pois eles costumam escolher senhas muito simples
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       3
      </sup>
      <span class="refCtt closed">
       <span>
        [3] M.A. Carnut e E.C. Hora, in: Anais do Simp√≥sio de Seguran√ßa em Inform√°tica, editado por P.S.M. Pires e C.T. Fernandes, S√£o Jos√© dos Campos, 2005.
       </span>
       <br/>
       <br/>
       <span>
        [4] K.-W. Lee and H.-T. Ewe, in: International Conference on Computational Intelligence and Security, Guangzhou, edited by Y. Cheung, Y. Wang and H. Liu (The Institute of Electrical and Electronics Engineers, Piscataway, 2006).
       </span>
      </span>
     </span>
     <sup>
      -
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       5
      </sup>
      <span class="refCtt closed">
       <span>
        [5] B. Schneier, Applied Cryptography: Protocols, Algorithms, and Source Code in C, (John Wiley &amp; Sons, New York, 1996), 2nd ed.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Desta forma, √© extremamente importante que o usu√°rio saiba criar uma senha robusta o suficiente para dificultar o sucesso de ataques que buscam quebr√°-la.
    </p>
    <p>
     Uma forma de quebrar uma senha √© realizar uma busca exaustiva dentre todas as possibilidades.
    </p>
    <p>
    </p>
    <div class="row fig" id="f7">
     <a name="f7">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf7" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <br/>
     </div>
    </div>
    <p>
     Esta busca, ilustrada no Algoritmo 1, garantida mente encontrar√° a senha e, para tanto, levar√° um tempo de no m√°ximo
     <i>
      k
     </i>
     |
     <i>
      C
     </i>
     |, onde
     <i>
      k
     </i>
     √© o tempo necess√°rio para o algoritmo realizar um √∫nico teste para verificar se uma determinada senha
     <i>
      c
     </i>
     √© correta, |
     <i>
      C
     </i>
     | √© a cardinalidade do conjunto de escolhas. Podemos supor que o agente externo de um ataque n√£o possui maneiras para alterar
     <i>
      k
     </i>
     , pois
     <i>
      k
     </i>
     √© determinado pela arquitetura do sistema de criptografia, por√©m o usu√°rio, inconscientemente, pode ter restringido o conjunto
     <i>
      C
     </i>
     . Sabendo disso, o agente, ao realizar um ataque, pode realizar a busca em apenas um subconjunto de
     <i>
      C
     </i>
     , no qual, √© mais prov√°vel, embora n√£o garantido, encontrar a senha. Este tipo de busca √© conhecido como ataque baseado em dicion√°rios. Estamos aqui supondo que podemos testar se
     <i>
      c
     </i>
     √© a senha quantas vezes desejarmos, o que n√£o √© verdade para ataques
     <i>
      online
     </i>
     , uma vez que os administradores geralmente adotam pol√≠ticas para bloquear novas tentativas ou tornar
     <i>
      k
     </i>
     cada vez maior √† medida que tentativas s√£o realizadas. Se, de alguma forma, obtiv√©ssemos a base de dados com as senhas e a fun√ß√£o de encripta√ß√£o, seria poss√≠vel realizar um ataque
     <i>
      offline
     </i>
     e, desta forma, o limite imposto por
     <i>
      k
     </i>
     ser√° apenas o custo computacional de verificar uma senha.
    </p>
    <p>
     Em busca de evitar ataques baseados em dicion√°rios (muitos deles constitu√≠dos a partir de listas de senhas de servi√ßos virtuais que acabaram tornando-se p√∫blicas ap√≥s ataques maliciosos), os administradores de sistemas muitas vezes tentam impor certas limita√ß√µes √†s senhas, exigindo que elas satisfa√ßam algumas restri√ß√µes como, por exemplo, n√∫mero m√≠nimo de caracteres, presen√ßa de caracteres n√£o alfanum√©ricos, utiliza√ß√£o de algarismos num√©ricos, ou ainda exigindo que o usu√°rio mude sua senha com frequ√™ncia. Entretanto, usu√°rios buscam escolher senhas que sejam de f√°cil memoriza√ß√£o, tarefa esta que fica mais dif√≠cil √† medida que o comprimento da senha cresce e √† medida em que a senha torna-se mais imprevis√≠vel. Em entre vista ao programa do apresentador John Oliver
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       6
      </sup>
      <span class="refCtt closed">
       <span>
        [6] J. Oliver, Last Week Tonight with John Oliver: Government Surveillance (HBO, 2015), disponivel em https://youtu.be/XEVlyP4_11M.
       </span>
       <br/>
       <a href="https://youtu.be/XEVlyP4_11M" target="_blank">
        https://youtu.be/XEVlyP4_11M...
       </a>
      </span>
     </span>
     <sup>
      ]
     </sup>
     , Edward Snowden explica que ao inv√©s de utilizar combina√ß√µes de palavras, n√∫meros e anagramas, √© melhor mudar o paradigma e passar a utilizar passphrases, ou seja, criar uma frase para ser utilizada como senha. Nesta entrevista ele oferece como exemplo a
     <i>
      passphrase
     </i>
     'MargaretThatcheris110%SEXY', ou seja, uma sequ√™ncia de palavras constituindo uma frase. Por outro lado, n√£o queremos que a senha seja constitu√≠da de muitos caracteres, pois isto implicar√° em mais tempo gasto para digit√°-la sempre que for utilizada. Al√©m do mais, muitos servi√ßos imp√µem um limite m√°ximo sobre o n√∫mero de caracteres para uma senha.
    </p>
    <p>
     Neste artigo iremos analisar, sob o ponto de vista de teoria da informa√ß√£o, quatro diferentes abordagens para criar senhas:
    </p>
    <div>
     <ol type="1">
      <li>
       <p>
        utiliza√ß√£o de palavras;
       </p>
      </li>
      <li>
       <p>
        utiliza√ß√£o de
        <i>
         passphrases
        </i>
        ;
       </p>
      </li>
      <li>
       <p>
        m√©todo
        <i>
         Diceware
        </i>
        <sup>
         [
        </sup>
        <span class="ref">
         <sup class="xref xrefblue">
          7
         </sup>
         <span class="refCtt closed">
          <span>
           [7] A.G. Reinhold, The Diceware Passphrase Home Page (1995), disponivel em http://world.std.com/reinhold/diceware.html.
          </span>
          <br/>
          <a href="http://world.std.com/reinhold/diceware.html" target="_blank">
           http://world.std.com/reinhold/diceware.h...
          </a>
         </span>
        </span>
        <sup>
         ]
        </sup>
        ;
       </p>
      </li>
      <li>
       <p>
        utiliza√ß√£o de acr√¥nimos, sequ√™ncias de caracteres formadas pela primeira letra de cada palavra em uma frase.
       </p>
      </li>
     </ol>
    </div>
    <p>
     Argumentaremos que, quando imposta uma rela√ß√£o de compromisso entre seguran√ßa e facilidade de memoriza√ß√£o e uso, a melhor estrat√©gia ser√° utilizar acr√¥nimos, pois, dentre as estrat√©gias analisadas, √© aquela que fornece maior entropia por caractere.
    </p>
    <h1 class="articleSectionTitle">
     2. Utiliza√ß√£o de senhas
    </h1>
    <p>
     Para grande parte dos usu√°rios, e at√© mesmo especialistas na √°rea de seguran√ßa digital, a utiliza√ß√£o de senhas sempre foi um fardo e, por conseguinte, sua import√¢ncia muitas vezes √© dirimida, possibilitando assim a instaura√ß√£o de brechas de seguran√ßa. Buscando resolver este problema, novas tecnologias v√™em sendo desenvolvidas com a finalidade de sobre pujar as convencionais senhas
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       8
      </sup>
      <span class="refCtt closed">
       <span>
        [8] J. Bonneau, C. Herley, P.C. van Oorschot and F. Stajano, in: Proc. IEEE Symposium on Security and Privacy, San Francisco, 2012, editado por P. Kellenberger (Institute of Electrical and Electronics Engineers Inc., Piscataway, 2012), p. 553.
       </span>
       <br/>
       <br/>
       <span>
        [9] N. Lee, Yahoo mail drops passwords and adds thirdparty email support for new apps (2015), disponivel em http://www.engadget.com/2015/10/15/yahoo-mail-update/.
       </span>
       <br/>
       <a href="http://www.engadget.com/2015/10/15/yahoo-mail-update/" target="_blank">
        http://www.engadget.com/2015/10/15/yahoo...
       </a>
      </span>
     </span>
     <sup>
      -
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       10
      </sup>
      <span class="refCtt closed">
       <span>
        [10] E. Grosse and M. Upadhyay, IEEE Computer and Reliability Societies 11, 1 (2013).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Alguns exemplos s√£o as chaves eletr√¥nicas, m√©todos biom√©tricos, m√©todos de autentica√ß√£o por dois fatores, dentre outros. Entretanto, a utiliza√ß√£o de senhas convencionais ainda √© predominante, barata, simples, ub√≠qua e est√° sempre presente com o usu√°rio, n√£o necessi tando que este carregue algum outro objeto
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       8
      </sup>
      <span class="refCtt closed">
       <span>
        [8] J. Bonneau, C. Herley, P.C. van Oorschot and F. Stajano, in: Proc. IEEE Symposium on Security and Privacy, San Francisco, 2012, editado por P. Kellenberger (Institute of Electrical and Electronics Engineers Inc., Piscataway, 2012), p. 553.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     Enquanto as senhas ainda perduram em nosso cotidiano, algumas empresas desenvolveram gerenciadores de senhas, tais como KeyPass, LastPass, iCloud Keychain, dentre outros. Entretanto, estes gerenciadores armazenam uma base de dados com a rela√ß√£o de senhas e contas associadas. As senhas s√£o armazenadas em sua forma textual, e n√£o um
     <i>
      hash
     </i>
     <span class="ref footnote">
      <sup class="xref">
       2
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         2
        </strong>
        Uma fun√ß√£o hash √© uma fun√ß√£o determin√≠stica que realiza o mapeamento de um conjunto de v√°rios (ou at√© mesmo infinitos) membros em valores de um conjunto com um n√∫mero fixo de membros. Uma fun√ß√£o hash possui a propriedade de n√£o ser revers√≠vel, f√°cil de ser calculada e, al√©m disso, √© dif√≠cil modificar um membro do dom√≠nio sem gerar um valor de hash distinto para ele, √© ainda improv√°vel encontrar dois membros do dom√≠nio que possuam o mesmo mapeamento pela fun√ß√£o hash. S√£o exemplo conhecidos de fun√ß√£o hash: MD5, SHA1 e SHA2.
       </span>
      </span>
     </span>
     para elas, pois elas dever√£o ser posteriormente utilizadas para realizar a autentica√ß√£o em algum outro servi√ßo. A base de dados √© armazenada na forma criptografada, utilizando para tanto uma senha mestre. Desta forma, os gerenciadores de senhas tamb√©m est√£o sujeitos a ataques, com o agravante de que um ataque bem sucedido √© capaz de revelar diversas senhas de um determinado usu√°rio.
    </p>
    <p>
     A hist√≥ria recente √© repleta de incidentes de vazamento de informa√ß√µes de usu√°rios
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       11
      </sup>
      <span class="refCtt closed">
       <span>
        [11] Wikipedia, Data breach: Major incidents (2015), disponivel em https://en.wikipedia.org/wiki/ Data_breach, (acessada em 20 de Outubro de 2015).
       </span>
       <br/>
       <a href="https://en.wikipedia.org/wiki/%20Data_breach" target="_blank">
        https://en.wikipedia.org/wiki/ Data_brea...
       </a>
      </span>
     </span>
     <sup>
      ]
     </sup>
     , muitos dos quais atrav√©s da apropria√ß√£o de senhas de usu√°rios, como por exemplo o vazamento de fotos de celebrida des ocorrido em Agosto de 2014
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       12
      </sup>
      <span class="refCtt closed">
       <span>
        [12] J. Ryall, Social media goes wild over massive celebrity nude photo leak (2014), disponivel em http://mashable.com/2014/08/31.
       </span>
       <br/>
       <a href="http://mashable.com/2014/08/31" target="_blank">
        http://mashable.com/2014/08/31...
       </a>
      </span>
     </span>
     <sup>
      ,
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       13
      </sup>
      <span class="refCtt closed">
       <span>
        [13] N. Kerris and T. Muller, Update to Celebrity Photo Investigation (2014), disponivel em https://www.apple.com/pr/library/2014/09/02Apple-Media-Advisory.html.
       </span>
       <br/>
       <a href="https://www.apple.com/pr/library/2014/09/02Apple-Media-Advisory.html" target="_blank">
        https://www.apple.com/pr/library/2014/09...
       </a>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Al√©m deste, muitos outros casos foram amplamente divulgados na m√≠dia, como por exemplo: AOL (2006), RockYou! (2009), Sony (2011), Yahoo (2012), Adobe Systems (2013), Sony (2014), Ashley Madison (2015)
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       11
      </sup>
      <span class="refCtt closed">
       <span>
        [11] Wikipedia, Data breach: Major incidents (2015), disponivel em https://en.wikipedia.org/wiki/ Data_breach, (acessada em 20 de Outubro de 2015).
       </span>
       <br/>
       <a href="https://en.wikipedia.org/wiki/%20Data_breach" target="_blank">
        https://en.wikipedia.org/wiki/ Data_brea...
       </a>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . A an√°lise de senhas mostra que os usu√°rios utilizam estrat√©gias fracas para criar suas senhas. As senhas mais comuns geralmente s√£o rudimentares como '123456', '12345', 'password', ou 'abc123', evidenciando que os usu√°rios escolhem senhas preocupados com a sua facilidade de memoriza√ß√£o. Um estudo emp√≠rico mostra que 80% das senhas utilizam apenas os 26 caracteres do alfabeto, 65,7% dos usu√°rios utilizam senhas que tenham entre 4 e 6 caracteres, 14% utilizam senhas com 7 caracteres e 13% utilizam senhas com 8 caracteres
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       14
      </sup>
      <span class="refCtt closed">
       <span>
        [14] M. Zviran and W.J. Haga, Journal of Management Information Systems 15, 4 (1999).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Estas an√°lises ilustram que a abordagem utilizada para a cria√ß√£o de senhas √© equivocada. Existem formas mais seguras e intuitivas de criar senhas, uma delas foi ilustrada, de forma bem humorada, nos quadrinhos de xkcd
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       15
      </sup>
      <span class="refCtt closed">
       <span>
        [15] R. Munroe, Password strength (2011), disponivel em https://xkcd.com/936/.
       </span>
       <br/>
       <a href="https://xkcd.com/936/" target="_blank">
        https://xkcd.com/936/...
       </a>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     Uma forma comum de mensurar a robustez de uma senha √© atrav√©s do c√°lculo da entropia por caractere. A medida de entropia surgiu com a teoria da informa√ß√£o formulada por Claude Shannon em 1948
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       16
      </sup>
      <span class="refCtt closed">
       <span>
        [16] C.E. Shannon, Bell Systems Technical Journal 27, 3 (1948).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Shannon explica que a entropia √© uma medida estat√≠stica de quanta informa√ß√£o √© produzida na m√©dia por s√≠mbolo de uma fonte como, por exemplo, letras de texto em uma determinada l√≠ngua. Claramente, o tamanho do alfabeto √© fator preponderante na estimativa da entropia. Al√©m disso, a probabilidade de ocorr√™ncia dos s√≠mbolos determina a entropia associada √† fonte. Isto implica que uma senha mais robusta √© aquela produzida de forma completamente aleat√≥ria. Se, de alguma forma, obtivermos conhecimento sobre a frequ√™ncia de ocorr√™ncia de s√≠mbolos utilizados para gerar senhas, a tarefa de determina√ß√£o destas senhas ser√° ent√£o facilitada.
    </p>
    <h1 class="articleSectionTitle">
     3. Teoria da informa√ß√£o
    </h1>
    <p>
     A teoria da informa√ß√£o fornece uma maneira de quantificar informa√ß√£o, estuda a compress√£o de dados, transmiss√£o atrav√©s de um canal ruidoso e corre√ß√£o de erros. Claude Shannon √© considerado o pai da teoria da informa√ß√£o e da criptografia moderna. O advento dos computadores nos forneceu uma maneira de manipular e processar informa√ß√£o de forma eficiente, enquanto Shannon nos forneceu uma maneira de interpretar e entender informa√ß√£o
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       16
      </sup>
      <span class="refCtt closed">
       <span>
        [16] C.E. Shannon, Bell Systems Technical Journal 27, 3 (1948).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Shannon evidencia as rela√ß√µes entre o estudo de sua teoria de comunica√ß√£o, abarcando corre√ß√£o de erros e teoria da informa√ß√£o, e cripto grafia
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       17
      </sup>
      <span class="refCtt closed">
       <span>
        [17] C.E. Shannon, Bell Systems Technical Journal 28, 4 (1949).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . No problema de comunica√ß√£o, temos o receptor que, ao receber uma mensagem atrav√©s de um canal ruidoso, busca decodificar a mensagem recebida e recuperar a informa√ß√£o original. Este problema √© similar √† criptografia, em que a chave √© utilizada para ofuscar a informa√ß√£o de forma que ainda seja poss√≠vel recuper√°-la sem perda.
    </p>
    <h2>
     3.1. Entropia
    </h2>
    <p>
     Shannon quantificou e explicou o significado de informa√ß√£o. A informa√ß√£o associada a um evento espec√≠fico, ou a uma realiza√ß√£o de uma vari√°vel aleat√≥ria, √© dada pelo logaritmo de sua probabilidade. Ele definiu ent√£o informa√ß√£o como o valor esperado da informa√ß√£o por evento, que √© expressa atrav√©s da equa√ß√£o
    </p>
    <div class="row formula" id="ee1">
     <a name="e1">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/5b9885be576509c72b4aecd88f128672bfb8bab7.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     onde
     <i>
      H
     </i>
     , chamado entropia, √© a medida de informa√ß√£o, ou incerteza, associada √† vari√°vel aleat√≥ria
     <i>
      X. H
     </i>
     √© uma fun√ß√£o exclusivamente das probabilidades
     <i>
      p
     </i>
     (
     <i>
      x
     </i>
     ) dos eventos
     <i>
      x
     </i>
     no alfabeto ùìß. Usualmente, utiliza-se o logaritmo na base 2 e desta forma a informa√ß√£o √© medida em bits. Adiante, quando utilizarmos log dentro do contexto de teoria da informa√ß√£o, estaremos sempre utilizando a base 2 e consequentemente teremos as medidas em bits.
    </p>
    <p>
     Golomb explica que informa√ß√£o √© quantificada pela "quantidade de informa√ß√£o adquirida (ou entropia removida) ao aprender a resposta sobre uma pergunta cujas duas respostas poss√≠veis s√£o igualmente prov√°veis"
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       18
      </sup>
      <span class="refCtt closed">
       <span>
        [18] S.W. Golomb, E. Berlekamp, T.M. Cover, R.G. Gallager, J.L. Massey and A.J. Viterbi, Notices of the American Mathematical Society 49, 1 (2002).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <h2>
     3.2. An√°lise de frequ√™ncia de ocorr√™ncia
    </h2>
    <p>
     Podemos definir a criptoan√°lise como a arte e ci√™ncia de resolver c√≥digos e cifras. Em geral o problema envolve inicialmente descobrir qual √© a linguagem utilizada na comunica√ß√£o, o tipo gen√©rico de codifica√ß√£o ou cifra adotado, as chaves espec√≠ficas e finalmente a reconstru√ß√£o da mensagem enviada. Para realizar esta an√°lise e determinar estes par√¢metros, devemos nos basear em uma grande quantidade de texto cifrado e informa√ß√µes relacionadas ao texto, como por exemplo, informa√ß√µes sobre o autor e destinat√°rio, algum conhecimento espec√≠fico sobre o conte√∫do da mensagem, etc.
    </p>
    <p>
     Vamos aqui assumir o princ√≠pio de Kerckhoffs, ou seja, um sistema de criptografia deve ser seguro mesmo que tudo sobre o sistema seja conhecido, exceto a chave. Auguste Kerckhoffs
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       19
      </sup>
      <span class="refCtt closed">
       <span>
        [19] A. Kerckhoffs, Journal des Sciences Militaires 9 (1883).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     estabeleceu seis princ√≠pios para criptografia:
    </p>
    <div>
     <ol type="1">
      <li>
       <p>
        o sistema deve ser praticamente indecifr√°vel;
       </p>
      </li>
      <li>
       <p>
        n√£o deve ser necess√°rio sigilo sob a forma como √© realizada a criptografia, de forma que n√£o seja um problema se o m√©todo cair nas m√£os do inimigo;
       </p>
      </li>
      <li>
       <p>
        √© desej√°vel que seja poss√≠vel transmitir e lembrar a chave sem a necessidade de notas textuais, sendo tamb√©m poss√≠vel modific√°-la se necess√°rio ou desej√°vel;
       </p>
      </li>
      <li>
       <p>
        deve ser apropriado para a comunica√ß√£o telegr√°fica;
       </p>
      </li>
      <li>
       <p>
        deve ser port√°vel e n√£o deve requerer v√°rias pessoas para operacionalizar;
       </p>
      </li>
      <li>
       <p>
        o sistema deve ser de f√°cil manuseio, n√£o necessitando grande esfor√ßo dos usu√°rios.
       </p>
      </li>
     </ol>
    </div>
    <p>
     Alguns destes n√£o s√£o mais relevantes, devido √† capacidade computacional de realizar criptografia complexa dispon√≠vel nos tempos atuais, entretanto, o segundo axioma ainda √© cr√≠tico e portanto conhecido como o princ√≠pio de Kerckhoffs. Este mesmo princ√≠pio √© formulado por Shannon em sua m√°xima: "o inimigo conhece o sistema"
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       17
      </sup>
      <span class="refCtt closed">
       <span>
        [17] C.E. Shannon, Bell Systems Technical Journal 28, 4 (1949).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     , ou seja, um sis tema de criptografia deve ser seguro mesmo que tudo sobre o sistema, exceto a chave, seja p√∫blico. Este paradigma de criptografia se op√µe claramente √† "seguran√ßa atrav√©s da obscuridade".
    </p>
    <p>
     Assumindo ent√£o que o m√©todo de criptografia √© p√∫blico, o ataque de for√ßa bruta consiste em procurar, atrav√©s de uma busca exaustiva no espa√ßo de chaves, aquela utilizada em um caso particular. Se esta for determinada com sucesso, o invasor ter√° ent√£o a capacidade de desvendar todas as futuras mensagens at√© que a chave seja alterada. O ataque for√ßa bruta √© um m√©todo passivo usualmente feito
     <i>
      offline
     </i>
     . Ele consiste em uma busca exaustiva pela senha, o que √© a alternativa adotada quando nenhuma outra informa√ß√£o existe sobre as fragilidades de um sistema de criptografia, ou nenhuma outra informa√ß√£o sobre o remetente, destinat√°rio ou a mensagem em si. No pior dos casos, o ataque for√ßa bruta ir√° percorrer todo espa√ßo de busca poss√≠vel. Uma chave com comprimento de
     <i>
      N
     </i>
     bits poder√° ser quebrada, em no m√°ximo 2
     <i>
      <sup>
       N
      </sup>
     </i>
     tentativas, e na m√©dia, metade do n√∫mero de tentativas ser√° suficiente.
    </p>
    <p>
     O princ√≠pio de Landauer estabelece que existe uma quantidade m√≠nima de energia necess√°ria para apagar um bit de informa√ß√£o, ou seja, este √© um processo dissipativo. Esta quantidade m√≠nima, segundo Landauer
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       20
      </sup>
      <span class="refCtt closed">
       <span>
        [20] R. Landauer, IBM J. Res. Dev. 5, 3 (1961).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     est√° relacionada com a temperatura e a constante de Boltzmann, sendo dada por
     <i>
      kT
     </i>
     ln 2, onde
     <i>
      k
     </i>
     √© a constante de Boltzmann (aproximadamente 1, 38 √ó 10
     <sup>
      ‚àí23
     </sup>
     J/K) e
     <i>
      T
     </i>
     √© a temperatura do circuito em Kelvin. Apesar de sua import√¢ncia para a teoria da informa√ß√£o e ci√™ncia da computa√ß√£o, o princ√≠pio n√£o havia sido demonstrado at√© poucos anos atr√°s, quando Eric Lutz e seus colegas mostram experimentalmente a exist√™ncia do limite de Landauer em um modelo gen√©rico de uma mem√≥ria de um bit, tomado como hip√≥tese por Landauer
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       21
      </sup>
      <span class="refCtt closed">
       <span>
        [21] A. B√©rut, A. Arakelyan, A. Petrosyan, S. Ciliberto, R. Dillenschneider and E. Lutz, Nature 483, 7388 (2012).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Este modelo consiste em uma √∫nica part√≠cula que poderia estar em um dos dois po√ßos de potencial, sendo que em um deles representaria o estado '0' e o outro o estado '1'. O bit ser√° apagado ao for√ßar a part√≠cula a entrar no estado '1'. Considerando que a tarefa ser√° realizada √† temperatura ambiente de 20 ¬∞C (293,15 K), o limite de Landauer ser√° aproximadamente 0,0172 el√©tron-volt (eV) ou 2,75 zeptojoules (zJ). Considerando uma chave de 128 bits, ser√£o necess√°rios 2
     <sup>
      128
     </sup>
     -1
     <i>
      flips
     </i>
     de bit para gerar todas as poss√≠veis combina√ß√µes de chaves. Vamos desconsiderar os recursos necess√°rios para verificar a validade da chave gerada. Ser√£o ent√£o necess√°rios 10
     <sup>
      128 log
      <sub>
       10
      </sub>
      2
     </sup>
     √ó 2, 75√ó10‚àí
     <sup>
      21
     </sup>
     = 9, 36√ó10
     <sup>
      17
     </sup>
     J = 260TWh, ou seja, toda energia gerada por 2,82 Usinas de Itaipu durante um ano inteiro (considerando a m√©dia dos √∫ltimos 10 anos), para gerar todas as poss√≠veis chaves de 128 bits. Se considerarmos ainda o gasto de energia para verificar cada uma dessas chaves, seria necess√°rio um consumo ainda muito maior.
    </p>
    <p>
     Se, de alguma forma (consciente ou n√£o), o usu√°rio delibera por um espa√ßo de chaves restrito, a tarefa de quebrar a criptografia utilizando for√ßa bruta ser√° facilitada. Em criptoan√°lise, utiliza-se a ideia de que as l√≠nguas naturais apresentam padr√µes determinados e redund√¢ncias que podem ser utilizados para ajudar a quebrar uma criptografia. A redund√¢ncia em uma l√≠ngua √© uma quest√£o tamb√©m de interesse de linguistas que buscam modelos matem√°ticos para descrever as l√≠nguas naturais. Apesar da aparente diversidade encontrada entre diversas l√≠nguas, de diferentes fam√≠lias lingu√≠sticas, o ordenamento dos constituintes
     <span class="ref footnote">
      <sup class="xref">
       3
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         3
        </strong>
        A ordem dos constituintes de uma senten√ßa √© um crit√©rio tipol√≥gico para a classifica√ß√£o das l√≠nguas de acordo com sua sintaxe.
       </span>
      </span>
     </span>
     utilizados em uma l√≠ngua possui uma entropia relativa associada que apresenta-se como uma caracter√≠stica universal das linguagens natu rais
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       22
      </sup>
      <span class="refCtt closed">
       <span>
        [22] M.A. Montemurro and D.H. Zanette, PLoS ONE 6, e19875 (2011).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     George Kingsley Zipf observou uma rela√ß√£o de pot√™ncia entre o ranqueamento e a frequ√™ncia de ocorr√™ ncia de palavras em textos
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       23
      </sup>
      <span class="refCtt closed">
       <span>
        [23] G.K. Zipf, Human Behaviour and the Principle of Least Effort: An Introduction to Human Ecology (Addison-Wesley Press Inc., Cambridge, 1949).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ,
    </p>
    <div class="row formula" id="ee2">
     <a name="e2">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/3226abff04464c800f74c80e6fb3dee00e56cace.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     onde
     <i>
      k
     </i>
     √© a posi√ß√£o que a palavra ocupa em uma lista ordenada (ranqueamento, posi√ß√£o ou ordem; em ingl√™s
     <i>
      rank
     </i>
     ),
     <i>
      f
      <sub>
       k
      </sub>
     </i>
     √© a frequ√™ncia de ocorr√™ncia da
     <i>
      k
     </i>
     -√©sima palavra e
     <i>
      s
     </i>
     √© uma constante que caracteriza a distribui√ß√£o. A
     <a class="open-asset-modal" data-target="#ModalFigf1" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 1
     </a>
     apresenta a rela√ß√£o observada entre a ordem de uma palavra e a sua frequ√™ncia de ocorr√™ncia. Foi utilizado o
     <i>
      Open American National Corpus
     </i>
     (veja a sec√ß√£o 5).
    </p>
    <p>
    </p>
    <div class="row fig" id="f1">
     <a name="f1">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf1" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 1
      </strong>
      <br/>
      Lei de Zipf no Ingl√™s escrito (dados do OANC).
      <i>
       Rank
      </i>
      (
      <i>
       k
      </i>
      ) versus frequ√™ncia de ocorr√™ncia (
      <i>
       f
      </i>
      ).
      <br/>
     </div>
    </div>
    <p>
     Para gerar o gr√°fico ilustrado na
     <a class="open-asset-modal" data-target="#ModalFigf1" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 1
     </a>
     , primeiramente selecionamos os arquivos de texto contidos no corpus que representam dados da l√≠ngua escrita. Podemos ent√£o contabilizar a frequ√™ncia de ocorr√™ncia de cada palavra, orden√°-las segundo a frequ√™ncia de ocorr√™ncia e, por fim, realizar o gr√°fico do
     <i>
      rank
     </i>
     (ordem) versus a frequ√™ncia de ocorr√™ncia. Notamos que, ao utilizar uma escala logar√≠tmica no eixos das abscissas e ordenadas, a rela√ß√£o torna-se aproximadamente linear.
    </p>
    <p>
     A lei de Zipf √© uma lei enigm√°tica, controversa e amplamente observada na natureza, inclusive nas linguagens humanas. Em alguns casos, temos grandezas em que a escala para sua medi√ß√£o varia em torno de um determinado valor, outras grandezas variam por uma enorme extens√£o, muitas vezes esta extens√£o cobre muitas ordens de grandeza. Um exemplo t√≠pico para o primeiro caso √© a altura de um ser humano adulto; j√° para o segundo caso, podemos tomar como exemplo o tamanho da popula√ß√£o de cidades. Distribui√ß√µes desta forma s√£o ditas seguir uma lei de pot√™ncia. Esta rela√ß√£o de pot√™ncia √© tamb√©m observada em outros diferentes fen√¥menos, tais como: magnitude de terremotos
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       24
      </sup>
      <span class="refCtt closed">
       <span>
        [24] S. Abe, N. Suzuki, Physica A: Statistical Mechanics and its Applications 350, 2 (2005).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; popula√ß√£o de cidades
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       25
      </sup>
      <span class="refCtt closed">
       <span>
        [25] X. Gabaix, Quarterly Journal of Economics 114,3 (1999).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; economia
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       26
      </sup>
      <span class="refCtt closed">
       <span>
        [26] B. Mandelbrot, The Journal of Business 36, 4 (1963).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; express√£o g√™ nica
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       27
      </sup>
      <span class="refCtt closed">
       <span>
        [27] C. Furusawa and K. Kaneko, Physical Review Letters 90, 8 (2003).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; sistemas din√¢micos ca√≥ticos
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       28
      </sup>
      <span class="refCtt closed">
       <span>
        [28] G. Nicolis, C. Nicolis and J.S. Nicolis, Journal of Statistical Physics 54, 3(1989).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; magnitude de ava lanches
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       29
      </sup>
      <span class="refCtt closed">
       <span>
        [29] P. Bak, How Nature Works: The Science of Selforganized Criticality (Copernicus, New York, 1996).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; tr√°fegos de dados na Internet
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       30
      </sup>
      <span class="refCtt closed">
       <span>
        [30] M.E. Crovella and A. Bestavros, in: Proceedings of the 1996 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems Philadelphia, 1996, edited by B.D. Gaither (ACM, New York, 1996).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; n√∫mero de cita√ß√µes de artigos cient√≠ficos
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       31
      </sup>
      <span class="refCtt closed">
       <span>
        [31] D.J. de Solla Price, Science 149, 3683 (1965).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; tiragem de livros e discos
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       32
      </sup>
      <span class="refCtt closed">
       <span>
        [32] R. Kohli and R.K. Sah, Management Science 52, 11 (2003).
       </span>
      </span>
     </span>
     <sup>
      ,
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       33
      </sup>
      <span class="refCtt closed">
       <span>
        [33] R.A.K. Cox, J.M. Felton and K.C. Chung, Journal of Cultural Economics 19 (1995).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     ; e muitos outros. A lei de Zipf pode ser interpretada como uma reformula√ß√£o do princ√≠pio do Pareto que afirma que, para muitos fen√¥menos, grande parte dos efeitos proveem de uma por√ß√£o minorit√°ria das poss√≠veis causas. O nome deste princ√≠pio foi dado em homenagem ao economista italiano Vilfredo Pareto que, em 1906, observou que 80% das terras da It√°lia estavam nas m√£os de apenas 20% da popula√ß√£o. Quando nos referimos √† lei de Zipf, a utilizamos num contexto em que ser√° relacionada frequ√™ncia de ocorr√™ncia de um evento com o seu
     <i>
      rank
     </i>
     , ou seja, podemos ver esta rela√ß√£o como uma fun√ß√£o massa de probabilidade sobre o
     <i>
      rank
     </i>
     . A lei do Pareto, por outro lado, √© dada em termos da fun√ß√£o distribui√ß√£o acumulada, quer dizer, o n√∫mero de eventos maiores que um determinado valor √© inversamente proporcional ao valor dado.
    </p>
    <p>
     Uma distribui√ß√£o de lei de pot√™ncia tamb√©m √© dita invariante √† escala, uma vez que esta √© a √∫nica distribui√ß√£o que possui o mesmo padr√£o em qual quer escala em que for observada
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       34
      </sup>
      <span class="refCtt closed">
       <span>
        [34] M.E.J. Newman, Contemporary Physics 46, 5 (2005).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Isto pode ser demonstrado seguindo os passos apresentados por Mark Newman
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       34
      </sup>
      <span class="refCtt closed">
       <span>
        [34] M.E.J. Newman, Contemporary Physics 46, 5 (2005).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Primeiramente, vamos supor que existe uma distribui√ß√£o
     <i>
      p
     </i>
     (
     <i>
      k
     </i>
     ) que √© invariante √† mudan√ßa de escala, ou seja, a seguinte rela√ß√£o dever√° ser satisfeita
    </p>
    <div class="row formula" id="ee3">
     <a name="e3">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/857b401d7efef2e20a5a75aeec25adf473e6254f.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     Para qualquer mudan√ßa de escala, por um fator b, na vari√°vel de medi√ß√£o
     <i>
      k
     </i>
     , teremos como reflexo uma mudan√ßa de escala na distribui√ß√£o dada por um fator multiplicativo
     <i>
      g
     </i>
     (
     <i>
      b
     </i>
     ). Esta rela√ß√£o dever√° ser verdadeira para qualquer
     <i>
      b
     </i>
     e ter validade ao longo de todos os valores de
     <i>
      k
     </i>
     . Vamos inicialmente supor
     <i>
      k
     </i>
     = 1, o que nos fornece
     <i>
      p
     </i>
     (
     <i>
      b
     </i>
     )=
     <i>
      g
     </i>
     (
     <i>
      b
     </i>
     )
     <i>
      p
     </i>
     (1), e assim, substituindo na Eq. (3) teremos
    </p>
    <div class="row formula" id="ee4">
     <a name="e4">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/206d844c064badbd6b4b4c849f9124f779f4e563.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     Como esta equa√ß√£o dever√° ser verdadeira para todo
     <i>
      b
     </i>
     , vamos diferenci√°-la com rela√ß√£o √†
     <i>
      b
     </i>
     , obtendo
    </p>
    <div class="row formula" id="ee5">
     <a name="e5">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/7d0925dab56b1594fac4ea80ce366fdf8c0134bc.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     onde ·πï √© a derivada de
     <i>
      p
     </i>
     com rela√ß√£o ao seu argumento. Escolhendo agora
     <i>
      b
     </i>
     = 1, teremos
    </p>
    <div class="row formula" id="ee6">
     <a name="e6">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/9d2eb115443e8396580a49cfb1b7172c794e1cda.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     Esta equa√ß√£o diferencial de primeira ordem possui como solu√ß√£o
    </p>
    <div class="row formula" id="ee7">
     <a name="e7">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/9bc39f9e7ccbcc21b8cda8c283f0586dc3a7a778.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     Fazendo
     <i>
      k
     </i>
     = 1, obtemos o valor da constante como sendo ln
     <i>
      p
     </i>
     (1). Podemos fazer a exponencial de ambos os lados da nossa solu√ß√£o e obteremos
    </p>
    <div class="row formula" id="ee8">
     <a name="e8">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/14269d5eb3fc4e2c312b1bdcaa42cadc300e5e61.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     onde
     <i>
      s
     </i>
     = ‚àí
     <i>
      p
     </i>
     (1)/·πï(1). Obtivemos assim a lei de pot√™ncia como sendo a distribui√ß√£o invariante a uma mudan√ßa de escala.
    </p>
    <p>
     A an√°lise de senhas, que tornaram-se p√∫blicas ap√≥s incidentes de seguran√ßa, mostra que estas tamb√©m apresentam a mesma rela√ß√£o observada por Zipf
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       35
      </sup>
      <span class="refCtt closed">
       <span>
        [35] D. Malone and K. Maher, in: Proceedings of the 21st International Conference on World Wide Web, Lyon, 2012 (ACM, New York, 2012).
       </span>
      </span>
     </span>
     <sup>
      ,
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       36
      </sup>
      <span class="refCtt closed">
       <span>
        [36] D. Wang, G. Jian, X. Huang and P. Wang, in: Transactions on Information and System Security (ACM, New York, 2015), volume 1.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . O expoente
     <i>
      s
     </i>
     da distribui√ß√£o de Zipf √© importante na caracteriza√ß√£o da fonte. Ele pode ser utilizado, por exemplo, como um par√¢metro importante para constituir a assinatura de um autor, evidenciada ao analisar quantitativamente seus textos
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       37
      </sup>
      <span class="refCtt closed">
       <span>
        [37] L.L. Gon√ßalves and L.B. Gon√ßalves, Physica A 360, 2 (2006).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . As linguagens naturais apresentam um expoente caracter√≠stico
     <i>
      s
     </i>
     ‚âà 1
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       38
      </sup>
      <span class="refCtt closed">
       <span>
        [38] R.G. Piotrovskii, V.E. Pashkovskii and V.R. Piotrovskii, Nauchno-Tekhnicheskaya Informatsiya 28, 11 (1994).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Para alguns ti pos de comunica√ß√£o humana o valor do expoente √© maior, por exemplo, um expoente s ‚âà 1,66 foi encontrado na fala das crian√ßas;
     <i>
      s
     </i>
     ‚âà 1,43 em comunica√ß√£o militar e
     <i>
      s
     </i>
     &lt; 1 para formas avan√ßadas de esquizofrenia
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       38
      </sup>
      <span class="refCtt closed">
       <span>
        [38] R.G. Piotrovskii, V.E. Pashkovskii and V.R. Piotrovskii, Nauchno-Tekhnicheskaya Informatsiya 28, 11 (1994).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Um expoente 0 &lt;
     <i>
      s
     </i>
     &lt; 1 restringe o l√©xico a um tamanho finito, j√° um expoente
     <i>
      s
     </i>
     &gt; 1 permite que l√©xico cres√ßa sem limites
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       39
      </sup>
      <span class="refCtt closed">
       <span>
        [39] L.C. Araujo, H.C. Yehia and T. Crist√≥faro-Silva, Glottometrics 26, 38 (2013).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Um ex poente pr√≥ximo de 1 √© encontrado como resultado da minimiza√ß√£o da fun√ß√£o de energia, combinando o esfor√ßo do ouvinte e o esfor√ßo do falante
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       40
      </sup>
      <span class="refCtt closed">
       <span>
        [40] R. Ferrer-i-Cancho, Proceedings of the National Academy of Sciences of the United States of America 100, 3 (2003).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . A estima√ß√£o da entropia √© mais sens√≠vel a erros quando
     <i>
      s
     </i>
     est√° nas proximidades de 1, podendo ser severa mente afetada pelo truncamento amostral
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       39
      </sup>
      <span class="refCtt closed">
       <span>
        [39] L.C. Araujo, H.C. Yehia and T. Crist√≥faro-Silva, Glottometrics 26, 38 (2013).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     Os primeiros experimentos para estimar a entropia da l√≠ngua inglesa escrita foram realizados por Shannon
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       41
      </sup>
      <span class="refCtt closed">
       <span>
        [41] C.E. Shannon, Bell Systems Technical Journal 30, 1 (1951).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     e posteriormente por Cover e King
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       42
      </sup>
      <span class="refCtt closed">
       <span>
        [42] T.M. Cover and R.C. King, IEEE Transactions on Information Theory 24, 4 (1978).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Estes experimentos buscavam averiguar se as pessoas eram capazes de predizer o pr√≥ximo caractere em um texto. Atrav√©s deste experimento, Shannon estimou um valor entre 0,6 e 1,3 bits por caractere para a entropia do ingl√™ s escrito
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       41
      </sup>
      <span class="refCtt closed">
       <span>
        [41] C.E. Shannon, Bell Systems Technical Journal 30, 1 (1951).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     , j√° Conver e King estimaram 1,25 bits por caractere
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       42
      </sup>
      <span class="refCtt closed">
       <span>
        [42] T.M. Cover and R.C. King, IEEE Transactions on Information Theory 24, 4 (1978).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Posteri ormente, foram realizados experimentos com aux√≠lio de recursos computacionais para comprimir textos em ingl√™s, buscando uma compress√£o eficiente, que deve aproximar-se da real entropia da fonte. Teahan e Cleary utilizaram uma varia√ß√£o do algoritmo
     <i>
      Prediction by Partial Matching
     </i>
     , que utiliza como modelo a cadeia de Markov, atingindo 1,46 bits por caractere
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       43
      </sup>
      <span class="refCtt closed">
       <span>
        [43] W.J. Teahan and J.G. Cleary, in: Data Compression Conference, Snowbird, 1996, edited by J.A. Storer and M. Cohn (IEEE Computer Society Press, 1996), p. 53.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     , valor este bem pr√≥ximo dos valores de entropia reportados anteriormente
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       41
      </sup>
      <span class="refCtt closed">
       <span>
        [41] C.E. Shannon, Bell Systems Technical Journal 30, 1 (1951).
       </span>
      </span>
     </span>
     <sup>
      ,
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       42
      </sup>
      <span class="refCtt closed">
       <span>
        [42] T.M. Cover and R.C. King, IEEE Transactions on Information Theory 24, 4 (1978).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     A
     <a class="open-asset-modal" data-target="#ModalFigf2" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 2
     </a>
     apresenta uma estimativa da probabilidade de ocorr√™ncia dos caracteres no Ingl√™s. Analisando esta probabilidade ao longo de um texto, constatamos que elas n√£o sofrem grandes altera√ß√µes e portanto podemos considerar que a fonte em quest√£o √© erg√≥dica estacion√°ria, ou seja, qualquer amostra estatisticamente significante do processo representa as caracter√≠sticas estat√≠sticas do processo (erg√≥tico) e estas caracter√≠sticas n√£o se alteram ao longo do tempo (estacion√°rio). Para a analisar o comportamento da estimativa para a fun√ß√£o massa de probabilidade ao longo do texto, selecionamos trechos com 1 milh√£o caracteres, com sobreposi√ß√£o de 995 mil caracteres, e estimamos as probabilidades ao longo da base de dados textual (veja a
     <a class="open-asset-modal" data-target="#ModalFigf3" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 3
     </a>
     , na qual a barra de erro representa a incerteza sobre a medi√ß√£o, dada por um desvio padr√£o). Conhecer a distribui√ß√£o da fonte √© importante quando desejamos estabelecer um crit√©rio de busca mais eficiente do que a mera busca exaustiva por todas as possibilidades. Veremos adiante como a distribui√ß√£o, ou melhor, a entropia afeta o tamanho do espa√ßo de chaves.
    </p>
    <p>
    </p>
    <div class="row fig" id="f2">
     <a name="f2">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf2" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 2
      </strong>
      <br/>
      Probabilidade dos caracteres em Ingl√™s (a-z) ao longo de um
      <i>
       corpus
      </i>
      (foram observadas sequ√™ncias subsequentes de 1 milh√£o caracteres com sobreposi√ß√£o de 995 mil caracteres).
      <br/>
     </div>
    </div>
    <p>
    </p>
    <div class="row fig" id="f3">
     <a name="f3">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf3" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 3
      </strong>
      <br/>
      Obten√ß√£o da probabilidade dos caracteres ao longo de um texto.
      <br/>
     </div>
    </div>
    <h2>
     3.3. Conjunto t√≠pico
    </h2>
    <p>
     Analisando sequ√™ncias de s√≠mbolos sob a √≥tica de teoria da informa√ß√£o, utiliza-se o conceito de conjunto t√≠pico para designar o menor conjunto das sequ√™ncias que correspondem a todas as sequ√™ncias observ√°veis, ou seja, tudo aquilo que ocorre. Se expressarmos o conjunto t√≠pico das sequ√™ncias de comprimento
     <i>
      n
     </i>
     por A
     <i>
      <sub>
       ‚àà
      </sub>
      <sup>
       (n)
      </sup>
     </i>
     , teremos que Pr(A
     <i>
      <sub>
       ‚àà
      </sub>
      <sup>
       (n)
      </sup>
     </i>
     ) ‚âà 1
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       44
      </sup>
      <span class="refCtt closed">
       <span>
        [44] J.A. Thomas and T.M. Cover, Elements of Information Theory (Wiley, Hoboken, 1991), 2nd edition.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     Considerando ent√£o uma sequ√™ncia de
     <i>
      n
     </i>
     vari√°veis aleat√≥rias independentes e identicamente distribu√≠das, podemos afirmar que a probabilidade de observamos uma determinada realiza√ß√£o para esta sequ√™ncia ser√° dada por
     <i>
      p
     </i>
     (
     <i>
      X
     </i>
     <sub>
      1
     </sub>
     =
     <i>
      x
     </i>
     <sub>
      1
     </sub>
     ,...,
     <i>
      X
      <sub>
       n
      </sub>
     </i>
     =
     <i>
      x
      <sub>
       n
      </sub>
     </i>
     ) = ‚àè
     <i>
      <sup>
       n
      </sup>
     </i>
     <sub>
      <i>
       i
      </i>
      = 1
     </sub>
     <i>
      p
     </i>
     (
     <i>
      X
      <sub>
       i
      </sub>
     </i>
     =
     <i>
      x
      <sub>
       i
      </sub>
     </i>
     ). A informa√ß√£o de Hartley-Shannon associada a um evento
     <i>
      x
     </i>
     √© dada por
     <i>
      I
     </i>
     (
     <i>
      x
     </i>
     )= ‚àí log
     <i>
      p
     </i>
     (
     <i>
      x
     </i>
     ). Desta forma, a informa√ß√£o associada a uma sequ√™ncia de s√≠mbolos
     <i>
      x
     </i>
     <sub>
      1
     </sub>
     ,...,
     <i>
      x
      <sub>
       n
      </sub>
     </i>
     ser√° dada por
     <i>
      I
     </i>
     (
     <i>
      x
     </i>
     <sub>
      1
     </sub>
     ,...,
     <i>
      x
      <sub>
       n
      </sub>
     </i>
     ) = ‚àë
     <i>
      <sup>
       n
      </sup>
     </i>
     <sub>
      <i>
       i
      </i>
      = 1
     </sub>
     <i>
      I
     </i>
     (
     <i>
      x
     </i>
     <sub>
      i
     </sub>
     ). Teremos assim
    </p>
    <div class="row formula" id="ee9">
     <a name="e9">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/42d4d85567a796460370ecc35a182e2a5b624c05.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     e desta forma poderemos mostrar que
     <i>
      p
     </i>
     (
     <i>
      x
     </i>
     <sub>
      1
     </sub>
     ,...,
     <i>
      x
      <sub>
       n
      </sub>
     </i>
     ) ‚âà 2
     <sup>
      ‚àí
      <i>
       nH
      </i>
     </sup>
     . As sequ√™ncias t√≠picas ter√£o aproximadamente a mesma probabilidade e, portanto, existem 2
     <i>
      <sup>
       nH
      </sup>
     </i>
     sequ√™ncias deste tipo
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       44
      </sup>
      <span class="refCtt closed">
       <span>
        [44] J.A. Thomas and T.M. Cover, Elements of Information Theory (Wiley, Hoboken, 1991), 2nd edition.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     Supondo, por exemplo, que tenhamos um ensaio de Bernoulli
     <span class="ref footnote">
      <sup class="xref">
       4
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         4
        </strong>
        Ensaio de Bernoulli √© um experimento aleat√≥rio, no qual apenas dois resultados s√£o poss√≠veis: verdadeiro (1) ou falso (0).
       </span>
      </span>
     </span>
     com
     <i>
      p
     </i>
     = 0,9. Em uma sequ√™ncia bin√°ria de comprimento 100, esperamos observar 90 vezes o n√∫mero 1. Uma sequ√™ncia t√≠pica ser√° aquela em que o n√∫mero de observa√ß√µes do n√∫mero 1 for pr√≥ximo de 90. Para o valor de
     <i>
      p
     </i>
     dado, teremos que
     <i>
      H
     </i>
     (
     <i>
      p
     </i>
     ) = 0,469 bits. Teremos ent√£o que o n√∫mero de sequ√™ncias t√≠picas ser√° aproximadamente 2
     <sup>
      <i>
       nH(p)
      </i>
     </sup>
     = 2
     <sup>
      100√ó0,469
     </sup>
     = 2
     <sup>
      46.9
     </sup>
     ‚âà 1,31 √ó 10
     <sup>
      14
     </sup>
     . Este n√∫mero √©, entretanto, bem menor do que o n√∫mero de poss√≠veis sequ√™ncias de comprimento 100, 2
     <sup>
      100
     </sup>
     ‚âà 10
     <sup>
      30,1
     </sup>
     , ou seja, usualmente teremos 2
     <sup>
      <i>
       nH
      </i>
     </sup>
     ¬´ 2
     <sup>
      <i>
       n
      </i>
      log |ùìß|
     </sup>
     . A igualdade s√≥ ocorrer√° quando a entropia for m√°xima, ou seja, quando a distribui√ß√£o da fonte for uniforme. Podemos verificar que a raz√£o entre o tamanho do conjunto t√≠pico e o tamanho do conjunto de todas sequ√™ncias √© exponencialmente decrescente com
     <i>
      n
     </i>
     , ou seja, 2
     <sup>
      <i>
       n
      </i>
      (H‚àílog |ùìß|)
     </sup>
     , teremos ent√£o um decrescimento mais acentuado quanto mais distante a entropia for do seu valor m√°ximo.
    </p>
    <p>
     A probabilidade de encontrarmos, em um ensaio de Bernoulli com
     <i>
      n
     </i>
     realiza√ß√µes (
     <i>
      x
     </i>
     <sub>
      1
     </sub>
     ,...,
     <i>
      x
      <sub>
       n
      </sub>
     </i>
     ) e com probabilidade
     <i>
      p
     </i>
     para 1 (um) e probabilidade (1 -
     <i>
      p
     </i>
     ) para 0 (zero), uma sequ√™ncia em que existam
     <i>
      k
     </i>
     ocorr√™ncias de 1 (um) ser√° dada por
    </p>
    <div class="row formula" id="ee10">
     <a name="e10">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/08d7de80bf1d23f61c3b9916cdc6a20c6b71c9e5.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
     onde
     <i>
      S
      <sub>
       n
      </sub>
     </i>
     =
     <i>
      x
     </i>
     <sub>
      1
     </sub>
     + ... +
     <i>
      x
      <sub>
       n
      </sub>
     </i>
     , ou seja, a soma das
     <i>
      n
     </i>
     realiza√ß√µes do ensaio de Bernoulli. A
     <a class="open-asset-modal" data-target="#ModalFigf4" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 4
     </a>
     apre senta esta probabilidade para sequ√™ncias de comprimentos diferentes. Podemos verificar neste exemplo que, √† medida que a sequ√™ncia cresce, o conjunto das sequ√™ncias, que efetivamente possui uma probabilidade significativa, torna-se proporcionalmente menor.
    </p>
    <p>
    </p>
    <div class="row fig" id="f4">
     <a name="f4">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf4" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 4
      </strong>
      <br/>
      O gr√°fico ilustra a probabilidade de se obter uma sequ√™ncia com
      <i>
       k
      </i>
      ocorr√™ncias do 1 em um ensaio de Bernoulli utilizando
      <i>
       p
      </i>
      = 0,9. √â ilustrado os casos em que o n√∫mero de realiza√ß√µes √©
      <i>
       n
      </i>
      = 100,
      <i>
       n
      </i>
      = 250 e
      <i>
       n
      </i>
      = 1000.
      <br/>
     </div>
    </div>
    <p>
     Note que a entropia √© fator preponderante na determina√ß√£o do tamanho do conjunto t√≠pico, que √© tipicamente muito menor que o tamanho do conjunto de todas as sequ√™ncias. Se estivermos utilizando uma fonte com menor entropia, logo tamb√©m teremos um menor conjunto de sequ√™ncias t√≠picas. No caso da busca exaustiva por uma chave, devemos inici√°-la pelas sequ√™ncias do conjunto t√≠pico. Sabendo disso, para proteger os dados contra um ataque por for√ßa bruta, devemos buscar aumentar a entropia, pois, apesar de n√£o alterar o tamanho do conjunto de todas as sequ√™ncias para um dado tamanho, estaremos tornando o conjunto t√≠pico t√£o maior quanto for a entropia. No limite da entropia m√°xima, n√£o ser√° poss√≠vel resumir o espa√ßo de busca pelo conjunto t√≠pico.
    </p>
    <h1 class="articleSectionTitle">
     4. Diceware
    </h1>
    <p>
     O
     <i>
      Diceware
     </i>
     √© um m√©todo para gerar senha √† m√£o, utilizando apenas um dado de 6 lados e a lista de palavras disponibilizada pelo m√©todo. O m√©todo recomenda jogar o dado 5 vezes para cada palavra que ser√° utilizada na senha, ou seja, os 5 lances de dados ir√£o gerar um n√∫mero que corresponder√° ao √≠ndice de uma das 7.776 poss√≠veis palavras no dicion√°rio
     <i>
      Diceware
     </i>
     . O m√©todo recomenda a utiliza√ß√£o de 5 palavras para formar a
     <i>
      passphrase
     </i>
     , o que levaria a uma entropia de 64,62 bits na senha final.
    </p>
    <p>
     E importante ressaltar que o
     <i>
      Diceware
     </i>
     utiliza uma lista de palavras com comprimentos variando de 1 a 6 caracteres, conforme a
     <a class="open-asset-modal" data-target="#ModalTablet1" data-toggle="modal" href="">
      <span class="sci-ico-fileTable">
      </span>
      Tabela 1
     </a>
     . Desta forma, a senha final poder√° ter de 5 a 30 caracteres e o comprimento esperado ser√° de 21,16 caracteres.
    </p>
    <div class="row table" id="t1">
     <a name="t1">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalTablet1" data-toggle="modal">
       <div class="thumbOff">
        Thumbnail
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Tabela 1
      </strong>
      <br/>
      N√∫mero de palavras com determinado comprimento no
      <i>
       Diceware
      </i>
      .
      <br/>
     </div>
    </div>
    <p>
     A utiliza√ß√£o do
     <i>
      Diceware
     </i>
     traz consigo algumas desvantagens:
    </p>
    <div>
     <ol type="1">
      <li>
       <p>
        estaremos utilizando um vocabul√°rio restrito;
       </p>
      </li>
      <li>
       <p>
        muitas palavras da lista s√£o desconhecidas, o que torna dif√≠cil a memoriza√ß√£o pelo usu√°rio;
       </p>
      </li>
      <li>
       <p>
        a lista cont√©m n√∫meros e caracteres n√£o alfanum√©ricos, o que tamb√©m n√£o √© uma prefer√™ncia dos usu√°rios;
       </p>
      </li>
      <li>
       <p>
        o comprimento da senha pode ser utilizado como ind√≠cio para facilitar a busca.
       </p>
      </li>
     </ol>
    </div>
    <p>
     Conhecer o comprimento da senha reduz a entropia de uma certa quantidade, variando de 2,67 a 36,12 bits no total, com valor esperado de 3,35 bits. Este vazamento de entropia corresponderia a uma perda de 0,12 a 7,22 bits por caractere, e uma perda esperada de 0,16 bits por caractere. O vazamento de entropia √© definido como a diferen√ßa entre a entropia m√°xima (log
     <sub>
      2
     </sub>
     do n√∫mero de poss√≠veis senhas geradas pelo m√©todo) e a entropia remanescente, dado que o comprimento da senha √© conhecido (log
     <sub>
      2
     </sub>
     do n√∫mero de poss√≠veis senhas com um dado comprimento geradas pelo m√©todo). A
     <a class="open-asset-modal" data-target="#ModalFigf5" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 5
     </a>
     ilustra como o n√∫mero de senhas dispon√≠veis no
     <i>
      Diceware
     </i>
     varia em fun√ß√£o do comprimento final da senha (n√∫mero de caracteres). Vamos representar o conjunto de palavras no
     <i>
      Diceware
     </i>
     por W. Uma senha gerada pelo m√©todo
     <i>
      Diceware
     </i>
     √© constitu√≠da por uma sequ√™ncia de
     <i>
      n
     </i>
     = 5 palavras. Esta sequ√™ncia ser√° representada por
     <i>
      w
     </i>
     <sub>
      1:
      <i>
       n
      </i>
     </sub>
     = w
     <sub>
      1
     </sub>
     ,...,w
     <sub>
      n
     </sub>
     , onde cada um dos w
     <sub>
      i
     </sub>
     ‚àà ùí≤, para
     <i>
      i
     </i>
     =1,...,
     <i>
      n
     </i>
     . Iremos definir
     <i>
      N
     </i>
     (
     <i>
      w
     </i>
     <sub>
      1:n
     </sub>
     ) como o n√∫mero existente de sequ√™ncias de comprimento
     <i>
      n
     </i>
     . Definindo tamb√©m
     <i>
      l
     </i>
     (
     <i>
      w
     </i>
     <sub>
      1:n
     </sub>
     ) como o comprimento (em caracteres) de uma determinada sequ√™ncia, podemos escrever
     <i>
      N
     </i>
     (w
     <sub>
      1:n
     </sub>
     | l(w
     <sub>
      1:n
     </sub>
     )=
     <i>
      k
     </i>
     ), ou seja, o n√∫mero de sequ√™ncias formadas por
     <i>
      n
     </i>
     palavras, tais que o comprimento (em caracteres) da sequ√™ncia √© igual a um determinado valor
     <i>
      k
     </i>
     . O vazamento de entropia para sequ√™ncias com um determinado comprimento (em caracteres)
     <i>
      k
     </i>
     ser√° dado por
    </p>
    <div class="row formula" id="ee11">
     <a name="e11">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/4f9dcf10c36d39c799e24d711ba7541492c4e6fb.jpg" style="max-width:100%"/>
      </div>
     </div>
    </div>
    <p>
    </p>
    <div class="row fig" id="f5">
     <a name="f5">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf5" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 5
      </strong>
      <br/>
      N√∫mero de senhas no
      <i>
       Diceware
      </i>
      para um determinado comprimento de senha.
      <br/>
     </div>
    </div>
    <p>
     A
     <a class="open-asset-modal" data-target="#ModalTablet2" data-toggle="modal" href="">
      <span class="sci-ico-fileTable">
      </span>
      Tabela 2
     </a>
     apresenta os valores de vazamento de entropia calculados para os diferentes comprimentos da senha final.
    </p>
    <div class="row table" id="t2">
     <a name="t2">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalTablet2" data-toggle="modal">
       <div class="thumbOff">
        Thumbnail
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Tabela 2
      </strong>
      <br/>
      N√∫mero de palavras com determinado comprimento no
      <i>
       Diceware
      </i>
      .
      <br/>
     </div>
    </div>
    <p>
     E importante ainda observar que, independente do m√©todo utilizado para criar uma senha, conhecer o comprimento de uma senha sempre resultar√° em um vazamento de entropia.
    </p>
    <p>
     O m√©todo ainda prop√µe que sejam descartadas as senhas geradas com menos do que 14 caracteres, o que evitaria muita perda de entropia causada pelas
     <i>
      passphrases
     </i>
     curtas, restringindo pouco o espa√ßo de senhas, uma vez que apenas 0,65% delas seriam eliminadas. Entretanto o m√©todo n√£o prop√µe a exclus√£o das
     <i>
      passphrases
     </i>
     muito longas que tamb√©m s√£o respons√°veis por grande parte do vazamento de entropia
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       3
      </sup>
      <span class="refCtt closed">
       <span>
        [3] M.A. Carnut e E.C. Hora, in: Anais do Simp√≥sio de Seguran√ßa em Inform√°tica, editado por P.S.M. Pires e C.T. Fernandes, S√£o Jos√© dos Campos, 2005.
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <h1 class="articleSectionTitle">
     5. Base de dados
    </h1>
    <p>
     Para realiza√ß√£o das an√°lises aqui propostas √© necess√°ria a utiliza√ß√£o de uma base de dados extensa. Dados textuais podem ser facilmente obtidos e manipulados; al√©m disso, as unidades de processamento textuais s√£o bem definidas em letras e palavras, o que n√£o podemos afirmar quando desejamos analisar a l√≠ngua enquanto fen√¥meno falado
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       45
      </sup>
      <span class="refCtt closed">
       <span>
        [45] R. Port, New Ideas in Psychology 25 (2007).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     .
    </p>
    <p>
     Neste trabalho utilizamos a base de dados
     <i>
      Open American National Corpus
     </i>
     (OANC)
     <span class="ref footnote">
      <sup class="xref">
       5
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         5
        </strong>
        <a href="http://www.anc.org/OANC" target="_blank">
         http://www.anc.org/OANC
        </a>
       </span>
      </span>
     </span>
     . Esta trata se de um corpus com anota√ß√µes constitu√≠do por 15 milh√µes de palavras e dispon√≠veis livremente no site da ANC para qualquer utiliza√ß√£o. O corpus cont√©m textos de diversos g√™neros e transcri√ß√µes de fala produzidas a partir de 1990. Os mesmos dados tamb√©m est√£o dispon√≠veis em outros formatos, como por exemplo um formato XML, contendo anota√ß√µes, tais como marca√ß√£o estrutural, limite de senten√ßas, classe gramatical e identifica√ß√£o de constituintes, dentre outras. Todas as anota√ß√£o utilizam o padr√£o ISO/TC37 SC4.
    </p>
    <p>
     As raz√µes pelas quais utiliza-se corpora diversos para efetuar an√°lises, assim como aqui realizamos, s√£o:
    </p>
    <div>
     <ol type="1">
      <li>
       <p>
        facilidade na obten√ß√£o dos dados, uma vez que estes j√° foram reunidos e organizados;
       </p>
      </li>
      <li>
       <p>
        os corpus s√£o criados de forma a possu√≠rem uma boa representa√ß√£o de uma determinada l√≠ngua, pois s√£o uma composi√ß√£o balanceada de textos de diferentes estilos;
       </p>
      </li>
      <li>
       <p>
        a utiliza√ß√£o de corpus permite a compara√ß√£o de resultados no meio acad√™mico;
       </p>
      </li>
      <li>
       <p>
        muitos corpus possuem anota√ß√µes lexicais e gramaticais;
       </p>
      </li>
      <li>
       <p>
        podem ser utilizados para analisar as altera√ß√µes temporais sofrida por uma l√≠ngua.
       </p>
      </li>
     </ol>
    </div>
    <p>
     Existem diversos corpora para a l√≠ngua portuguesa. Ao leitor interessado, recomendamos que consultem a L√≠nguateca
     <span class="ref footnote">
      <sup class="xref">
       6
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         6
        </strong>
        <a href="http://www.linguateca.pt/" target="_blank">
         http://www.linguateca.pt/
        </a>
       </span>
      </span>
     </span>
     , um centro de recursos distribu√≠dos para a l√≠ngua portuguesa.
    </p>
    <h1 class="articleSectionTitle">
     6. Resultados
    </h1>
    <p>
     Nas
     <a class="open-asset-modal" data-target="#ModalFigf1" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Figs. 1
     </a>
     e
     <a class="open-asset-modal" data-target="#ModalFigf2" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      2
     </a>
     podemos observar que a distribui√ß√£o dos s√≠mbolos (palavras ou letras) no ingl√™s n√£o √© uniforme. Existe uma evidente prefer√™ncia pela utiliza√ß√£o de alguns tipos, em detrimento de outros. Este efeito √© ainda mais acentuado quando analisamos a frequ√™ncia de ocorr√™ncia de palavras, ilustrada na
     <a class="open-asset-modal" data-target="#ModalFigf1" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Figura 1
     </a>
     . Esta caracter√≠stica √© comum √†s linguagens naturais e faz com que a aleatoriedade em uma l√≠ngua seja diminu√≠da. Considerando a l√≠ngua enquanto meio de comunica√ß√£o, redund√¢ncia √© importante pois permite que erros inseridos durante a transmiss√£o ou problemas de recep√ß√£o sejam corrigidos, garantindo assim a efici√™ncia da transmiss√£o de informa√ß√£o. Por outro lado, do ponto de vista da criptografia, redund√¢ncia √© algo indesej√°vel.
    </p>
    <p>
     Uma an√°lise lingu√≠stica pode ser feita sob duas perspectivas: sincr√¥nica ou diacr√¥nica. A investiga√ß√£o sincr√¥nica lida com a l√≠ngua como um fen√¥meno fixo em um determinado instante, enquanto a abordagem diacr√¥nica investiga a evolu√ß√£o da l√≠ngua no tempo Esta distin√ß√£o foi introduzida por Ferdinand de Saussure no in√≠cio do S√©culo XX
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       46
      </sup>
      <span class="refCtt closed">
       <span>
        [46] F. de Saussure, Curso de lingu√≠stica Geral (Editora Cultrix, S√£o Paulo, 1916).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . Embora seja incontest√°vel que l√≠nguas mudam ao longo do tempo, iremos considerar que, em uma an√°lise sincr√¥nica, suas caracter√≠sticas n√£o se alteram drasticamente e podem ser encontradas em qualquer amostra representativa de uma l√≠ngua, em um determinado instante
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       47
      </sup>
      <span class="refCtt closed">
       <span>
        [47] C. Bowern and B. Evans, The Routledge Handbook of Historical Linguistics (Taylor &amp; Francis, New York, 2015).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     . A
     <a class="open-asset-modal" data-target="#ModalFigf2" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 2
     </a>
     apresenta a probabilidade de ocorr√™ncia dos caracteres no ingl√™s escrito. Podemos verificar que a variabilidade destas probabilidades ao longo do
     <i>
      corpus
     </i>
     √© baixa, iremos assim considerar as probabilidades constantes.
    </p>
    <p>
     A
     <a class="open-asset-modal" data-target="#ModalTablet3" data-toggle="modal" href="">
      <span class="sci-ico-fileTable">
      </span>
      Tabela 3
     </a>
     apresenta as seguintes estat√≠sticas com rela√ß√£o ao ingl√™s escrito e ao ingl√™s falado:
    </p>
    <div class="row table" id="t3">
     <a name="t3">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalTablet3" data-toggle="modal">
       <div class="thumbOff">
        Thumbnail
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Tabela 3
      </strong>
      <br/>
      Compara√ß√£o realizada entre o Ingl√™s escrito e falado.
      <br/>
     </div>
    </div>
    <div>
     <ol type="1">
      <li>
       <p>
        o tamanho do vocabul√°rio |ùìß| (n√∫mero de palavras);
       </p>
      </li>
      <li>
       <p>
        a entropia m√°xima log |ùìß| (considerando distribui√ß√£o uniforme);
       </p>
      </li>
      <li>
       <p>
        a entropia quando consideramos palavras como s√≠mbolos
        <i>
         H
        </i>
        <sub>
         p
        </sub>
        ;
       </p>
      </li>
      <li>
       <p>
        o comprimento esperado das palavras
        <i>
         L
        </i>
        <sub>
         p
        </sub>
        ;
       </p>
      </li>
      <li>
       <p>
        a entropia por caractere
        <i>
         H
        </i>
        <sub>
         p
        </sub>
        /
        <i>
         L
        </i>
        <sub>
         p
        </sub>
        (quando consideramos as palavras como s√≠mbolos);
       </p>
      </li>
      <li>
       <p>
        a entropia quando consideramos os caracteres como s√≠mbolos
        <i>
         H
        </i>
        <sub>
         c
        </sub>
        ;
       </p>
      </li>
      <li>
       <p>
        e, por fim, a entropia do primeiro caractere das palavras
        <i>
         H
        </i>
        <sub>
         pc
        </sub>
        .
       </p>
      </li>
     </ol>
    </div>
    <p>
     Para calcular
     <i>
      H
     </i>
     <sub>
      p
     </sub>
     devemos contabilizar o n√∫mero de ocorr√™ncias de cada palavra no corpus, para ent√£o estimar suas probabilidades, necess√°rias para calcular a entropia. Procedemos de forma semelhante para calcular
     <i>
      H
     </i>
     <sub>
      c
     </sub>
     e
     <i>
      H
     </i>
     <sub>
      pc
     </sub>
     , ou seja, devemos contabilizar a frequ√™ncia de ocorr√™ncia dos caracteres (ou apenas o primeiro caractere de cada palavra, no caso do c√°lculo de
     <i>
      H
      <sub>
       pc
      </sub>
     </i>
     ), e ent√£o estimar as probabilidades. Para calcular
     <i>
      L
     </i>
     <sub>
      p
     </sub>
     devemos obter o comprimento das palavras e ent√£o computar
     <i>
      p
      <sub>
       i
      </sub>
     </i>
     e
     <i>
      l
      <sub>
       i
      </sub>
     </i>
     representam a probabilidade e o comprimento da
     <i>
      i
     </i>
     -√©sima palavra.
     <img src="https://minio.scielo.br/documentstore/1806-9126/MzrrZdc7bydG4qSvJxqXnkR/386765cef3704ba613b711c339417b79fc8b30d8.jpg" style="max-width:100%"/>
     , onde
    </p>
    <p>
     A utiliza√ß√£o de palavras ou
     <i>
      passphrases
     </i>
     fornecer√° aproximadamente 2,3 bits por caractere de entropia. Para o alfabeto de 26 caracteres, o valor m√°ximo ser√° de 4,7 bits por caractere. Com a utiliza√ß√£o do
     <i>
      Diceware
     </i>
     teremos log 7776 = 12, 92 bits por palavra, como o comprimento esperado das palavras no
     <i>
      Diceware
     </i>
     √© 5,24 caracteres, ter√≠amos ent√£o 2,47 bits por caractere de entropia, entretanto a distribui√ß√£o dos caracteres no
     <i>
      Diceware
     </i>
     n√£o √© uniforme e, desta forma, a real entropia por caractere √© 4,39 bits. Todavia, devemos lembrar que o
     <i>
      Diceware
     </i>
     utiliza um alfabeto com 54 s√≠mbolos. Para melhor compararmos os resultados, devemos normalizar para um alfabeto com 26 s√≠mbolos. Realizando esta normaliza√ß√£o obteremos uma incerteza de 3,58 bits por caractere agregados por cada letra na sequ√™ncia.
    </p>
    <p>
     Este valor n√£o difere muito do valor anterior obtido para a incerteza agregada por cada caractere acrescido em uma sequ√™ncia. Como podemos verificar na
     <a class="open-asset-modal" data-target="#ModalTablet3" data-toggle="modal" href="">
      <span class="sci-ico-fileTable">
      </span>
      Tabela 3
     </a>
     , o valor da entropia por caractere Hc √© praticamente o dobro dos valores calculados nos dois casos anteriores. A melhor estrat√©gia seria sortear letras utilizando uma distribui√ß√£o uniforme e assegurar assim o m√°ximo de 4,7 bits por caractere. Uma outra estrat√©gia seria sortear letras de um texto, desta forma as letras ser√£o escolhidas conforme a sua distribui√ß√£o na l√≠ngua. Esta estrat√©gia fornecer√° 4,19 bits de entropia por caractere, valor bem mais pr√≥ximo do m√°ximo; entretanto, as sequ√™ncias formadas ser√£o de dif√≠cil memoriza√ß√£o. Uma ultima estrat√©gia, visando maximizar a entropia por caractere, sem criar grandes dificuldades em sua memoriza√ß√£o, consiste em utilizar acr√¥nimos. Memorizar uma frase com 10, 15 ou 20 palavras n√£o √© t√£o dif√≠cil quanto memorizar uma sequ√™ncia de 10, 15 ou 20 letras aleat√≥rias. A t√≠tulo de exemplo, podemos criar a frase: "N√£o posso afirmar que a Margaret Thatcher ou sequer a Dilma sejam sexys como certa vez sugeriu Edward Snowden para exemplificar uma senha dif√≠cil de ser quebrada", que nos geraria a seguinte senha "npaqamtosadssccvsespeusddsq". Esta ultima estrat√©gia garante um acr√©scimo de 4,14 bits de incerteza por caractere adicionado √† sequ√™ncia, sendo ent√£o, sob esta an√°lise, a melhor estrat√©gia quando existe uma rela√ß√£o de compromisso entre maximiza√ß√£o da entropia e facilidade de memoriza√ß√£o da senha.
    </p>
    <h1 class="articleSectionTitle">
     7. Conclus√µes
    </h1>
    <p>
     Para dificultar o sucesso de um ataque de for√ßa bruta √© importante que a escolha da chave seja feita de forma a maximizar o espa√ßo de chaves que dever√° ser percorrido na busca exaustiva realizada pelo agressor. Muitas vezes os sistemas restringem o comprimento da chave, por exemplo, a Microsoft restringe a 16 caracteres, muitas lojas de com√©rcio eletr√¥nico tamb√©m restringem drasticamente o n√∫mero de caracteres de uma senha. Lojas virtuais como Submarino e Americanas.com utilizam o m√°ximo de 8 caracteres, enquanto Netshoes utiliza 15 e, ainda, Mercado Livre, 20. Mais grave ainda s√£o os bancos que, al√©m de restringir o tamanho de sua senha, restringem tamb√©m o alfabeto, aceitando apenas d√≠gitos de 0 a 9. Algumas empresas fornecem um limite muito maior, como √© o caso da Amazon (128 caracteres) e Google (100 caracteres).
    </p>
    <p>
     Um m√©todo para criar senhas deve ser pautado por uma rela√ß√£o de compromisso entre:
    </p>
    <div>
     <ol type="1">
      <li>
       <p>
        usabilidade por um leigo;
       </p>
      </li>
      <li>
       <p>
        seguran√ßa computacional.
       </p>
      </li>
     </ol>
    </div>
    <p>
     Obedecendo ent√£o √†s usuais e severas restri√ß√µes no comprimento de uma senha, e como n√£o desejamos utilizar senhas muito longas que demandariam esfor√ßo e tempo para digit√°-las, devemos buscar maximizar a entropia por caractere. Sob esta perspectiva, foram analisadas 4 diferentes estrat√©gias para se criar senhas, observando a quantidade de entropia por caractere que cada uma delas fornece. Os resultados obtidos foram os seguintes:
    </p>
    <div>
     <ol type="1">
      <li>
       <p>
        a utiliza√ß√£o de simples palavras fornecem aproximadamente 2,3 bits por caractere;
       </p>
      </li>
      <li>
       <p>
        ousode
        <i>
         passphrases
        </i>
        n√£o apresenta nenhum ganho sobre a anterior, uma vez de que trata-se de uma simples concatena√ß√£o de palavras, n√£o alterando, desta forma, a incerteza associada por caractere agregado na sequ√™ncia;
       </p>
      </li>
      <li>
       <p>
        o m√©todo
        <i>
         Diceware
        </i>
        fornecer√° 3,8 bits por caractere;
       </p>
      </li>
      <li>
       <p>
        a utiliza√ß√£o de acr√¥nimos fornecer√° 4,19 bits por caractere, o melhor resultado dentre aqueles analisado neste trabalho.
       </p>
      </li>
     </ol>
    </div>
    <p>
     Os resultados mostram que a melhor rela√ß√£o de compromisso entre maximiza√ß√£o da entropia por caractere e facilidade de memoriza√ß√£o ser√° obtida ao utilizarmos a √∫ltima estrat√©gia, estaremos assim a apenas 0,56 bits da entropia m√°xima. Poderemos melhorar ainda esta estrat√©gia se utilizarmos tamb√©m algarismos e caracteres n√£o-alfanum√©ricos.
    </p>
    <p>
     Ao empregarmos palavras de uma l√≠ngua para constituir uma senha, estaremos pagando um alto pre√ßo que incorre da redund√¢ncia existente nas l√≠nguas. A lei de Zipf √© observada nas linguagens naturais e, como consequ√™ncia, teremos uma dr√°stica redu√ß√£o da entropia. Aplicamos o m√©todo proposto em
     <sup>
      [
     </sup>
     <span class="ref">
      <sup class="xref xrefblue">
       39
      </sup>
      <span class="refCtt closed">
       <span>
        [39] L.C. Araujo, H.C. Yehia and T. Crist√≥faro-Silva, Glottometrics 26, 38 (2013).
       </span>
      </span>
     </span>
     <sup>
      ]
     </sup>
     para estimar a entropia para uma distribui√ß√£o zipfiana. A partir das estimativas para diferentes expoentes caracter√≠sticos e diferentes tamanhos de alfabeto, foi poss√≠vel tra√ßar o gr√°fico ilus trado na
     <a class="open-asset-modal" data-target="#ModalFigf6" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 6
     </a>
     . Este evidencia o fator de redu√ß√£o da incerteza ocasionada pela distribui√ß√£o de Zipf. Observamos que em todas as situa√ß√µes, a entropia √© reduzida quando existe uma distribui√ß√£o zipfiana. Para as distribui√ß√µes apresentadas com
     <i>
      s
     </i>
     ‚â• 1, verifica-se que a redu√ß√£o √© mais acentuada com o crescimento do tamanho do conjunto de s√≠mbolos |ùìß| e com o crescimento do expoente
     <i>
      s
     </i>
     . As l√≠nguas naturais apresentam
     <i>
      s
     </i>
     ‚âà 1 e assim experienciamos uma diminui√ß√£o de, no m√≠nimo, 20% na entropia. Esta diminui√ß√£o √© grande o suficiente para assegurar que teremos um conjunto t√≠pico que ser√° constitu√≠do por uma fra√ß√£o exponencialmente √≠nfima do todo, quando
     <i>
      n
     </i>
     for suficientemente grande.
    </p>
    <p>
    </p>
    <div class="row fig" id="f6">
     <a name="f6">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf6" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 6
      </strong>
      <br/>
      Fator de redu√ß√£o da entropia causado pela lei de Zipf.
      <br/>
     </div>
    </div>
   </div>
   <div>
    <h1>
    </h1>
    <div class="ref-list">
     <ul class="refList footnote">
      <li>
       <span class="xref big">
        1
       </span>
       <div>
        Espa√ßo de chaves √© o conjunto formado por todas as poss√≠veis chaves. O tamanho deste conjunto √© determinado pelo comprimento m√°ximo das chaves e pelo tamanho do alfabeto utilizado. Se
        <i>
         C
        </i>
        √© o espa√ßo de chaves, ùìß √© o alfabeto e
        <i>
         n
        </i>
        o comprimento m√°ximo das chaves, ent√£o |C| = |ùìß|
        <sup>
         <i>
          n
         </i>
        </sup>
        .
       </div>
      </li>
      <li>
       <span class="xref big">
        2
       </span>
       <div>
        Uma fun√ß√£o
        <i>
         hash
        </i>
        √© uma fun√ß√£o determin√≠stica que realiza o mapeamento de um conjunto de v√°rios (ou at√© mesmo infinitos) membros em valores de um conjunto com um n√∫mero fixo de membros. Uma fun√ß√£o
        <i>
         hash
        </i>
        possui a propriedade de n√£o ser revers√≠vel, f√°cil de ser calculada e, al√©m disso, √© dif√≠cil modificar um membro do dom√≠nio sem gerar um valor de
        <i>
         hash
        </i>
        distinto para ele, √© ainda improv√°vel encontrar dois membros do dom√≠nio que possuam o mesmo mapeamento pela fun√ß√£o
        <i>
         hash
        </i>
        . S√£o exemplo conhecidos de fun√ß√£o
        <i>
         hash
        </i>
        : MD5, SHA1 e SHA2.
       </div>
      </li>
      <li>
       <span class="xref big">
        3
       </span>
       <div>
        A ordem dos constituintes de uma senten√ßa √© um crit√©rio tipol√≥gico para a classifica√ß√£o das l√≠nguas de acordo com sua sintaxe.
       </div>
      </li>
      <li>
       <span class="xref big">
        4
       </span>
       <div>
        Ensaio de Bernoulli √© um experimento aleat√≥rio, no qual apenas dois resultados s√£o poss√≠veis: verdadeiro (1) ou falso (0).
       </div>
      </li>
      <li>
       <span class="xref big">
        5
       </span>
       <div>
        <a href="http://www.anc.org/OANC" target="_blank">
         http://www.anc.org/OANC
        </a>
       </div>
      </li>
      <li>
       <span class="xref big">
        6
       </span>
       <div>
        <a href="http://www.linguateca.pt/" target="_blank">
         http://www.linguateca.pt/
        </a>
       </div>
      </li>
     </ul>
    </div>
   </div>
   <div class="articleSection" data-anchor="Refer√™ncias">
    <h1 class="articleSectionTitle">
     Refer√™ncias
    </h1>
    <div class="ref-list">
     <ul class="refList">
      <li>
       <sup class="xref big">
        [1]
       </sup>
       <div>
        Dictionary.com Unabridged, Online Dictionary (2015). disponivel em http://dictionary.reference.com/browse/cryptography
        <br/>
        <a href="http://Dictionary.com" target="_blank">
         ¬ª Dictionary.com
        </a>
        <a href="http://dictionary.reference.com/browse/cryptography" target="_blank">
         ¬ª http://dictionary.reference.com/browse/cryptography
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [2]
       </sup>
       <div>
        D. Kahn,
        <i>
         The Codebreakers: The Comprehensive History of Secret Communication from Ancient Times to the Internet
        </i>
        (Macmillan, New York, 1967).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [3]
       </sup>
       <div>
        M.A. Carnut e E.C. Hora, in:
        <i>
         Anais do Simp√≥sio de Seguran√ßa em Inform√°tica
        </i>
        , editado por P.S.M. Pires e C.T. Fernandes, S√£o Jos√© dos Campos, 2005.
       </div>
      </li>
      <li>
       <sup class="xref big">
        [4]
       </sup>
       <div>
        K.-W. Lee and H.-T. Ewe, in:
        <i>
         International Conference on Computational Intelligence and Security
        </i>
        , Guangzhou, edited by Y. Cheung, Y. Wang and H. Liu (The Institute of Electrical and Electronics Engineers, Piscataway, 2006).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [5]
       </sup>
       <div>
        B. Schneier,
        <i>
         Applied Cryptography: Protocols, Algorithms, and Source Code in C
        </i>
        , (John Wiley &amp; Sons, New York, 1996), 2nd ed.
       </div>
      </li>
      <li>
       <sup class="xref big">
        [6]
       </sup>
       <div>
        J. Oliver,
        <i>
         Last Week Tonight with John Oliver: Government Surveillance
        </i>
        (HBO, 2015), disponivel em https://youtu.be/XEVlyP4_11M
        <br/>
        <a href="https://youtu.be/XEVlyP4_11M" target="_blank">
         ¬ª https://youtu.be/XEVlyP4_11M
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [7]
       </sup>
       <div>
        A.G. Reinhold, The Diceware Passphrase Home Page (1995), disponivel em http://world.std.com/reinhold/diceware.html
        <br/>
        <a href="http://world.std.com/reinhold/diceware.html" target="_blank">
         ¬ª http://world.std.com/reinhold/diceware.html
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [8]
       </sup>
       <div>
        J. Bonneau, C. Herley, P.C. van Oorschot and F. Stajano, in:
        <i>
         Proc. IEEE Symposium on Security and Privacy
        </i>
        , San Francisco, 2012, editado por P. Kellenberger (Institute of Electrical and Electronics Engineers Inc., Piscataway, 2012), p. 553.
       </div>
      </li>
      <li>
       <sup class="xref big">
        [9]
       </sup>
       <div>
        N. Lee, Yahoo mail drops passwords and adds thirdparty email support for new apps (2015), disponivel em http://www.engadget.com/2015/10/15/yahoo-mail-update/
        <br/>
        <a href="http://www.engadget.com/2015/10/15/yahoo-mail-update/" target="_blank">
         ¬ª http://www.engadget.com/2015/10/15/yahoo-mail-update/
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [10]
       </sup>
       <div>
        E. Grosse and M. Upadhyay, IEEE Computer and Reliability Societies
        <b>
         11
        </b>
        , 1 (2013).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [11]
       </sup>
       <div>
        Wikipedia, Data breach: Major incidents (2015), disponivel em https://en.wikipedia.org/wiki/ Data_breach, (acessada em 20 de Outubro de 2015).
        <br/>
        <a href="https://en.wikipedia.org/wiki/%20Data_breach" target="_blank">
         ¬ª https://en.wikipedia.org/wiki/ Data_breach
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [12]
       </sup>
       <div>
        J. Ryall, Social media goes wild over massive celebrity nude photo leak (2014), disponivel em http://mashable.com/2014/08/31
        <br/>
        <a href="http://mashable.com/2014/08/31" target="_blank">
         ¬ª http://mashable.com/2014/08/31
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [13]
       </sup>
       <div>
        N. Kerris and T. Muller,
        <i>
         Update to Celebrity Photo Investigation
        </i>
        (2014), disponivel em https://www.apple.com/pr/library/2014/09/02Apple-Media-Advisory.html
        <br/>
        <a href="https://www.apple.com/pr/library/2014/09/02Apple-Media-Advisory.html" target="_blank">
         ¬ª https://www.apple.com/pr/library/2014/09/02Apple-Media-Advisory.html
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [14]
       </sup>
       <div>
        M. Zviran and W.J. Haga, Journal of Management Information Systems
        <b>
         15
        </b>
        , 4 (1999).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [15]
       </sup>
       <div>
        R. Munroe, Password strength (2011), disponivel em https://xkcd.com/936/
        <br/>
        <a href="https://xkcd.com/936/" target="_blank">
         ¬ª https://xkcd.com/936/
        </a>
       </div>
      </li>
      <li>
       <sup class="xref big">
        [16]
       </sup>
       <div>
        C.E. Shannon, Bell Systems Technical Journal
        <b>
         27
        </b>
        , 3 (1948).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [17]
       </sup>
       <div>
        C.E. Shannon, Bell Systems Technical Journal
        <b>
         28
        </b>
        , 4 (1949).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [18]
       </sup>
       <div>
        S.W. Golomb, E. Berlekamp, T.M. Cover, R.G. Gallager, J.L. Massey and A.J. Viterbi, Notices of the American Mathematical Society
        <b>
         49
        </b>
        , 1 (2002).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [19]
       </sup>
       <div>
        A. Kerckhoffs, Journal des Sciences Militaires
        <b>
         9
        </b>
        (1883).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [20]
       </sup>
       <div>
        R. Landauer, IBM J. Res. Dev.
        <b>
         5
        </b>
        , 3 (1961).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [21]
       </sup>
       <div>
        A. B√©rut, A. Arakelyan, A. Petrosyan, S. Ciliberto, R. Dillenschneider and E. Lutz, Nature
        <b>
         483
        </b>
        , 7388 (2012).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [22]
       </sup>
       <div>
        M.A. Montemurro and D.H. Zanette, PLoS ONE
        <b>
         6
        </b>
        , e19875 (2011).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [23]
       </sup>
       <div>
        G.K. Zipf,
        <i>
         Human Behaviour and the Principle of Least Effort: An Introduction to Human Ecology
        </i>
        (Addison-Wesley Press Inc., Cambridge, 1949).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [24]
       </sup>
       <div>
        S. Abe, N. Suzuki, Physica A: Statistical Mechanics and its Applications
        <b>
         350
        </b>
        , 2 (2005).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [25]
       </sup>
       <div>
        X. Gabaix, Quarterly Journal of Economics
        <b>
         114
        </b>
        ,3 (1999).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [26]
       </sup>
       <div>
        B. Mandelbrot, The Journal of Business
        <b>
         36
        </b>
        , 4 (1963).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [27]
       </sup>
       <div>
        C. Furusawa and K. Kaneko, Physical Review Letters
        <b>
         90
        </b>
        , 8 (2003).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [28]
       </sup>
       <div>
        G. Nicolis, C. Nicolis and J.S. Nicolis, Journal of Statistical Physics
        <b>
         54
        </b>
        , 3(1989).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [29]
       </sup>
       <div>
        P. Bak,
        <i>
         How Nature Works: The Science of Selforganized Criticality
        </i>
        (Copernicus, New York, 1996).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [30]
       </sup>
       <div>
        M.E. Crovella and A. Bestavros, in:
        <i>
         Proceedings of the 1996 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems
        </i>
        Philadelphia, 1996, edited by B.D. Gaither (ACM, New York, 1996).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [31]
       </sup>
       <div>
        D.J. de Solla Price, Science
        <b>
         149
        </b>
        , 3683 (1965).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [32]
       </sup>
       <div>
        R. Kohli and R.K. Sah, Management Science
        <b>
         52
        </b>
        , 11 (2003).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [33]
       </sup>
       <div>
        R.A.K. Cox, J.M. Felton and K.C. Chung, Journal of Cultural Economics
        <b>
         19
        </b>
        (1995).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [34]
       </sup>
       <div>
        M.E.J. Newman, Contemporary Physics
        <b>
         46
        </b>
        , 5 (2005).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [35]
       </sup>
       <div>
        D. Malone and K. Maher, in:
        <i>
         Proceedings of the 21st International Conference on World Wide Web
        </i>
        , Lyon, 2012 (ACM, New York, 2012).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [36]
       </sup>
       <div>
        D. Wang, G. Jian, X. Huang and P. Wang, in:
        <i>
         Transactions on Information and System Security
        </i>
        (ACM, New York, 2015), volume 1.
       </div>
      </li>
      <li>
       <sup class="xref big">
        [37]
       </sup>
       <div>
        L.L. Gon√ßalves and L.B. Gon√ßalves, Physica A
        <b>
         360
        </b>
        , 2 (2006).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [38]
       </sup>
       <div>
        R.G. Piotrovskii, V.E. Pashkovskii and V.R. Piotrovskii, Nauchno-Tekhnicheskaya Informatsiya
        <b>
         28
        </b>
        , 11 (1994).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [39]
       </sup>
       <div>
        L.C. Araujo, H.C. Yehia and T. Crist√≥faro-Silva, Glottometrics
        <b>
         26
        </b>
        , 38 (2013).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [40]
       </sup>
       <div>
        R. Ferrer-i-Cancho, Proceedings of the National Academy of Sciences of the United States of America
        <b>
         100
        </b>
        , 3 (2003).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [41]
       </sup>
       <div>
        C.E. Shannon, Bell Systems Technical Journal
        <b>
         30
        </b>
        , 1 (1951).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [42]
       </sup>
       <div>
        T.M. Cover and R.C. King, IEEE Transactions on Information Theory
        <b>
         24
        </b>
        , 4 (1978).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [43]
       </sup>
       <div>
        W.J. Teahan and J.G. Cleary, in:
        <i>
         Data Compression Conference
        </i>
        , Snowbird, 1996, edited by J.A. Storer and M. Cohn (IEEE Computer Society Press, 1996), p. 53.
       </div>
      </li>
      <li>
       <sup class="xref big">
        [44]
       </sup>
       <div>
        J.A. Thomas and T.M. Cover,
        <i>
         Elements of Information Theory
        </i>
        (Wiley, Hoboken, 1991), 2nd edition.
       </div>
      </li>
      <li>
       <sup class="xref big">
        [45]
       </sup>
       <div>
        R. Port, New Ideas in Psychology
        <b>
         25
        </b>
        (2007).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [46]
       </sup>
       <div>
        F. de Saussure,
        <i>
         Curso de lingu√≠stica Geral
        </i>
        (Editora Cultrix, S√£o Paulo, 1916).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [47]
       </sup>
       <div>
        C. Bowern and B. Evans,
        <i>
         The Routledge Handbook of Historical Linguistics
        </i>
        (Taylor &amp; Francis, New York, 2015).
       </div>
      </li>
     </ul>
    </div>
   </div>
   <div class="articleSection" data-anchor="Datas de Publica√ß√£o ">
    <h1 class="articleSectionTitle">
     Datas de Publica√ß√£o
    </h1>
    <div class="row">
     <div class="col-md-12 col-sm-12">
      <ul class="articleTimeline">
       <li>
        <strong>
         Publica√ß√£o nesta cole√ß√£o
        </strong>
        <br/>
        Mar¬†2016
       </li>
      </ul>
     </div>
    </div>
   </div>
   <div class="articleSection" data-anchor="Hist√≥rico">
    <h1 class="articleSectionTitle">
     Hist√≥rico
    </h1>
    <div class="row">
     <div class="col-md-12 col-sm-12">
      <ul class="articleTimeline">
       <li>
        <strong>
         Recebido
        </strong>
        <br/>
        05¬†Set¬†2015
       </li>
       <li>
        <strong>
         Aceito
        </strong>
        <br/>
        05¬†Dez¬†2015
       </li>
      </ul>
     </div>
    </div>
   </div>
   <section class="documentLicense">
    <div class="container-license">
     <div class="row">
      <div class="col-sm-3 col-md-2">
       <a href="http://creativecommons.org/licenses/by/4.0" target="_blank" title="">
        <img alt="Creative Common - by 4.0" src="https://licensebuttons.net/l/by/4.0/88x31.png"/>
       </a>
      </div>
      <div class="col-sm-9 col-md-10">
       <a href="http://creativecommons.org/licenses/by/4.0" target="_blank" title="">
        Este √© um artigo publicado em acesso aberto (Open Access) sob a licen√ßa Creative Commons Attribution, que permite uso, distribui√ß√£o e reprodu√ß√£o em qualquer meio, sem restri√ß√µes desde que o trabalho original seja corretamente citado.
       </a>
      </div>
     </div>
    </div>
   </section>
  </article>
 </div>
</div>