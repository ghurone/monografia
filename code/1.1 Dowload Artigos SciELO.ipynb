{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Download Artigos SciELO\n",
    "\n",
    "Neste notebook é possível fazer download dos artigos da RBEF que podem ser encontrados no site de [SciELO](https://www.scielo.br/j/rbef/). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functions import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request()  # objeto de requisição"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scielo_url = lambda x: urljoin('https://www.scielo.br', x)  # Função geradora de urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volumes_por_ano() -> dict[int:list]:\n",
    "    \"\"\"\n",
    "        Função que pega o link de cada volume da revista por ano.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicionário onde as chaves é o ano e os valores os links dos volumes do respectivo ano.\n",
    "    \"\"\"\n",
    "    req = request.get(scielo_url('j/rbef/grid'))\n",
    "    soup = BeautifulSoup(req.text, 'lxml', parse_only=SoupStrainer('tbody'))\n",
    "\n",
    "    links = {int(linha.td.text): [scielo_url(a['href']) for a in linha.find_all('a')] for linha in soup.find_all('tr')}\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artigos_por_volume(url: str, formato: str = 'html', idioma: str = 'pt') -> list[str]:\n",
    "    \"\"\"\n",
    "        Função que coleta o link de todos os artigos presentes no volume.\n",
    "\n",
    "    Args:\n",
    "        url (str): url do volume\n",
    "        formato (str, optional): Formato dos artigos. Defaults to 'html'.\n",
    "        idioma (str, optional): Idioma dos artigos. Defaults to 'pt'.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista com os links dos artigos deste volume.\n",
    "    \"\"\"\n",
    "    languages = {'pt': 'Português', 'en': 'Inglês', 'es': 'Espanhol'}\n",
    "    formatos = {'html': '/?lang=', 'pdf': '/?format=pdf&'}\n",
    "\n",
    "    lang = languages.get(idioma.lower())\n",
    "    form = formatos.get(formato.lower())\n",
    "\n",
    "    req = request.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(req.text, 'lxml', parse_only=SoupStrainer('a', attrs={'title': lang}))\n",
    "\n",
    "    return [scielo_url(a['href']) for a in soup.find_all('a') if 'abstract' not in a['href'] and form in a['href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_do_artigo(url: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "        Função que recebe uma url de um artigo e retorna uma string do seu html.\n",
    "\n",
    "    Args:\n",
    "        url (str): Url do artigo\n",
    "\n",
    "    Returns:\n",
    "        str: Retorna uma string do html da página do artigo.\n",
    "    \"\"\"\n",
    "    req = request.get(url)\n",
    "    \n",
    "    strainer = SoupStrainer('div', attrs={'class': 'articleTxt'})\n",
    "    \n",
    "    return BeautifulSoup(req.text, 'lxml', parse_only=strainer).prettify()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código\n",
    "\n",
    "Nesta parte do notebook consiste no código de download dos artigos que estão no site da SciELO e que são da língua portugesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = volumes_por_ano() \n",
    "del volumes[2023]  # remove 2023 dos volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pasta `artigos_brutos` já existe!\n",
      "A pasta `artigos_brutos\\SciELO` já existe!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'artigos_brutos')) # cria a pasta dos artigos brutos\n",
    "except FileExistsError:\n",
    "    print('A pasta `artigos_brutos` já existe!')\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'artigos_brutos', 'SciELO')) # cria a pasta da SciELO dentro dos artigos brutos\n",
    "except FileExistsError:\n",
    "    print('A pasta `artigos_brutos\\SciELO` já existe!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'artigos_brutos', 'SciELO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anos: 100%|██████████| 22/22 [18:02<00:00, 49.22s/it]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for ano, urls in tqdm(volumes.items(), desc='Anos'):\n",
    "    for volume_url in urls:\n",
    "        todos = artigos_por_volume(volume_url)\n",
    "        for artigo_url in todos:\n",
    "            nome_arquivo = str(n).rjust(4, \"0\") + '.html'\n",
    "            path_save = os.path.join(path, nome_arquivo)\n",
    "            \n",
    "            with open(path_save, 'w', encoding='utf8') as file:\n",
    "                file.write(html_do_artigo(artigo_url))\n",
    "            \n",
    "            n += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de fazer o download dos arquivos e salvá-los em uma pasta, podemos criar um Dataset para facilitar a navegação pelo conteúdo.\n",
    "\n",
    "Após a criação do dataset esses arquivos brutos são apagados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monografia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e668fd8da83d294f882a9b1881bcdf4fad9aa355d0eef119f9c689d241b75da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
