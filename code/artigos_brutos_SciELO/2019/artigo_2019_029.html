<!DOCTYPE html>
<div class="articleTxt">
 <div class="articleBadge-editionMeta-doi-copyLink">
  <span class="_articleBadge">
   Artigos Gerais
  </span>
  <span class="_separator">
   •
  </span>
  <span class="_editionMeta">
   Rev. Bras. Ensino Fís. 41 
                (2)
   <span class="_separator">
    •
   </span>
   2019
  </span>
  <span class="_separator">
   •
  </span>
  <span class="group-doi">
   <a class="_doi" href="https://doi.org/10.1590/1806-9126-RBEF-2018-0197" target="_blank">
    https://doi.org/10.1590/1806-9126-RBEF-2018-0197
   </a>
   <a class="copyLink" data-clipboard-text="https://doi.org/10.1590/1806-9126-RBEF-2018-0197">
    <span class="sci-ico-link">
    </span>
    copiar
   </a>
  </span>
 </div>
 <h1 class="article-title">
  <span class="sci-ico-openAccess showTooltip" data-original-title="by 4.0 " data-toggle="tooltip">
  </span>
  Aplicações da teoria da informação à neurociência
  <a class="short-link" href="#" id="shorten">
   <span class="sci-ico-link">
   </span>
  </a>
 </h1>
 <h2 class="article-title">
  Information theory applications in neuroscience
 </h2>
 <div class="articleMeta">
 </div>
 <div class="contribGroup">
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor1">
    <span>
     Vinícius Lima Cordeiro
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor1" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade de São Paulo, Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto, Departamento de Física, Laboratório de Sistemas Neurais, Ribeirão Preto, SP, Brasil
    <div class="corresp">
     <sup>
      *
     </sup>
     Endereço de correspondência:
     <a href="mailto:vinicius.lima.cordeiro@usp.br.">
      vinicius.lima.cordeiro@usp.br.
     </a>
    </div>
    <a class="btnContribLinks orcid" href="http://orcid.org/0000-0001-7115-9041">
     http://orcid.org/0000-0001-7115-9041
    </a>
   </ul>
  </span>
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor2">
    <span>
     Rodrigo Felipe de Oliveira Pena
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor2" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade de São Paulo, Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto, Departamento de Física, Laboratório de Sistemas Neurais, Ribeirão Preto, SP, Brasil
    <a class="btnContribLinks orcid" href="http://orcid.org/0000-0002-2037-9746">
     http://orcid.org/0000-0002-2037-9746
    </a>
   </ul>
  </span>
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor3">
    <span>
     Cesar Augusto Celis Ceballos
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor3" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade de São Paulo, Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto, Departamento de Física, Laboratório de Sistemas Neurais, Ribeirão Preto, SP, BrasilUniversidade de São Paulo, Faculdade de Medicina, Departamento de Fisiologia, Ribeirão Preto, SP, Brasil
   </ul>
  </span>
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor4">
    <span>
     Renan Oliveira Shimoura
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor4" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade de São Paulo, Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto, Departamento de Física, Laboratório de Sistemas Neurais, Ribeirão Preto, SP, Brasil
    <a class="btnContribLinks orcid" href="http://orcid.org/0000-0002-6580-5999">
     http://orcid.org/0000-0002-6580-5999
    </a>
   </ul>
  </span>
  <span class="dropdown">
   <a class="dropdown-toggle" data-toggle="dropdown" id="contribGroupTutor5">
    <span>
     Antonio Carlos Roque
    </span>
   </a>
   <ul aria-labelledby="contribGrupoTutor5" class="dropdown-menu" role="menu">
    <strong>
    </strong>
    Universidade de São Paulo, Faculdade de Filosofia, Ciências e Letras de Ribeirão Preto, Departamento de Física, Laboratório de Sistemas Neurais, Ribeirão Preto, SP, Brasil
    <a class="btnContribLinks orcid" href="http://orcid.org/0000-0003-1260-4840">
     http://orcid.org/0000-0003-1260-4840
    </a>
   </ul>
  </span>
  <a class="outlineFadeLink" data-target="#ModalTutors" data-toggle="modal" href="">
   Sobre os autores
  </a>
 </div>
 <div class="row">
  <ul class="col-md-2 hidden-sm articleMenu">
  </ul>
  <article class="col-md-10 col-md-offset-2 col-sm-12 col-sm-offset-0" id="articleText">
   <div class="articleSection" data-anchor="Resumo">
    <h1 class="articleSectionTitle">
     Resumo
    </h1>
    <p>
     Neurônios respondem a estímulos externos emitindo sequências de potenciais de ação (trens de disparos). Desse modo, pode-se dizer que o trem de disparos é a resposta neuronal a um estímulo de entrada. Potenciais de ação são fenômenos do tipo “tudo ou nada”, isto é, um trem de disparos pode ser representado por uma sequência de zeros e uns. No contexto da teoria da informação, pode-se então questionar: quanta informação acerca do estímulo externo o trem de disparos carrega? Ou ainda, quais aspectos do estímulo são codificados pela resposta neuronal? Neste artigo, faz-se uma introdução à teoria da informação em que são apresentados aspectos históricos, conceitos fundamentais da teoria e aplicações à neurociência. A conexão com a neurociência é feita com o uso de demonstrações e discussões de diferentes métodos da teoria da informação. Exemplos são fornecidos com o uso de simulações computacionais de dois modelos de neurônios, o neurônio Poisson e o neurônio integra-e-dispara, e um modelo de rede de autômatos celulares. No ultimo caso, demonstra-se como se pode utilizar medidas da teoria da informação para reconstruir a matriz de conectividade de uma rede. Todos os códigos utilizados para estas simulações foram disponibilizados publicamente na plataforma GitHub, acessíveis pelo url: github.com/ViniciusLima94/ticodigoneural.
    </p>
    <p>
     <strong>
      Palavras-chave:
     </strong>
     <br/>
     teoria da informação; neurociência; simulação computacional; neurônios
    </p>
   </div>
   <div class="articleSection" data-anchor="Abstract">
    <h1 class="articleSectionTitle">
     Abstract
    </h1>
    <p>
     Neurons respond to external stimuli by emitting sequences of action potentials (spike trains). In this way, one can say that the spike train is the neuronal response to an input stimulus. Action potentials are “all-or-none” phenomena, which means that a spike train can be represented by a sequence of zeros and ones. In the context of information theory, one can then ask: how much information about a given stimulus the spike train conveys? Or rather, what aspects of the stimulus are encoded by the neuronal response? In this article, an introduction to information theory is presented which consists of historical aspects, fundamental concepts of the theory, and applications to neuroscience. The connection to neuroscience is made with the use of demonstrations and discussions of different methods of the theory of information. Examples are given through computer simulations of two neuron models, the Poisson neuron and the integrate-and-fire neuron, and a cellular automata network model. In the latter case, it is shown how one can use information theory measures to retrieve the connectivity matrix of a network. All codes used in the simulations were made publicly available at the GitHub platform and are accessible trough the url: github.com/ViniciusLima94/ticodigoneural.
    </p>
    <p>
     <strong>
      Keywords:
     </strong>
     <br/>
     information theory; neuroscience; computer simulation; neurons
    </p>
   </div>
   <div class="articleSection" data-anchor="Text">
    <h1 class="articleSectionTitle">
     1. Introdução
    </h1>
    <p>
     O sistema nervoso tem como unidade básica o neurônio. Neurônios são células eletricamente excitáveis considerados os principais elementos envolvidos na transmissão de informação pelo cérebro, seja por meio de sinais elétricos ou químicos. Existe uma diferença de potencial elétrico
     <math>
      <mi>
       v
      </mi>
     </math>
     entre os meios intra- e extracelular de um neurônio, através da membrana plasmática, chamado de potencial de membrana. O potencial de membrana pode ser alterado pela chegada de estímulos externos ao neurônio. Na ausência de estímulos, o potencial de membrana flutua em torno de um valor constante negativo chamado potencial de repouso (
     <math>
      <msub>
       <mi>
        v
       </mi>
       <mtext>
        R
       </mtext>
      </msub>
     </math>
     ). Estímulos que levam o potencial de membrana a valores mais positivos são chamados de despolarizantes ou excitatórios; por outro lado, estímulos que levam o potencial de membrana a valores mais negativos são chamados de hiperpolarizantes ou inibitórios. Quando um estímulo despolarizante é capaz de elevar o potencial de membrana acima de uma potencial limiar (
     <math>
      <msub>
       <mi>
        v
       </mi>
       <mtext>
        T
       </mtext>
      </msub>
     </math>
     ), ocorre um potencial de ação ou disparo. A existência de um limiar para a ocorrência de um potencial de ação permite a interpretação deste fenômeno como sendo de tipo tudo ou nada
     <span class="ref footnote">
      <sup class="xref">
       1
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         1
        </strong>
        O primeiro a notar isso foi o neurofisiologista inglês Lord Adrian [1].
       </span>
      </span>
     </span>
     . Se o estímulo despolarizante for mantido por um longo tempo, o neurônio emite uma sequência de potenciais de ação ou trem de disparos (
     <a class="open-asset-modal" data-target="#ModalFigf1" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      Fig. 1
     </a>
     ).
    </p>
    <div class="row fig" id="f1">
     <a name="f1">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf1" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 1
      </strong>
      <br/>
      Registro eletrofisiológico
      <i>
       in vitro
      </i>
      de uma célula piramidal do hipocampo do rato estimulada por uma injeção de corrente constante (
      <math>
       <mrow>
        <mi>
         I
        </mi>
        <mo>
         =
        </mo>
        <mn>
         260
        </mn>
       </mrow>
      </math>
      pA) aplicada ao corpo celular (soma), mostrando parte do trem de disparos gerado pela célula. Acima do registro há uma sequência de zeros e uns ilustrando a representação binária da série temporal dos disparos da célula.
      <br/>
     </div>
    </div>
    <p>
     Acredita-se que a capacidade de processamento do cérebro é decorrente da interação entre os trens de disparos dos seus bilhões de neurônios, que pode ser entendida como uma espécie de
     <i>
      codificação neural
     </i>
     <span class="ref">
      <strong class="xref xrefblue">
       [2]
      </strong>
      <span class="refCtt closed">
       <span>
        [2] W. Gerstner, W.M. Kistler, R. Naud e L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition (Cambridge University Press, Cambridge, 2014).
       </span>
      </span>
     </span>
     . Neste artigo, o termo “codificação neural” será usado neste sentido
     <span class="ref footnote">
      <sup class="xref">
       2
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         2
        </strong>
        Alguns autores criticam a concepção de código neural. Ver, por exemplo, [3].
       </span>
      </span>
     </span>
     . Os estímulos sensoriais são inicialmente codificados em trens de disparos por neurônios sensoriais localizados em estruturas periféricas do sistema nervoso, por exemplo, células da retina ou sensíveis ao tato na pele. Esses disparos propagam-se em direção ao cérebro pelos axônios dos neurônios, fazendo sinapses com outros neurônios e gerando cascatas de trens de disparos que se propagam estimulando mais neurônios, cobrindo vastas regiões cerebrais. A complexa interação entre essa multiplicidade de neurônios codifica as diferentes características dos estímulos (contorno, cor, intensidade, temperatura, cheiro, etc). Conjectura-se que a capacidade de processamento das diferentes características contidas nos sinais sensoriais provêm em parte da capacidade de processamento individual dos neurônios [
     <span class="ref">
      <sup class="xref xrefblue">
       4
      </sup>
      <span class="refCtt closed">
       <span>
        [4] P. König, K.E. Andreas e W. Singer, Trends neurosci. 19, 130 (1996).
       </span>
      </span>
     </span>
     ,
     <span class="ref">
      <sup class="xref xrefblue">
       5
      </sup>
      <span class="refCtt closed">
       <span>
        [5] B.B. Averbeck, E.L. Peter e A. Pouget, Nat. Rev. Neurosci. 7, 358 (2006).
       </span>
      </span>
     </span>
     ].
    </p>
    <p>
     Considerando um neurônio isolado, o código neuronal pode ser entendido como a representação da informação de um estímulo aplicado ao neurônio na forma de um trem de disparos [
     <span class="ref">
      <sup class="xref xrefblue">
       6
      </sup>
      <span class="refCtt closed">
       <span>
        [6] W. Bialek, F. Rieke, R.D.R. Van Steveninck e D. Warland, Science 252, 1854 (1991).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [7] J.J. Eggermont, Neurosci. Biobehav. R. 22, 355 (1998).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [8] F. Rieke, D. Warland, R.D.R. Van Steveninck e W. Bialek, Spikes: exploring the neural code (MIT press, Cambridge, 1999).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [9] L. Paninski, Neural. Comput. 15, 1191 (2003).
       </span>
      </span>
     </span>
     -
     <span class="ref">
      <sup class="xref xrefblue">
       10
      </sup>
      <span class="refCtt closed">
       <span>
        [10] C. Laing e J.L. Gabriel, Stochastic methods in neuroscience (Oxford University Press, Oxford, 2010).
       </span>
      </span>
     </span>
     ]. Há várias hipóteses que concordam entre si que o potencial de ação é o principal meio de transmissão de informação pelo cérebro
     <span class="ref">
      <strong class="xref xrefblue">
       [11]
      </strong>
      <span class="refCtt closed">
       <span>
        [11] B. Naundorf, F. Wolf e M. Volgushev, Nature 440, 1060 (2006).
       </span>
      </span>
     </span>
     . Entretanto, ainda discute-se a respeito de qual a melhor maneira de interpretar e extrair a informação contida em séries temporais de disparos de um neurônio.
    </p>
    <p>
     Independentemente da abordagem, o primeiro passo no estudo do código neural é detectar os sinais elétricos correspondentes aos trens de disparos e convertê-los em sinais mais simples e de fácil manipulação. Aproveitando a natureza “tudo ou nada” dos potenciais de ação, uma forma de simplificar o sinal é discretizar a série temporal e atribuir valores binários para os eventos. Assim, uma série de disparos pode ser “traduzida” por uma sequência binária (
     <span class="ref">
      <strong class="xref xrefblue">
       figura 1B
      </strong>
      <span class="refCtt closed">
       <span>
        [1] E.D. Adrian, J. Physiol. 47, 460 (1914).
       </span>
      </span>
     </span>
     ). Matematicamente, uma sequência desse tipo pode ser escrita como na
     <a class="open-asset-modal" data-target="#ModalSchemeeq1" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      Eq. 1
     </a>
     ,
    </p>
    <div class="row formula" id="eeq1">
     <a name="eq1">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (1)
       </span>
       <math display="block" id="m1">
        <mrow>
         <msub>
          <mi>
           x
          </mi>
          <mi>
           i
          </mi>
         </msub>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <munder>
          <mo>
           ∑
          </mo>
          <mrow>
           <mo>
            [
           </mo>
           <msubsup>
            <mi>
             t
            </mi>
            <mi>
             i
            </mi>
            <mi>
             f
            </mi>
           </msubsup>
           <mo>
            ]
           </mo>
          </mrow>
         </munder>
         <mi>
          δ
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           −
          </mo>
          <msubsup>
           <mi>
            t
           </mi>
           <mi>
            i
           </mi>
           <mi>
            f
           </mi>
          </msubsup>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <mrow>
       <mi>
        δ
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é a função delta de Dirac e
     <math>
      <msubsup>
       <mi>
        t
       </mi>
       <mi>
        i
       </mi>
       <mi>
        f
       </mi>
      </msubsup>
     </math>
     são os tempos em que ocorrem os disparos.
    </p>
    <p>
     Em geral, pode-se classificar a codificação de informação feita pelo trem de disparos de um neurônio de duas maneiras diferentes
     <span class="ref">
      <strong class="xref xrefblue">
       [12]
      </strong>
      <span class="refCtt closed">
       <span>
        [12] F. David e S. Nelson, Science 270, 756 (1995).
       </span>
      </span>
     </span>
     : (i) por frequência de disparos
     <span class="ref">
      <strong class="xref xrefblue">
       [13]
      </strong>
      <span class="refCtt closed">
       <span>
        [13] D.J. Foster e M.A. Wilson, Nature 440, 680 (2006).
       </span>
      </span>
     </span>
     e (ii) por padrão temporal de disparos
     <span class="ref">
      <strong class="xref xrefblue">
       [14]
      </strong>
      <span class="refCtt closed">
       <span>
        [14] M.R. Mehta, A.K. Lee e M.A. Wilson, Nature 417, 741 (2002).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [15] C. Passaglia, F. Dodge, E. Herzog, S.Jackson e R. Barlow, P. Natl. Acad. Sci. 94, 12649 (1997).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [16] V.E. Abraira e D.D. Ginty, Neuron 79, 618 (2013).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [17] P. Dayan e L.F. Abbott, Theoretical neuroscience (MIT Press, Cambridge, 2001).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [18] J. O'Keefe e M.L. Recce, Hippocampus. 3, 317 (1993).
       </span>
      </span>
     </span>
     . Exemplos desses dois tipos de codificação são dados no Apêndice A.
    </p>
    <p>
     O objetivo deste artigo é mostrar como as ferramentas da teoria da informação podem ser utilizadas para quantificar a informação contida nos trens de disparos neuronais. Isso será feito por meio de simulações computacionais de modelos de neurônios e da análise de seus trens de disparos por medidas da teoria da informação.
    </p>
    <p>
     Este artigo está organizado da seguinte forma: na próxima seção, dois modelos computacionais de neurônios e um de rede de neurônios são usados com o objetivo de ilustrar a aplicação de métodos da teoria da informação em sinais artificialmente gerados. Adicionalmente, a seção 3 introduz todos os conceitos básicos para entendimento das principais medidas e, na seção 4, esses conceitos são trabalhados e ilustrados com uso de simulações computacionais. Discussões sobre problemas que podem ser encontrados ao implementar essas medidas também são encontradas nessa seção. Por fim, conclusões são apresentadas na seção 6.
    </p>
    <h1 class="articleSectionTitle">
     2. Modelos de neurônio
    </h1>
    <p>
     De forma a aplicar os conceitos de teoria da informação que serão introduzidos neste artigo, optou-se por gerar sinais de neurônios a partir de modelos computacionais. No presente artigo foram utilizados três modelos: O neurônio Poisson, o neurônio integra-e-dispara estocástico e o neurônio binário estocástico; este último foi utilizado para a construção de uma rede neural simples. A escolha desses modelos se deve ao fato de que todos possuem trens de disparo estocásticos. Além disso, sua implementação computacional é simples.
    </p>
    <h2>
     2.1. Neurônio Poisson
    </h2>
    <p>
     O primeiro modelo considerado toma o sinal de entrada
     <math>
      <mrow>
       <mi>
        s
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     recebido pelo neurônio como a taxa de disparos dependente do tempo
     <math>
      <mrow>
       <mi>
        r
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     de um processo de Poisson
     <span class="ref">
      <strong class="xref xrefblue">
       [19]
      </strong>
      <span class="refCtt closed">
       <span>
        [19] D. Bernardi e B. Lindner, J. Neurophysiol. 113, 1342 (2015).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [21] C.E. Shannon, Bell. Syst. Tech. J. 27, 623 (1948).
       </span>
      </span>
     </span>
     . Para tal modelo, assumindo que
     <math>
      <mrow>
       <mi>
        r
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     não varia significativamente dentro de um intervalo de tempo curto
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     , a probabilidade de que
     <math>
      <mi>
       k
      </mi>
     </math>
     disparos sejam gerados nesse intervalo de tempo é
     <math>
      <mrow>
       <msup>
        <mi>
         e
        </mi>
        <mrow>
         <mo>
          −
         </mo>
         <mi>
          r
         </mi>
         <mo>
          (
         </mo>
         <mi>
          t
         </mi>
         <mo>
          )
         </mo>
         <mi>
          Δ
         </mi>
         <mi>
          t
         </mi>
        </mrow>
       </msup>
       <msup>
        <mfenced close="]" open="[" separators="">
         <mi>
          r
         </mi>
         <mo>
          (
         </mo>
         <mi>
          t
         </mi>
         <mo>
          )
         </mo>
         <mi>
          Δ
         </mi>
         <mi>
          t
         </mi>
        </mfenced>
        <mi>
         k
        </mi>
       </msup>
       <mo>
        /
       </mo>
       <mi>
        k
       </mi>
       <mo>
        !
       </mo>
      </mrow>
     </math>
     . Se
     <math>
      <mrow>
       <mi>
        r
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
       <mo>
        ≪
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     , a expansão em série de Taylor de
     <math>
      <mrow>
       <mi>
        r
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     até primeira ordem resulta na
     <a class="open-asset-modal" data-target="#ModalSchemeeq2" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (2)
     </a>
     , que é a taxa de disparos do neurônio,
    </p>
    <div class="row formula" id="eeq2">
     <a name="eq2">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (2)
       </span>
       <math display="block" id="m2">
        <mrow>
         <mi>
          r
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <msub>
          <mi>
           r
          </mi>
          <mn>
           0
          </mn>
         </msub>
         <mrow>
          <mo>
           [
          </mo>
          <mn>
           1
          </mn>
          <mo>
           +
          </mo>
          <mi>
           ϵ
          </mi>
          <mi>
           s
          </mi>
          <mrow>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            )
           </mo>
          </mrow>
          <mo>
           ]
          </mo>
         </mrow>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <msub>
       <mi>
        r
       </mi>
       <mn>
        0
       </mn>
      </msub>
     </math>
     é a frequência média de disparos e
     <math>
      <mi>
       ϵ
      </mi>
     </math>
     é um parâmetro adimensional que controla a força do sinal
     <math>
      <mrow>
       <mi>
        s
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     . Este, por sua vez, é tomado como uma entrada gaussiana de média zero e variância unitária.
    </p>
    <p>
     O algoritmo para gerar o trem de disparos pode ser implementado da seguinte forma:
    </p>
    <ol type="1">
     <li>
      <p>
       Para cada passo de tempo
       <math>
        <mrow>
         <mi>
          Δ
         </mi>
         <mi>
          t
         </mi>
        </mrow>
       </math>
       na simulação, calcula-se a taxa de disparos instantânea
       <math>
        <mrow>
         <mi>
          r
         </mi>
         <mo>
          (
         </mo>
         <mi>
          t
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       ;
      </p>
     </li>
     <li>
      <p>
       Determina-se a probabilidade de emitir um disparo (
       <math>
        <msub>
         <mi>
          p
         </mi>
         <mtext>
          disp
         </mtext>
        </msub>
       </math>
       ) nesse passo de tempo, dada por
       <math>
        <mrow>
         <msub>
          <mi>
           p
          </mi>
          <mtext>
           disp
          </mtext>
         </msub>
         <mo>
          =
         </mo>
         <mi>
          r
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mi>
          Δ
         </mi>
         <mi>
          t
         </mi>
        </mrow>
       </math>
       ;
      </p>
     </li>
     <li>
      <p>
       Gera-se um número aleatório (
       <math>
        <mrow>
         <mn>
          0
         </mn>
         <mo>
          &lt;
         </mo>
         <mi>
          η
         </mi>
         <mo>
          &lt;
         </mo>
         <mn>
          1
         </mn>
        </mrow>
       </math>
       ) utilizando um gerador uniforme (note que
       <math>
        <mi>
         η
        </mi>
       </math>
       faz o papel do ruído nesse modelo). Caso
       <math>
        <mrow>
         <mi>
          η
         </mi>
         <mo>
          &lt;
         </mo>
         <msub>
          <mi>
           p
          </mi>
          <mtext>
           disp
          </mtext>
         </msub>
        </mrow>
       </math>
       é dito que o neurônio emitiu um disparo e nesse passo de tempo é atribuído o valor
       <math>
        <mrow>
         <mn>
          1
         </mn>
         <mo>
          /
         </mo>
         <mi>
          Δ
         </mi>
         <mi>
          t
         </mi>
        </mrow>
       </math>
       ao vetor correspondente ao trem de disparos; caso contrário é atribuído o valor 0.
      </p>
     </li>
    </ol>
    <p>
     Os parâmetros utilizados na implementação são os mesmos reportados em
     <span class="ref">
      <strong class="xref xrefblue">
       [19]
      </strong>
      <span class="refCtt closed">
       <span>
        [19] D. Bernardi e B. Lindner, J. Neurophysiol. 113, 1342 (2015).
       </span>
      </span>
     </span>
     , sendo
     <math>
      <mrow>
       <msub>
        <mi>
         r
        </mi>
        <mn>
         0
        </mn>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        100
       </mn>
      </mrow>
     </math>
     ,
     <math>
      <mrow>
       <mi>
        ϵ
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        2
       </mn>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     ms.
    </p>
    <h2>
     2.2. Neurônio integra-e-dispara estocástico
    </h2>
    <p>
     O modelo integra-e-dispara com vazamento (LIF, do inglês
     <i>
      “leaky integrate-and-fire”
     </i>
     ) é um modelo simples, mas amplamente utilizado em neurociência computacional
     <span class="ref">
      <strong class="xref xrefblue">
       [2]
      </strong>
      <span class="refCtt closed">
       <span>
        [2] W. Gerstner, W.M. Kistler, R. Naud e L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition (Cambridge University Press, Cambridge, 2014).
       </span>
      </span>
     </span>
     . A versão do modelo implementada neste trabalho é a mesma descrita em
     <span class="ref">
      <strong class="xref xrefblue">
       [19]
      </strong>
      <span class="refCtt closed">
       <span>
        [19] D. Bernardi e B. Lindner, J. Neurophysiol. 113, 1342 (2015).
       </span>
      </span>
     </span>
     , a qual possui entrada e ruído gaussianos, sendo por isso chamada de modelo LIF estocástico. A dinâmica desse modelo é descrita pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq3" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (3)
     </a>
     .
    </p>
    <div class="row formula" id="eeq3">
     <a name="eq3">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (3)
       </span>
       <math display="block" id="m3">
        <mrow>
         <msub>
          <mi>
           τ
          </mi>
          <mi>
           m
          </mi>
         </msub>
         <mover accent="true">
          <mi>
           v
          </mi>
          <mo>
           ˙
          </mo>
         </mover>
         <mo>
          =
         </mo>
         <mo>
          −
         </mo>
         <mi>
          v
         </mi>
         <mo>
          +
         </mo>
         <mi>
          μ
         </mi>
         <mo>
          +
         </mo>
         <msqrt>
          <mrow>
           <mn>
            2
           </mn>
           <mi>
            D
           </mi>
           <mi>
            c
           </mi>
          </mrow>
         </msqrt>
         <msub>
          <mi>
           ξ
          </mi>
          <mtext>
           s
          </mtext>
         </msub>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          +
         </mo>
         <msqrt>
          <mrow>
           <mn>
            2
           </mn>
           <mi>
            D
           </mi>
           <mo>
            (
           </mo>
           <mn>
            1
           </mn>
           <mo>
            −
           </mo>
           <mi>
            c
           </mi>
           <mo>
            )
           </mo>
          </mrow>
         </msqrt>
         <msub>
          <mi>
           ξ
          </mi>
          <mtext>
           n
          </mtext>
         </msub>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <msub>
       <mi>
        τ
       </mi>
       <mi>
        m
       </mi>
      </msub>
     </math>
     é a constante de tempo da membrana;
     <math>
      <mi>
       μ
      </mi>
     </math>
     determina o potencial de repouso;
     <math>
      <mrow>
       <msub>
        <mi>
         ξ
        </mi>
        <mtext>
         s
        </mtext>
       </msub>
       <mrow>
        <mo>
         (
        </mo>
        <mi>
         t
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <msub>
        <mi>
         ξ
        </mi>
        <mtext>
         n
        </mtext>
       </msub>
       <mrow>
        <mo>
         (
        </mo>
        <mi>
         t
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     são processos gaussianos independentes, ambos com média zero e desvio padrão unitário, onde o primeiro é o estímulo de entrada e o segundo o ruído;
     <math>
      <mi>
       D
      </mi>
     </math>
     é a intensidade do ruído; e
     <math>
      <mi>
       c
      </mi>
     </math>
     é um parâmetro entre
     <math>
      <mrow>
       <mn>
        0
       </mn>
       <mo>
        ≤
       </mo>
       <mi>
        c
       </mi>
       <mo>
        ≤
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     que representa a força relativa do sinal
     <span class="ref">
      <strong class="xref xrefblue">
       [19]
      </strong>
      <span class="refCtt closed">
       <span>
        [19] D. Bernardi e B. Lindner, J. Neurophysiol. 113, 1342 (2015).
       </span>
      </span>
     </span>
     .
    </p>
    <p>
     A dinâmica do modelo é complementada por uma regra de “disparo-e-reset” dada pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq4" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (4)
     </a>
     ,
    </p>
    <div class="row formula" id="eeq4">
     <a name="eq4">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (4)
       </span>
       <math display="block" id="m4">
        <mrow>
         <mi>
          v
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          ≥
         </mo>
         <msub>
          <mi>
           v
          </mi>
          <mtext>
           T
          </mtext>
         </msub>
         <mo>
          ⇒
         </mo>
         <mfenced close="" open="{" separators="">
          <mtable>
           <mtr>
            <mtd columnalign="left">
             <mrow>
              <mi>
               disparo
              </mi>
              <mtext>
              </mtext>
              <mi>
               em
              </mi>
              <mi>
               t
              </mi>
             </mrow>
            </mtd>
           </mtr>
           <mtr>
            <mtd columnalign="left">
             <mrow>
              <mi>
               v
              </mi>
              <mrow>
               <mo>
                (
               </mo>
               <mi>
                t
               </mi>
               <mo>
                +
               </mo>
               <msub>
                <mi>
                 τ
                </mi>
                <mtext>
                 abs
                </mtext>
               </msub>
               <mo>
                )
               </mo>
              </mrow>
              <mo>
               =
              </mo>
              <msub>
               <mi>
                v
               </mi>
               <mtext>
                R
               </mtext>
              </msub>
              <mo>
               .
              </mo>
             </mrow>
            </mtd>
           </mtr>
          </mtable>
         </mfenced>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     Nesta regra, quando a voltagem atinge o valor
     <math>
      <msub>
       <mi>
        v
       </mi>
       <mtext>
        T
       </mtext>
      </msub>
     </math>
     (potencial limiar), ela é resetada para
     <math>
      <msub>
       <mi>
        v
       </mi>
       <mtext>
        R
       </mtext>
      </msub>
     </math>
     (voltagem de reset) e mantida nele por um período
     <math>
      <msub>
       <mi>
        τ
       </mi>
       <mtext>
        abs
       </mtext>
      </msub>
     </math>
     (período refratário absoluto).
    </p>
    <p>
     Nessa versão do modelo o potencial de membrana é normalizado, sendo
     <math>
      <mrow>
       <mi>
        v
       </mi>
       <mo>
        ∈
       </mo>
       <mo>
        [
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        1
       </mn>
       <mo>
        ]
       </mo>
      </mrow>
     </math>
     mV, de modo que define-se
     <math>
      <mrow>
       <msub>
        <mi>
         v
        </mi>
        <mtext>
         R
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <msub>
        <mi>
         v
        </mi>
        <mtext>
         T
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     . O valor inicial do potencial de membrana é dado por um número aleatório uniforme também contido no intervalo
     <math>
      <mrow>
       <mo>
        [
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        1
       </mn>
       <mo>
        ]
       </mo>
      </mrow>
     </math>
     .
    </p>
    <p>
     A
     <a class="open-asset-modal" data-target="#ModalSchemeeq3" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (3)
     </a>
     é resolvida numericamente neste artigo com uso do método de Euler--Maruyama
     <span class="ref">
      <strong class="xref xrefblue">
       [20]
      </strong>
      <span class="refCtt closed">
       <span>
        [20] D.J. Higham, Siam. Rev. 43, 525 (2001).
       </span>
      </span>
     </span>
     e reescrita na forma de um processo iterativo dado pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq5" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (5)
     </a>
     .
    </p>
    <div class="row formula" id="eeq5">
     <a name="eq5">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (5)
       </span>
       <math display="block" id="m5">
        <mtable columnalign="left">
         <mtr>
          <mtd>
           <mi>
            v
           </mi>
           <mrow>
            <mo>
             (
            </mo>
            <mrow>
             <mi>
              t
             </mi>
             <mo>
              +
             </mo>
             <mi>
              Δ
             </mi>
             <mi>
              t
             </mi>
            </mrow>
            <mo>
             )
            </mo>
           </mrow>
           <mo>
            =
           </mo>
           <mi>
            v
           </mi>
           <mrow>
            <mo>
             (
            </mo>
            <mi>
             t
            </mi>
            <mo>
             )
            </mo>
           </mrow>
           <mo>
            +
           </mo>
           <mrow>
            <mo>
             (
            </mo>
            <mrow>
             <mfrac>
              <mrow>
               <mo>
                −
               </mo>
               <mi>
                v
               </mi>
               <mo>
                +
               </mo>
               <mi>
                μ
               </mi>
              </mrow>
              <mrow>
               <msub>
                <mi>
                 τ
                </mi>
                <mi>
                 m
                </mi>
               </msub>
              </mrow>
             </mfrac>
            </mrow>
            <mo>
             )
            </mo>
           </mrow>
           <mtext>
           </mtext>
           <mi>
            Δ
           </mi>
           <mi>
            t
           </mi>
          </mtd>
         </mtr>
         <mtr>
          <mtd>
           <mtext>
           </mtext>
           <mtext>
           </mtext>
           <mtext>
           </mtext>
           <mtext>
           </mtext>
           <mtext>
           </mtext>
           <mtext>
           </mtext>
           <mtext>
           </mtext>
           <mo>
            +
           </mo>
           <mfrac>
            <mrow>
             <msqrt>
              <mrow>
               <mn>
                2
               </mn>
               <mi>
                D
               </mi>
               <mi>
                c
               </mi>
               <mi>
                Δ
               </mi>
               <mi>
                t
               </mi>
              </mrow>
             </msqrt>
            </mrow>
            <mrow>
             <msub>
              <mi>
               τ
              </mi>
              <mi>
               m
              </mi>
             </msub>
            </mrow>
           </mfrac>
           <msub>
            <mi>
             ξ
            </mi>
            <mi>
             s
            </mi>
           </msub>
           <mrow>
            <mo>
             (
            </mo>
            <mi>
             t
            </mi>
            <mo>
             )
            </mo>
           </mrow>
           <mo>
            +
           </mo>
           <mfrac>
            <mrow>
             <msqrt>
              <mrow>
               <mn>
                2
               </mn>
               <mi>
                D
               </mi>
               <mrow>
                <mo>
                 (
                </mo>
                <mrow>
                 <mn>
                  1
                 </mn>
                 <mo>
                  −
                 </mo>
                 <mi>
                  c
                 </mi>
                </mrow>
                <mo>
                 )
                </mo>
               </mrow>
               <mi>
                Δ
               </mi>
               <mi>
                t
               </mi>
              </mrow>
             </msqrt>
            </mrow>
            <mrow>
             <msub>
              <mi>
               τ
              </mi>
              <mi>
               m
              </mi>
             </msub>
            </mrow>
           </mfrac>
           <msub>
            <mi>
             ξ
            </mi>
            <mi>
             n
            </mi>
           </msub>
           <mrow>
            <mo>
             (
            </mo>
            <mi>
             t
            </mi>
            <mo>
             )
            </mo>
           </mrow>
           <mo>
            ,
           </mo>
          </mtd>
         </mtr>
        </mtable>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     é o passo de integração.
    </p>
    <p>
     O trem de disparos pode ser gerado da seguinte forma:
    </p>
    <ol type="1">
     <li>
      <p>
       Em cada passo de tempo da simulação, calcula-se
       <math>
        <mrow>
         <mi>
          v
         </mi>
         <mo>
          (
         </mo>
         <mi>
          t
         </mi>
         <mo>
          +
         </mo>
         <mi>
          Δ
         </mi>
         <mi>
          t
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       , utilizando a
       <a class="open-asset-modal" data-target="#ModalSchemeeq5" data-toggle="modal" href="">
        <span class="sci-ico-fileFormula">
        </span>
        equação (5)
       </a>
       ;
      </p>
     </li>
     <li>
      <p>
       Caso a condição de disparo-e-reset dada pela
       <a class="open-asset-modal" data-target="#ModalSchemeeq4" data-toggle="modal" href="">
        <span class="sci-ico-fileFormula">
        </span>
        equação (4)
       </a>
       seja cumprida:
      </p>
      <ol type="a">
       <li>
        <p>
         Atribui-se
         <math>
          <mrow>
           <mn>
            1
           </mn>
           <mo>
            /
           </mo>
           <mi>
            Δ
           </mi>
           <mi>
            t
           </mi>
          </mrow>
         </math>
         ao vetor correspondente ao trem de disparos;
        </p>
       </li>
       <li>
        <p>
         Entrar no período refratário, então voltar ao passo 1.
        </p>
       </li>
      </ol>
     </li>
     <li>
      <p>
       Senão, atribui-se 0 ao vetor correspondente ao trem de disparos. Então, voltar ao passo 1.
      </p>
     </li>
    </ol>
    <p>
     Para esse modelo foram simulados dois casos com diferentes parâmetros:
     <math>
      <mrow>
       <msub>
        <mi>
         τ
        </mi>
        <mi>
         m
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        10
       </mn>
      </mrow>
     </math>
     ,
     <math>
      <mrow>
       <mi>
        μ
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        75
       </mn>
      </mrow>
     </math>
     ,
     <math>
      <mrow>
       <mi>
        D
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        6
       </mn>
      </mrow>
     </math>
     ms,
     <math>
      <mrow>
       <mi>
        c
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        34
       </mn>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <msub>
        <mi>
         τ
        </mi>
        <mrow>
         <mi>
          a
         </mi>
         <mi>
          b
         </mi>
         <mi>
          s
         </mi>
        </mrow>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     ms (sinal fraco) e
     <math>
      <mrow>
       <msub>
        <mi>
         τ
        </mi>
        <mi>
         m
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        10
       </mn>
      </mrow>
     </math>
     ms,
     <math>
      <mrow>
       <mi>
        μ
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        75
       </mn>
      </mrow>
     </math>
     ,
     <math>
      <mrow>
       <mi>
        D
       </mi>
       <mo>
        =
       </mo>
       <mn>
        3
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        3
       </mn>
      </mrow>
     </math>
     ms,
     <math>
      <mrow>
       <mi>
        c
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        98
       </mn>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <msub>
        <mi>
         τ
        </mi>
        <mrow>
         <mi>
          a
         </mi>
         <mi>
          b
         </mi>
         <mi>
          s
         </mi>
        </mrow>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     ms (sinal forte). Para ambos os casos
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     ms.
    </p>
    <h2>
     2.3. Rede de neurônios binários estocásticos
    </h2>
    <p>
     Neurônios binários representam uma simplificação da resposta de um neurônio, podendo assumir apenas dois estados a cada passo de tempo
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     : o estado de repouso
     <math>
      <mrow>
       <msub>
        <mi>
         S
        </mi>
        <mtext>
         0
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     e o estado ativo (ou disparo)
     <math>
      <mrow>
       <msub>
        <mi>
         S
        </mi>
        <mtext>
         1
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     . Desta forma, o trem de disparos
     <math>
      <mrow>
       <mi>
        S
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     desse modelo de neurônio é dado por uma série temporal de zeros e uns.
    </p>
    <p>
     A rede de neurônios binários considerada neste trabalho consiste de
     <math>
      <mi>
       N
      </mi>
     </math>
     neurônios interconectados sendo
     <math>
      <msub>
       <mi>
        N
       </mi>
       <mtext>
        ex
       </mtext>
      </msub>
     </math>
     excitatórios e
     <math>
      <msub>
       <mi>
        N
       </mi>
       <mtext>
        in
       </mtext>
      </msub>
     </math>
     inibitórios. As conexões excitatórias são feitas com probabilidade
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        e
       </mtext>
      </msub>
     </math>
     , e as inibitórias com probabilidade
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        i
       </mtext>
      </msub>
     </math>
     . Essas conexões são representadas na forma de uma matriz de adjacência (
     <math>
      <msup>
       <mi>
        M
       </mi>
       <mtext>
        adj
       </mtext>
      </msup>
     </math>
     ), de dimensão
     <math>
      <mi>
       N
      </mi>
     </math>
     x
     <math>
      <mi>
       N
      </mi>
     </math>
     . Caso haja uma conexão entre os neurônios
     <math>
      <mi>
       i
      </mi>
     </math>
     e
     <math>
      <mi>
       j
      </mi>
     </math>
     , diz-se que
     <math>
      <mrow>
       <msubsup>
        <mi>
         M
        </mi>
        <mrow>
         <mi>
          i
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          j
         </mi>
        </mrow>
        <mtext>
         adj
        </mtext>
       </msubsup>
       <mo>
        =
       </mo>
       <mi>
        G
       </mi>
      </mrow>
     </math>
     , senão
     <math>
      <mrow>
       <msubsup>
        <mi>
         M
        </mi>
        <mrow>
         <mi>
          i
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          j
         </mi>
        </mrow>
        <mtext>
         adj
        </mtext>
       </msubsup>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     .
    </p>
    <p>
     O valor
     <math>
      <mi>
       G
      </mi>
     </math>
     define o peso (ou força) da conexão sináptica, sendo que para sinapses excitatórias
     <math>
      <mi>
       G
      </mi>
     </math>
     assume valores positivos e para sinapses inibitórias
     <math>
      <mi>
       G
      </mi>
     </math>
     assume valores negativos.
    </p>
    <p>
     No caso considerado aqui, os disparos dos neurônios se comportam de maneira estocástica. A probabilidade de haver um disparo
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        d
       </mtext>
      </msub>
     </math>
     de um dado neurônio
     <math>
      <mi>
       i
      </mi>
     </math>
     no tempo
     <math>
      <mrow>
       <mi>
        t
       </mi>
       <mo>
        +
       </mo>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     é determinada pelo estado dos neurônios
     <math>
      <mrow>
       <mi>
        j
       </mi>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        2
       </mn>
       <mo>
        ,
       </mo>
       <mo>
        .
       </mo>
       <mo>
        .
       </mo>
       <mo>
        .
       </mo>
       <mo>
        ,
       </mo>
       <mi>
        N
       </mi>
      </mrow>
     </math>
     com
     <math>
      <mrow>
       <mi>
        j
       </mi>
       <mo>
        ≠
       </mo>
       <mi>
        i
       </mi>
      </mrow>
     </math>
     no tempo
     <math>
      <mi>
       t
      </mi>
     </math>
     , de acordo com a
     <a class="open-asset-modal" data-target="#ModalSchemeeq6" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (6)
     </a>
     :
    </p>
    <div class="row formula" id="eeq6">
     <a name="eq6">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (6)
       </span>
       <math display="block" id="m6">
        <mrow>
         <msub>
          <mi>
           P
          </mi>
          <mtext>
           d
          </mtext>
         </msub>
         <mo>
          =
         </mo>
         <mi>
          I
         </mi>
         <mo>
          +
         </mo>
         <munderover>
          <mo>
           ∑
          </mo>
          <mrow>
           <mi>
            j
           </mi>
           <mo>
            =
           </mo>
           <mn>
            1
           </mn>
           <mo>
            ,
           </mo>
           <mi>
            j
           </mi>
           <mo>
            ≠
           </mo>
           <mi>
            i
           </mi>
          </mrow>
          <mi>
           N
          </mi>
         </munderover>
         <msubsup>
          <mi>
           M
          </mi>
          <mrow>
           <mi>
            i
           </mi>
           <mo>
            ,
           </mo>
           <mi>
            j
           </mi>
          </mrow>
          <mtext>
           adj
          </mtext>
         </msubsup>
         <msub>
          <mi>
           S
          </mi>
          <mi>
           j
          </mi>
         </msub>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <mi>
       I
      </mi>
     </math>
     é um estímulo constante igual para todos os neurônios. Note que conexões excitatórias aumentam
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        d
       </mtext>
      </msub>
     </math>
     enquanto que conexões inibitórias diminuem.
    </p>
    <p>
     Para manter a simplicidade, será usada uma rede cujas conexões são exclusivamente excitatórias (ou seja, não há sinapses inibitórias), com
     <math>
      <mrow>
       <msub>
        <mi>
         N
        </mi>
        <mtext>
         ex
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        100
       </mn>
      </mrow>
     </math>
     ,
     <math>
      <mrow>
       <msub>
        <mi>
         P
        </mi>
        <mtext>
         e
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        01
       </mn>
      </mrow>
     </math>
     ,
     <math>
      <mrow>
       <msub>
        <mi>
         G
        </mi>
        <mi>
         e
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        3
       </mn>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mi>
        I
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        01
       </mn>
      </mrow>
     </math>
     . A simulação foi feita com duração
     <math>
      <mrow>
       <msub>
        <mi>
         T
        </mi>
        <mtext>
         sim
        </mtext>
       </msub>
       <mo>
        =
       </mo>
       <msup>
        <mn>
         10
        </mn>
        <mn>
         4
        </mn>
       </msup>
      </mrow>
     </math>
     passos de tamanho
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     .
    </p>
    <h1 class="articleSectionTitle">
     3. Conceitos básicos de teoria da informação
    </h1>
    <p>
     A teoria da informação foi desenvolvida por Claude Shannon em 1948 em um artigo cujo título se referia a ela como
     <i>
      teoria da comunicação
     </i>
     <span class="ref">
      <strong class="xref xrefblue">
       [23]
      </strong>
      <span class="refCtt closed">
       <span>
        [23] S. Haykin, Redes Neurais: Princípios e prática (Artmed editora, São Paulo, 2008).
       </span>
      </span>
     </span>
     <span class="ref footnote">
      <sup class="xref">
       3
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         2
        </strong>
        Alguns autores criticam a concepção de código neural. Ver, por exemplo, [3].
       </span>
      </span>
     </span>
     . Vários excelentes livros oferecem tratamento detalhado da teoria da informação, cobrindo seu desenvolvimento histórico e aplicações em campos distintos [
     <span class="ref">
      <sup class="xref xrefblue">
       22
      </sup>
      <span class="refCtt closed">
       <span>
        [22] J.R. Pierce, An introduction to information theory: symbols, signals and noise (Courier Corporation, Mineola, 2012).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [23] S. Haykin, Redes Neurais: Princípios e prática (Artmed editora, São Paulo, 2008).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [24] D.J.C. MacKay, Information theory, inference and learning algorithms (Cambridge University Press, Cambridge, 2003), v. 4
       </span>
      </span>
     </span>
     -
     <span class="ref">
      <sup class="xref xrefblue">
       25
      </sup>
      <span class="refCtt closed">
       <span>
        [25] J.V. Stone, Information theory: a tutorial introduction (Sebtel Press, Sheffield, 2015), v. 2
       </span>
      </span>
     </span>
     ].
    </p>
    <p>
     O objetivo desta seção é apresentar duas grandezas fundamentais da teoria, entropia e informação mútua discreta, para em seguida apresentar métodos de calculá-las para trens de disparos.
    </p>
    <h2>
     3.1. Entropia
    </h2>
    <p>
     Seja a variável aleatória:
    </p>
    <div class="row formula" id="eeq7">
     <a name="eq7">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (7)
       </span>
       <math display="block" id="m7">
        <mrow>
         <mi>
          X
         </mi>
         <mo>
          =
         </mo>
         <mo>
          {
         </mo>
         <msub>
          <mi>
           x
          </mi>
          <mi>
           k
          </mi>
         </msub>
         <mo>
          |
         </mo>
         <mi>
          k
         </mi>
         <mo>
          =
         </mo>
         <mn>
          1
         </mn>
         <mo>
          ,
         </mo>
         <mn>
          2
         </mn>
         <mo>
          ,
         </mo>
         <mo>
          .
         </mo>
         <mo>
          .
         </mo>
         <mo>
          .
         </mo>
         <mo>
          ,
         </mo>
         <mi>
          m
         </mi>
         <mo>
          }
         </mo>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     A ocorrência de um evento
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     em uma sequência de eventos ocorre com probabilidade:
    </p>
    <div class="row formula" id="eeq8">
     <a name="eq8">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (8)
       </span>
       <math display="block" id="m8">
        <mrow>
         <msub>
          <mi>
           p
          </mi>
          <mi>
           k
          </mi>
         </msub>
         <mo>
          =
         </mo>
         <mi>
          P
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           X
          </mi>
          <mo>
           =
          </mo>
          <msub>
           <mi>
            x
           </mi>
           <mi>
            k
           </mi>
          </msub>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     obedecendo:
    </p>
    <div class="row formula" id="eeq9">
     <a name="eq9">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (9)
       </span>
       <math display="block" id="m9">
        <mrow>
         <mn>
          0
         </mn>
         <mo>
          ≤
         </mo>
         <msub>
          <mi>
           p
          </mi>
          <mi>
           k
          </mi>
         </msub>
         <mo>
          ≤
         </mo>
         <mn>
          1
         </mn>
         <mspace width="1em">
         </mspace>
         <mtext>
          e
         </mtext>
         <mspace width="1em">
         </mspace>
         <munderover>
          <mo>
           ∑
          </mo>
          <mrow>
           <mi>
            k
           </mi>
           <mo>
            =
           </mo>
           <mn>
            1
           </mn>
          </mrow>
          <mi>
           m
          </mi>
         </munderover>
         <msub>
          <mi>
           p
          </mi>
          <mi>
           k
          </mi>
         </msub>
         <mo>
          =
         </mo>
         <mn>
          1
         </mn>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     A quantidade de informação
     <math>
      <msub>
       <mi>
        I
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     associada ao evento
     <math>
      <mi>
       k
      </mi>
     </math>
     foi definida por Shannon como,
    </p>
    <div class="row formula" id="eeq10">
     <a name="eq10">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (10)
       </span>
       <math display="block" id="m10">
        <mrow>
         <msub>
          <mi>
           I
          </mi>
          <mi>
           k
          </mi>
         </msub>
         <mo>
          =
         </mo>
         <mo form="prefix">
          log
         </mo>
         <mfenced close=")" open="(" separators="">
          <mfrac>
           <mn>
            1
           </mn>
           <msub>
            <mi>
             p
            </mi>
            <mi>
             k
            </mi>
           </msub>
          </mfrac>
         </mfenced>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     Observe que esta definição corresponde à noção intuitiva de informação. Para exemplificar, considere que o evento
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     possui probabilidade
     <math>
      <mrow>
       <msub>
        <mi>
         p
        </mi>
        <mi>
         k
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     de ocorrência, o que faz com que todos os outros eventos
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        i
       </mi>
      </msub>
     </math>
     com
     <math>
      <mrow>
       <mi>
        i
       </mi>
       <mo>
        ≠
       </mo>
       <mi>
        k
       </mi>
      </mrow>
     </math>
     tenham probabilidade
     <math>
      <mrow>
       <msub>
        <mi>
         p
        </mi>
        <mi>
         i
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     de ocorrência. Neste caso, a ocorrência do evento
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     na sequência não causa nenhuma surpresa, logo a informação obtida com sua observação é nula (
     <math>
      <mrow>
       <mo form="prefix">
        log
       </mo>
       <mo>
        (
       </mo>
       <mn>
        1
       </mn>
       <mo>
        /
       </mo>
       <mn>
        1
       </mn>
       <mo>
        )
       </mo>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
      </mrow>
     </math>
     ).
    </p>
    <p>
     Imagine agora o caso em que os eventos ocorrem com probabilidades distintas entre si. Cada evento observado dará uma quantidade de informação diferente. Em particular, eventos com menores probabilidades causarão maiores surpresas, resultando em maiores ganhos de informação. Antes da ocorrência do próximo evento na sequência, há uma incerteza a respeito de qual dos
     <math>
      <mi>
       m
      </mi>
     </math>
     possíveis eventos poderá aparecer. Quando o evento ocorre, digamos
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     , ganha-se uma quantidade de informação
     <math>
      <mrow>
       <msub>
        <mi>
         I
        </mi>
        <mi>
         k
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mo form="prefix">
        log
       </mo>
       <mrow>
        <mo>
         (
        </mo>
        <mn>
         1
        </mn>
        <mo>
         /
        </mo>
        <msub>
         <mi>
          p
         </mi>
         <mi>
          k
         </mi>
        </msub>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     e, por consequência, há uma redução na incerteza que é igual a essa quantidade de informação. Fica claro que a definição de Shannon associa informação ao grau de surpresa de um evento e faz corresponder o ganho de informação obtido pela observação de um evento a uma redução na incerteza
     <span class="ref">
      <strong class="xref xrefblue">
       [23]
      </strong>
      <span class="refCtt closed">
       <span>
        [23] S. Haykin, Redes Neurais: Princípios e prática (Artmed editora, São Paulo, 2008).
       </span>
      </span>
     </span>
     (veja a
     <a class="open-asset-modal" data-target="#ModalFigf2" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 2A
     </a>
     ).
    </p>
    <div class="row fig" id="f2">
     <a name="f2">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf2" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 2
      </strong>
      <br/>
      (A) Quantidade de informação de Shannon
      <math>
       <msub>
        <mi>
         I
        </mi>
        <mi>
         k
        </mi>
       </msub>
      </math>
      em função da probabilidade
      <math>
       <mi>
        p
       </mi>
      </math>
      de ocorrência do evento
      <math>
       <mi>
        k
       </mi>
      </math>
      . (B) Entropia informacional em função da probabilidade
      <math>
       <mi>
        p
       </mi>
      </math>
      quando apenas dois eventos podem ocorrer
      <math>
       <mrow>
        <msub>
         <mi>
          x
         </mi>
         <mi>
          k
         </mi>
        </msub>
        <mo>
         =
        </mo>
        <mrow>
         <mo>
          {
         </mo>
         <msub>
          <mi>
           x
          </mi>
          <mn>
           1
          </mn>
         </msub>
         <mo>
          ,
         </mo>
         <msub>
          <mi>
           x
          </mi>
          <mn>
           2
          </mn>
         </msub>
         <mo>
          }
         </mo>
        </mrow>
       </mrow>
      </math>
      com probabilidades
      <math>
       <mrow>
        <msub>
         <mi>
          p
         </mi>
         <mi>
          k
         </mi>
        </msub>
        <mo>
         =
        </mo>
        <mrow>
         <mo>
          {
         </mo>
         <mi>
          p
         </mi>
         <mo>
          ,
         </mo>
         <mn>
          1
         </mn>
         <mo>
          −
         </mo>
         <mi>
          p
         </mi>
         <mo>
          }
         </mo>
        </mrow>
       </mrow>
      </math>
      respectivamente.
      <br/>
     </div>
    </div>
    <p>
     Pode-se calcular o logaritmo na
     <a class="open-asset-modal" data-target="#ModalSchemeeq10" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (10)
     </a>
     em qualquer base, desde que a mesma seja usada para todos os cálculos. A base usada determina a unidade em que se mede a quantidade de informação. Aqui será usada a base 2, para a qual a unidade de
     <math>
      <msub>
       <mi>
        I
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     é o
     <math>
      <mrow>
       <mi>
        b
       </mi>
       <mi>
        i
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     .
    </p>
    <p>
     A quantidade média de informação associada a uma dada sequência de eventos é definida como a entropia
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     da sequência:
    </p>
    <div class="row formula" id="eeq11">
     <a name="eq11">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (11)
       </span>
       <math display="block" id="m11">
        <mrow>
         <mi>
          H
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           X
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <mo>
          −
         </mo>
         <munderover>
          <mo>
           ∑
          </mo>
          <mrow>
           <mi>
            k
           </mi>
           <mo>
            =
           </mo>
           <mn>
            1
           </mn>
          </mrow>
          <mi>
           m
          </mi>
         </munderover>
         <msub>
          <mi>
           p
          </mi>
          <mi>
           k
          </mi>
         </msub>
         <mo form="prefix">
          log
         </mo>
         <mrow>
          <mo>
           (
          </mo>
          <msub>
           <mi>
            p
           </mi>
           <mi>
            k
           </mi>
          </msub>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <mi mathvariant="double-struck">
          E
         </mi>
         <mrow>
          <mo>
           [
          </mo>
          <msub>
           <mi>
            I
           </mi>
           <mi>
            k
           </mi>
          </msub>
          <mo>
           ]
          </mo>
         </mrow>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     Para ilustrar o significado de
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     , considere o caso simples em que
     <math>
      <mi>
       X
      </mi>
     </math>
     possui apenas dois valores possíveis
     <math>
      <mrow>
       <msub>
        <mi>
         x
        </mi>
        <mi>
         k
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mrow>
        <mo>
         {
        </mo>
        <msub>
         <mi>
          x
         </mi>
         <mn>
          1
         </mn>
        </msub>
        <mo>
         ,
        </mo>
        <msub>
         <mi>
          x
         </mi>
         <mn>
          2
         </mn>
        </msub>
        <mo>
         }
        </mo>
       </mrow>
      </mrow>
     </math>
     com probabilidades
     <math>
      <mrow>
       <msub>
        <mi>
         p
        </mi>
        <mi>
         k
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mrow>
        <mo>
         {
        </mo>
        <mi>
         p
        </mi>
        <mo>
         ,
        </mo>
        <mn>
         1
        </mn>
        <mo>
         −
        </mo>
        <mi>
         p
        </mi>
        <mo>
         }
        </mo>
       </mrow>
      </mrow>
     </math>
     , respectivamente. Para diferentes valores de
     <math>
      <mi>
       p
      </mi>
     </math>
     no intervalo [0,1], é possível ver que a entropia possui um máximo para o caso em que
     <math>
      <mrow>
       <mi>
        p
       </mi>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
       <mo>
        −
       </mo>
       <mi>
        p
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        5
       </mn>
      </mrow>
     </math>
     (veja
     <span class="ref">
      <strong class="xref xrefblue">
       figura 2B
      </strong>
      <span class="refCtt closed">
       <span>
        [2] W. Gerstner, W.M. Kistler, R. Naud e L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition (Cambridge University Press, Cambridge, 2014).
       </span>
      </span>
     </span>
     ). Tal fato mostra que entropia é uma medida de incerteza, pois o caso em que
     <math>
      <mrow>
       <mi>
        p
       </mi>
       <mo>
        =
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        5
       </mn>
      </mrow>
     </math>
     corresponde a uma distribuição uniforme (todos os eventos têm a mesma probabilidade) e este é o caso com maior incerteza a respeito do próximo valor na sequência de eventos.
    </p>
    <p>
     A entropia é limitada ao intervalo
     <math>
      <mrow>
       <mn>
        0
       </mn>
       <mo>
        ≤
       </mo>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        )
       </mo>
       <mo>
        ≤
       </mo>
       <mo form="prefix">
        log
       </mo>
       <mo>
        (
       </mo>
       <mi>
        m
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     , assumindo valor zero para o caso em que um dos eventos
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     tenha probabilidade
     <math>
      <mrow>
       <msub>
        <mi>
         p
        </mi>
        <mi>
         k
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     de ocorrência (incerteza mínima) e valor
     <math>
      <mrow>
       <mo form="prefix">
        log
       </mo>
       <mo>
        (
       </mo>
       <mi>
        m
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     para o caso em que todos os eventos
     <math>
      <msub>
       <mi>
        x
       </mi>
       <mi>
        k
       </mi>
      </msub>
     </math>
     sejam equiprováveis com probabilidade
     <math>
      <mrow>
       <msub>
        <mi>
         p
        </mi>
        <mi>
         k
        </mi>
       </msub>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
       <mo>
        /
       </mo>
       <mi>
        m
       </mi>
      </mrow>
     </math>
     (incerteza máxima).
    </p>
    <h2>
     3.2. Informação mútua
    </h2>
    <p>
     Para introduzir o conceito de informação mútua, considere o caso em que os eventos (pense neles como sinais em uma série temporal)
     <math>
      <mrow>
       <mi>
        x
       </mi>
       <mo>
        ∈
       </mo>
       <mi>
        X
       </mi>
      </mrow>
     </math>
     são transmitidas através de um canal ruidoso
     <span class="ref footnote">
      <sup class="xref">
       4
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         4
        </strong>
        É dito que X é a entrada do canal e Y a saída.
       </span>
      </span>
     </span>
     tendo sua saída observada em
     <math>
      <mrow>
       <mi>
        y
       </mi>
       <mo>
        ∈
       </mo>
       <mi>
        Y
       </mi>
      </mrow>
     </math>
     (veja a
     <a class="open-asset-modal" data-target="#ModalFigf3" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 3
     </a>
     ). Dada a existência do ruido, a saída observada pode não corresponder fielmente à entrada (por exemplo, a sequência binária 01001110 na entrada
     <math>
      <mi>
       X
      </mi>
     </math>
     se torna 01011100 na saída
     <math>
      <mi>
       Y
      </mi>
     </math>
     devido à ação do ruído sobre o terceiro evento na sequência).
    </p>
    <div class="row fig" id="f3">
     <a name="f3">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf3" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 3
      </strong>
      <br/>
      Entrada
      <math>
       <mi>
        X
       </mi>
      </math>
      sendo transmitida através de um canal contaminado com ruído
      <math>
       <mi>
        η
       </mi>
      </math>
      , gerando a saída
      <math>
       <mi>
        Y
       </mi>
      </math>
      .
      <br/>
     </div>
    </div>
    <p>
     Neste caso, o problema que se põe é o seguinte: como medir a incerteza restante em
     <math>
      <mi>
       X
      </mi>
     </math>
     após ser observado um único valor de
     <math>
      <mi>
       Y
      </mi>
     </math>
     ?
    </p>
    <p>
     Para responder a esta pergunta, comecemos definindo a grandeza chamada de entropia condicional pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq12" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (12)
     </a>
     :
    </p>
    <div class="row formula" id="eeq12">
     <a name="eq12">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (12)
       </span>
       <math display="block" id="m12">
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          |
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          =
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          −
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     A entropia condicional
     <span class="ref footnote">
      <sup class="xref">
       5
      </sup>
      <span class="refCtt closed">
       <span class="refCttPadding">
        <strong class="fn-title">
         2
        </strong>
        Alguns autores criticam a concepção de código neural. Ver, por exemplo, [3].
       </span>
      </span>
     </span>
     é a quantidade de incerteza restante sobre a entrada
     <math>
      <mi>
       X
      </mi>
     </math>
     após a saída
     <math>
      <mi>
       Y
      </mi>
     </math>
     ter sido observada [
     <span class="ref">
      <sup class="xref xrefblue">
       23
      </sup>
      <span class="refCtt closed">
       <span>
        [23] S. Haykin, Redes Neurais: Princípios e prática (Artmed editora, São Paulo, 2008).
       </span>
      </span>
     </span>
     ,
     <span class="ref">
      <sup class="xref xrefblue">
       25
      </sup>
      <span class="refCtt closed">
       <span>
        [25] J.V. Stone, Information theory: a tutorial introduction (Sebtel Press, Sheffield, 2015), v. 2
       </span>
      </span>
     </span>
     ]. O primeiro termo do lado direito da
     <a class="open-asset-modal" data-target="#ModalSchemeeq12" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (12)
     </a>
     é chamado de entropia conjunta, definida pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq13" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (13)
     </a>
     .
    </p>
    <div class="row formula" id="eeq13">
     <a name="eq13">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (13)
       </span>
       <math display="block" id="m13">
        <mrow>
         <mi>
          H
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           X
          </mi>
          <mo>
           ,
          </mo>
          <mi>
           Y
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <mo>
          −
         </mo>
         <munder>
          <mo>
           ∑
          </mo>
          <mrow>
           <mi>
            x
           </mi>
           <mo>
            ∈
           </mo>
           <mi>
            X
           </mi>
          </mrow>
         </munder>
         <munder>
          <mo>
           ∑
          </mo>
          <mrow>
           <mi>
            y
           </mi>
           <mo>
            ∈
           </mo>
           <mi>
            Y
           </mi>
          </mrow>
         </munder>
         <mi>
          p
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           x
          </mi>
          <mo>
           ,
          </mo>
          <mi>
           y
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo form="prefix">
          log
         </mo>
         <mi>
          p
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           x
          </mi>
          <mo>
           ,
          </mo>
          <mi>
           y
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <mrow>
       <mi>
        p
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        ,
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é a probabilidade de ocorrência do par
     <math>
      <mi>
       x
      </mi>
     </math>
     e
     <math>
      <mi>
       y
      </mi>
     </math>
     .
    </p>
    <p>
     Para o caso em que
     <math>
      <mi>
       X
      </mi>
     </math>
     e
     <math>
      <mi>
       Y
      </mi>
     </math>
     são estatisticamente independentes,
     <math>
      <mrow>
       <mi>
        p
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        ,
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
       <mo>
        =
       </mo>
       <mi>
        p
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        )
       </mo>
       <mi>
        p
       </mi>
       <mo>
        (
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     e a
     <a class="open-asset-modal" data-target="#ModalSchemeeq13" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (13)
     </a>
     pode ser reescrita como:
    </p>
    <div class="row formula" id="eeq14">
     <a name="eq14">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (14)
       </span>
       <math display="block" id="m14">
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          =
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          )
         </mo>
         <mo>
          +
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     O limite superior da entropia condicional é obtido pela substituição da
     <a class="open-asset-modal" data-target="#ModalSchemeeq14" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (14)
     </a>
     em
     <a class="open-asset-modal" data-target="#ModalSchemeeq12" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      (12)
     </a>
     resultando em
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        |
       </mo>
       <mi>
        Y
       </mi>
       <mo>
        )
       </mo>
       <mo>
        =
       </mo>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     . Ou seja, caso a entrada e a saída sejam descorrelacionadas, a incerteza em
     <math>
      <mi>
       X
      </mi>
     </math>
     se mantém inalterada independentemente do valor observado em
     <math>
      <mi>
       Y
      </mi>
     </math>
     . Por outro lado, se
     <math>
      <mrow>
       <mi>
        Y
       </mi>
       <mo>
        =
       </mo>
       <mi>
        X
       </mi>
      </mrow>
     </math>
     a incerteza é reduzida a zero, neste caso é dito que o canal é livre de ruído. Assim,
     <math>
      <mrow>
       <mn>
        0
       </mn>
       <mo>
        ≤
       </mo>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        |
       </mo>
       <mi>
        Y
       </mi>
       <mo>
        )
       </mo>
       <mo>
        ≤
       </mo>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     .
    </p>
    <p>
     Como a entropia
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é a incerteza acerca das entradas
     <math>
      <mi>
       X
      </mi>
     </math>
     antes de se observar a saída
     <math>
      <mi>
       Y
      </mi>
     </math>
     , e a entropia condicional
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        |
       </mo>
       <mi>
        Y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é a incerteza restante sobre
     <math>
      <mi>
       X
      </mi>
     </math>
     após a obervação de
     <math>
      <mi>
       Y
      </mi>
     </math>
     , é possível definir a redução média da incerteza sobre
     <math>
      <mi>
       X
      </mi>
     </math>
     após a observação de
     <math>
      <mi>
       Y
      </mi>
     </math>
     pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq15" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (15)
     </a>
     . Essa grandeza é chamada de informação mútua
     <math>
      <mrow>
       <mi>
        M
       </mi>
       <mi>
        I
       </mi>
       <mo>
        (
       </mo>
       <mi>
        X
       </mi>
       <mo>
        ;
       </mo>
       <mi>
        Y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     (do inglês para
     <i>
      mutual information
     </i>
     ) [
     <span class="ref">
      <sup class="xref xrefblue">
       23
      </sup>
      <span class="refCtt closed">
       <span>
        [23] S. Haykin, Redes Neurais: Princípios e prática (Artmed editora, São Paulo, 2008).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [24] D.J.C. MacKay, Information theory, inference and learning algorithms (Cambridge University Press, Cambridge, 2003), v. 4
       </span>
      </span>
     </span>
     -
     <span class="ref">
      <sup class="xref xrefblue">
       25
      </sup>
      <span class="refCtt closed">
       <span>
        [25] J.V. Stone, Information theory: a tutorial introduction (Sebtel Press, Sheffield, 2015), v. 2
       </span>
      </span>
     </span>
     ].
    </p>
    <div class="row formula" id="eeq15">
     <a name="eq15">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (15)
       </span>
       <math display="block" id="m15">
        <mrow>
         <mi>
          M
         </mi>
         <mi>
          I
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ;
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          =
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          )
         </mo>
         <mo>
          −
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          |
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     De uma forma intuitiva, a informação mútua pode ser entendida como a informação compartilhada por
     <math>
      <mi>
       X
      </mi>
     </math>
     e
     <math>
      <mi>
       Y
      </mi>
     </math>
     .
    </p>
    <p>
     É importante notar algumas propriedades da informação mútua:
    </p>
    <ul list-type="bullet">
     <li>
      <p>
       A informação mútua é simétrica, isto é,
       <math>
        <mrow>
         <mi>
          M
         </mi>
         <mi>
          I
         </mi>
         <mo>
          (
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          ;
         </mo>
         <mi>
          X
         </mi>
         <mo>
          )
         </mo>
         <mo>
          =
         </mo>
         <mi>
          M
         </mi>
         <mi>
          I
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ;
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       ;
      </p>
     </li>
     <li>
      <p>
       A informação mútua é não negativa, isto é,
       <math>
        <mrow>
         <mi>
          M
         </mi>
         <mi>
          I
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ;
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          ≥
         </mo>
         <mspace width="3.33333pt">
         </mspace>
         <mn>
          0
         </mn>
        </mrow>
       </math>
       .
      </p>
     </li>
    </ul>
    <p>
     Substituindo (12) em (15) e isolando a entropia conjunta, temos:
    </p>
    <div class="row formula" id="eeq16">
     <a name="eq16">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (16)
       </span>
       <math display="block" id="m16">
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          =
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          )
         </mo>
         <mo>
          +
         </mo>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          −
         </mo>
         <mi>
          M
         </mi>
         <mi>
          I
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     Dessa forma, a entropia conjunta age como uma espécie de contêiner para as várias entropias, incluindo a informação mútua
     <span class="ref">
      <strong class="xref xrefblue">
       [25]
      </strong>
      <span class="refCtt closed">
       <span>
        [25] J.V. Stone, Information theory: a tutorial introduction (Sebtel Press, Sheffield, 2015), v. 2
       </span>
      </span>
     </span>
     . A relação entre as grandezas apresentadas aqui é ilustrada na
     <a class="open-asset-modal" data-target="#ModalFigf4" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 4
     </a>
     . Resumindo:
    </p>
    <div class="row fig" id="f4">
     <a name="f4">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf4" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 4
      </strong>
      <br/>
      Relação entre as diversas entropias definidas e a informação mútua (ver texto).
      <br/>
     </div>
    </div>
    <ul list-type="bullet">
     <li>
      <p>
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       é a incerteza relativa a
       <math>
        <mi>
         X
        </mi>
       </math>
       , isto é, a incerteza sobre qual mensagem
       <math>
        <mi>
         x
        </mi>
       </math>
       será transmitida;
      </p>
     </li>
     <li>
      <p>
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       é a incerteza relativa a
       <math>
        <mi>
         Y
        </mi>
       </math>
       , isto é, a incerteza sobre qual mensagem
       <math>
        <mi>
         Y
        </mi>
       </math>
       será recebida;
      </p>
     </li>
     <li>
      <p>
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       é a incerteza sobre a ocorrência do par
       <math>
        <mi>
         x
        </mi>
       </math>
       e
       <math>
        <mi>
         y
        </mi>
       </math>
       , ela age como um contêiner para as várias componentes da entropia;
      </p>
     </li>
     <li>
      <p>
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          X
         </mi>
         <mo>
          |
         </mo>
         <mi>
          Y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       é a quantidade de incerteza restante sobre a entrada
       <math>
        <mi>
         X
        </mi>
       </math>
       após a saída
       <math>
        <mi>
         Y
        </mi>
       </math>
       ser observada.
      </p>
     </li>
    </ul>
    <h2>
     3.3. Teoria da informação em neurociência
    </h2>
    <p>
     Os conceitos da teoria da informação discutidos nas seções anteriores começaram a ser aplicados em neurociência poucos anos após o aparecimento do artigo de Shannon, tendo sido intensamente utilizados desde então [
     <span class="ref">
      <sup class="xref xrefblue">
       8
      </sup>
      <span class="refCtt closed">
       <span>
        [9] L. Paninski, Neural. Comput. 15, 1191 (2003).
       </span>
      </span>
     </span>
     ,
     <span class="ref">
      <sup class="xref xrefblue">
       26
      </sup>
      <span class="refCtt closed">
       <span>
        [26] F. Attneave, Psychol. Rev. 61, 183 (1954).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [27] H.B. Barlow, Current problems in animal behaviour (Cambridge University Press, Cambridge, 1961).
       </span>
      </span>
     </span>
     -
     <span class="ref">
      <sup class="xref xrefblue">
       28
      </sup>
      <span class="refCtt closed">
       <span>
        [28] A.G. Dimitrov, A.A Lazar e J.D. Victor, J. Comput. Neurosci. 30, 1 (2011).
       </span>
      </span>
     </span>
     ].
    </p>
    <p>
     Algumas das principais contribuições atuais da teoria da informação na neurociência permitem:
    </p>
    <ul list-type="bullet">
     <li>
      <p>
       estudar a capacidade de transmissão de informação pelas células nervosas e quantificar a informação que flui através do sistema nervoso [
       <span class="ref">
        <sup class="xref xrefblue">
         29
        </sup>
        <span class="refCtt closed">
         <span>
          [29] D. MacKay e W.S. McCulloch, Bull. Math. Biophys. 14, 127 (1952).
         </span>
        </span>
       </span>
       ,
       <span class="ref">
        <sup class="xref xrefblue">
         30
        </sup>
        <span class="refCtt closed">
         <span>
          [30] S.P. Strong, R. Koberle, R.R. de Ruyter van Steveninck, W. Bialek, Phys. Rev. Lett. 80, 197 (1998).
         </span>
        </span>
       </span>
       ];
      </p>
     </li>
     <li>
      <p>
       inferir a conectividade estrutural de redes neurais biológicas por meio do registro da sua atividade elétrica [
       <span class="ref">
        <sup class="xref xrefblue">
         31
        </sup>
        <span class="refCtt closed">
         <span>
          [31] J.G. Orlandi, O. Stetter, J. Soriano, T. Geisel e D. Battaglia, Plos One 9, e98842 (2014).
         </span>
        </span>
       </span>
       ,
       <span class="ref">
        <sup class="xref xrefblue">
         32
        </sup>
        <span class="refCtt closed">
         <span>
          [32] S.A. Neymotin, K.M. Jacobs, A.A. Fenton e W.W. Lytton, J. Comput. Neurosci. 30, 69 (2011)
         </span>
        </span>
       </span>
       ]. Medidas de entropia demonstram que correlações na atividade elétrica das redes são dependentes do grau de conectividade interna;
      </p>
     </li>
     <li>
      <p>
       inferir a conectividade funcional de redes neurais biológicas por meio de dados de EEG, MEG ou fMRI [
       <span class="ref">
        <sup class="xref xrefblue">
         33
        </sup>
        <span class="refCtt closed">
         <span>
          [33] M. Wibral, B. Rahm, M. Rieder, M. Lindner, R. Vicente e J. Kaiser, Prog. Biophys. Mol. Bio. 105, 80 (2011).
         </span>
        </span>
       </span>
       ,
       <span class="ref">
        <sup class="xref xrefblue">
         34
        </sup>
        <span class="refCtt closed">
         <span>
          [34] W. Liao, J. Ding, D. Marinazzo, Q. Xu, Z. Wang, C. Yuan, Z. Zhang, G. Lu e H.Chen, Neuroimage 54, 2683 (2011).
         </span>
        </span>
       </span>
       ]. Regiões cerebrais não necessariamente conectadas apresentam atividade correlacionada interpretada como uma conexão funcional entre elas.
      </p>
     </li>
    </ul>
    <h1 class="articleSectionTitle">
     4. Medindo a informação de trens de disparo
    </h1>
    <p>
     Nesta seção, são utilizadas grandezas da teoria da informação para determinar a quantidade de informação carregada por trens de disparos neuronais e recuperar a matriz de conectividade de uma rede de neurônios binários.
    </p>
    <h2>
     4.1. Método direto
    </h2>
    <p>
     Para um neurônio binário (ver seção 2), considera-se que existe uma probabilidade de disparo
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        d
       </mtext>
      </msub>
     </math>
     e uma probabilidade de silêncio
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        s
       </mtext>
      </msub>
     </math>
     (a princípio estacionárias). Essas probabilidades podem ser estimadas a partir do trem de disparos:
    </p>
    <div class="row formula" id="eeq17">
     <a name="eq17">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (17)
       </span>
       <math display="block" id="m17">
        <mrow>
         <msub>
          <mi>
           P
          </mi>
          <mtext>
           d
          </mtext>
         </msub>
         <mo>
          =
         </mo>
         <mfrac>
          <mrow>
           <msubsup>
            <mo>
             ∑
            </mo>
            <mrow>
             <mi>
              i
             </mi>
             <mo>
              =
             </mo>
             <mn>
              1
             </mn>
            </mrow>
            <mi>
             N
            </mi>
           </msubsup>
           <mi>
            x
           </mi>
           <mrow>
            <mo>
             (
            </mo>
            <msub>
             <mi>
              t
             </mi>
             <mi>
              i
             </mi>
            </msub>
            <mo>
             )
            </mo>
           </mrow>
          </mrow>
          <mi>
           N
          </mi>
         </mfrac>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     e,
    </p>
    <div class="row formula" id="eeq18">
     <a name="eq18">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (18)
       </span>
       <math display="block" id="m18">
        <mrow>
         <msub>
          <mi>
           P
          </mi>
          <mtext>
           s
          </mtext>
         </msub>
         <mo>
          =
         </mo>
         <mn>
          1
         </mn>
         <mo>
          −
         </mo>
         <msub>
          <mi>
           P
          </mi>
          <mtext>
           d
          </mtext>
         </msub>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <mi>
       N
      </mi>
     </math>
     é o número de passos na simulação do neurônio.
    </p>
    <p>
     Em seguida, a entropia pode ser determinada utilizando diretamente a
     <a class="open-asset-modal" data-target="#ModalSchemeeq11" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (11)
     </a>
     .
    </p>
    <p>
     A informação mútua entre dois trens de disparos
     <math>
      <mrow>
       <mi>
        x
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mi>
        y
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é calculada utilizando-se a
     <a class="open-asset-modal" data-target="#ModalSchemeeq16" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação 16
     </a>
     . Isso requer que se determine, além das entropias dos dois trens de disparos
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     , a entropia da distribuição conjunta
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        ,
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     . Note que a dependência do tempo nos trens de disparos é omitida com o intuito de não carregar a notação.
    </p>
    <p>
     A determinação dessas entropias pode ser feita seguindo os passos abaixo:
    </p>
    <ol type="1">
     <li>
      <p>
       Determina-se as probabilidade de disparo e silêncio, para os trens de disparo
       <math>
        <mrow>
         <mi>
          x
         </mi>
         <mo>
          (
         </mo>
         <mi>
          t
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       e
       <math>
        <mrow>
         <mi>
          y
         </mi>
         <mo>
          (
         </mo>
         <mi>
          t
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       , utilizando as
       <a class="open-asset-modal" data-target="#ModalSchemeeq17" data-toggle="modal" href="">
        <span class="sci-ico-fileFormula">
        </span>
        equações (17)
       </a>
       e
       <a class="open-asset-modal" data-target="#ModalSchemeeq18" data-toggle="modal" href="">
        <span class="sci-ico-fileFormula">
        </span>
        (18)
       </a>
       ;
      </p>
     </li>
     <li>
      <p>
       Com as probabilidades encontradas no passo anterior, determina-se as entropias
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          x
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       e
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       com a
       <a class="open-asset-modal" data-target="#ModalSchemeeq11" data-toggle="modal" href="">
        <span class="sci-ico-fileFormula">
        </span>
        equação (11)
       </a>
       ;
      </p>
     </li>
     <li>
      <p>
       A determinação da probabilidade conjunta é feita em 3 sub-passos:
      </p>
      <ol type="a">
       <li>
        <p>
         Desloca-se o trem de disparos
         <math>
          <mrow>
           <mi>
            y
           </mi>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            )
           </mo>
          </mrow>
         </math>
         por um intervalo de tempo
         <math>
          <mi>
           τ
          </mi>
         </math>
         em relação ao trem de disparos
         <math>
          <mrow>
           <mi>
            x
           </mi>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            )
           </mo>
          </mrow>
         </math>
         ;
        </p>
       </li>
       <li>
        <p>
         Em seguida, para cada intervalo
         <math>
          <mrow>
           <mi>
            Δ
           </mi>
           <mi>
            t
           </mi>
          </mrow>
         </math>
         , analisa-se o par de valores
         <math>
          <mrow>
           <mi>
            x
           </mi>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            )
           </mo>
          </mrow>
         </math>
         e
         <math>
          <mrow>
           <mi>
            y
           </mi>
           <mo>
            (
           </mo>
           <mi>
            t
           </mi>
           <mo>
            +
           </mo>
           <mi>
            τ
           </mi>
           <mo>
            )
           </mo>
          </mrow>
         </math>
         para se determinar o número de ocorrências de cada um dos pares, [0, 0], [0,1], [1,0] e [1,1] e calcular a probabilidade de ocorrência de cada um deles;
        </p>
       </li>
       <li>
        <p>
         Com as probabilidades encontradas no passo anterior, calcula-se a entropia conjunta dada pela
         <a class="open-asset-modal" data-target="#ModalSchemeeq13" data-toggle="modal" href="">
          <span class="sci-ico-fileFormula">
          </span>
          equação (13)
         </a>
         .
        </p>
       </li>
      </ol>
     </li>
     <li>
      <p>
       Após a determinação das entropias
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          x
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       ,
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       e
       <math>
        <mrow>
         <mi>
          H
         </mi>
         <mo>
          (
         </mo>
         <mi>
          x
         </mi>
         <mo>
          ,
         </mo>
         <mi>
          y
         </mi>
         <mo>
          )
         </mo>
        </mrow>
       </math>
       , a informação mútua pode ser determinada com a
       <a class="open-asset-modal" data-target="#ModalSchemeeq16" data-toggle="modal" href="">
        <span class="sci-ico-fileFormula">
        </span>
        equação (16)
       </a>
       .
      </p>
     </li>
    </ol>
    <p>
     A
     <a class="open-asset-modal" data-target="#ModalFigf5" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 5
     </a>
     , resume os passos para determinação da entropia
     <math>
      <mrow>
       <mi>
        H
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        ,
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     .
    </p>
    <div class="row fig" id="f5">
     <a name="f5">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf5" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 5
      </strong>
      <br/>
      Resumo do método para determinação da entropia conjunta
      <math>
       <mrow>
        <mi>
         H
        </mi>
        <mo>
         (
        </mo>
        <mi>
         x
        </mi>
        <mo>
         ,
        </mo>
        <mi>
         y
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      explicada no texto. Após a obtenção dos trens de disparo
      <math>
       <mrow>
        <mi>
         x
        </mi>
        <mo>
         (
        </mo>
        <mi>
         t
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      e
      <math>
       <mrow>
        <mi>
         y
        </mi>
        <mo>
         (
        </mo>
        <mi>
         t
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      (ver seção 1), desloca-se um deles por um valor
      <math>
       <mi>
        τ
       </mi>
      </math>
      , e determina-se a probabilidade de ocorrência dos pares [0,0], [0,1], [1,0] e [1,1] (retângulos vermelhos tracejados) para em seguida determinar
      <math>
       <mrow>
        <mi>
         H
        </mi>
        <mo>
         (
        </mo>
        <mi>
         x
        </mi>
        <mo>
         ,
        </mo>
        <mi>
         y
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      .
      <br/>
     </div>
    </div>
    <p>
     O valor de
     <math>
      <mi>
       τ
      </mi>
     </math>
     para o qual a informação mútua entre
     <math>
      <mrow>
       <mi>
        x
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mi>
        y
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        +
       </mo>
       <mi>
        τ
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é máxima é o tempo que a informação leva para ir de um neurônio ao outro.
    </p>
    <p>
     As estimativas da entropia e da informação mútua por meio das probabilidades
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        d
       </mtext>
      </msub>
     </math>
     e
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        s
       </mtext>
      </msub>
     </math>
     e das probabilidades de ocorrência dos pares [0,0], [0,1], [1,0] e [1,1] para duas séries
     <math>
      <mrow>
       <mi>
        x
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mi>
        y
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     podem ser consideradas boas para neurônios cujos disparos se dão de forma independente. Para o caso em que há autocorrelações em um trem de disparos, as probabilidades
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        s
       </mtext>
      </msub>
     </math>
     e
     <math>
      <msub>
       <mi>
        P
       </mi>
       <mtext>
        d
       </mtext>
      </msub>
     </math>
     são erroneamente estimadas. Neste caso, deve-se adotar uma estratégia em que se determina a probabilidade de ocorrência de um bloco constituído de uma sequência binária dentro de um trem de disparos. Para isso, primeiro determina-se essas probabilidades aplicando-se diversos estímulos ao neurônio e amostrando sua resposta, i.e. os trens de disparos.
    </p>
    <p>
     Em seguida, repete-se o procedimento porém com um estímulo fixo, de modo que a distribuição de probabilidade do conjunto de trens de disparos seja a probabilidade condicional
     <math>
      <mrow>
       <mi>
        P
       </mi>
       <mo>
        (
       </mo>
       <mtext>
        resposta
       </mtext>
       <mo>
        |
       </mo>
       <mtext>
        estímulo
       </mtext>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     . O cálculo da entropia utilizando essa distribuição dará a entropia condicional (
     <a class="open-asset-modal" data-target="#ModalSchemeeq12" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (12)
     </a>
     ), ou seja, nesta etapa determina-se a informação relativa ao ruído existente na resposta do neurônio. A razão para isso é que com repetidas apresentações do mesmo estímulo, respostas ruidosas tornam-se menos aparentes em meio ao padrão típico de resposta ao neurônio
     <span class="ref">
      <strong class="xref xrefblue">
       [30]
      </strong>
      <span class="refCtt closed">
       <span>
        [30] S.P. Strong, R. Koberle, R.R. de Ruyter van Steveninck, W. Bialek, Phys. Rev. Lett. 80, 197 (1998).
       </span>
      </span>
     </span>
     . A
     <a class="open-asset-modal" data-target="#ModalFigf6" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 6
     </a>
     resume esquematicamente este procedimento, denominado “método direto”.
    </p>
    <div class="row fig" id="f6">
     <a name="f6">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf6" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 6
      </strong>
      <br/>
      Esquema do método direto. (A) O modelo de neurônio é submetido a um sinal acompanhado de ruído, gerando uma série temporal que, em seguida, é convertida em um trem de disparos. (B) A entropia é determinada pela estimulação do neurônio com diversos sinais. Mede-se as respostas
      <math>
       <mi>
        R
       </mi>
      </math>
      do neurônio para, em seguida, determinar a distribuição
      <math>
       <mrow>
        <mi>
         P
        </mi>
        <mo>
         (
        </mo>
        <mi>
         R
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      , com a qual pode-se calcular a entropia utilizando a
      <a class="open-asset-modal" data-target="#ModalSchemeeq11" data-toggle="modal" href="">
       <span class="sci-ico-fileFormula">
       </span>
       equação (11)
      </a>
      . (C) O ruído é determinado do mesmo modo, porém utiliza-se sempre um sinal fixo para estimular o neurônio.
      <br/>
     </div>
    </div>
    <p>
     Embora o método direto dê o valor real da informação contida no trem de disparos e tenha a vantagem de não necessitar de conhecimento prévio acerca da natureza do estímulo aplicado, na prática, sua implementação é difícil devido ao chamado “problema da dimensionalidade”. Por exemplo, uma simulação de duração
     <math>
      <msub>
       <mi>
        T
       </mi>
       <mrow>
        <mi>
         s
        </mi>
        <mi>
         i
        </mi>
        <mi>
         m
        </mi>
       </mrow>
      </msub>
     </math>
     com resolução temporal
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     gera um trem de disparos com
     <math>
      <mrow>
       <mi>
        L
       </mi>
       <mo>
        =
       </mo>
       <msub>
        <mi>
         T
        </mi>
        <mrow>
         <mi>
          s
         </mi>
         <mi>
          i
         </mi>
         <mi>
          m
         </mi>
        </mrow>
       </msub>
       <mo>
        /
       </mo>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     <i>
      bins
     </i>
     . Como o trem de disparos é binário, existem
     <math>
      <msup>
       <mn>
        2
       </mn>
       <mi>
        L
       </mi>
      </msup>
     </math>
     possíveis respostas do neurônio, ou seja, para estimar as distribuições de probabilidade das respostas é necessário uma grande quantidade de dados, o que geralmente leva a problemas de subamostragem [
     <span class="ref">
      <sup class="xref xrefblue">
       19
      </sup>
      <span class="refCtt closed">
       <span>
        [19] D. Bernardi e B. Lindner, J. Neurophysiol. 113, 1342 (2015).
       </span>
      </span>
     </span>
     ,
     <span class="ref">
      <sup class="xref xrefblue">
       35
      </sup>
      <span class="refCtt closed">
       <span>
        [35] A. Borst e F.E. Theunissen, Nat. Neurosci. 2, 947 (1999).
       </span>
      </span>
     </span>
     ].
    </p>
    <p>
     Além do problema da dimensionalidade, o método direto não indica quais aspectos do sinal são representados pela resposta neuronal.
    </p>
    <p>
     Uma maneira de contornar os problemas do método direto é criando-se estimativas para a informação mútua, como, por exemplo, o limite inferior para a informação mútua baseado na função de coerência entre entrada e saída. Isso será discutido na próxima seção.
    </p>
    <h2>
     4.2. Informação mútua dependente da frequência
    </h2>
    <p>
     Outra forma de calcular a quantidade de informação contida no trem de disparos
     <math>
      <mrow>
       <mi>
        x
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     de um neurônio em resposta a um dado estímulo
     <math>
      <mrow>
       <mi>
        s
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é pela chamada “coerência espectral”
     <math>
      <mrow>
       <msub>
        <mi>
         C
        </mi>
        <mrow>
         <mi>
          x
         </mi>
         <mi>
          s
         </mi>
        </mrow>
       </msub>
       <mrow>
        <mo>
         (
        </mo>
        <mi>
         f
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     , definida pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq19" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (19)
     </a>
     . A coerência espectral é dada pelo quadrado do coeficiente de correlação linear entre o estímulo e a resposta no domínio da frequência, sendo limitada entre
     <math>
      <mrow>
       <mn>
        0
       </mn>
       <mo>
        ≤
       </mo>
       <msub>
        <mi>
         C
        </mi>
        <mrow>
         <mi>
          x
         </mi>
         <mi>
          s
         </mi>
        </mrow>
       </msub>
       <mo>
        ≤
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     .
    </p>
    <div class="row formula" id="eeq19">
     <a name="eq19">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (19)
       </span>
       <math display="block" id="m19">
        <mrow>
         <msub>
          <mi>
           C
          </mi>
          <mtext>
           xs
          </mtext>
         </msub>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           f
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <munder>
          <mo form="prefix" movablelimits="true">
           lim
          </mo>
          <mrow>
           <mi>
            T
           </mi>
           <mo>
            →
           </mo>
           <mi>
            ∞
           </mi>
          </mrow>
         </munder>
         <mstyle displaystyle="true" scriptlevel="0">
          <mfrac>
           <mrow>
            <mrow>
             <mo>
              |
             </mo>
            </mrow>
            <mover accent="true">
             <mi>
              x
             </mi>
             <mo>
              ˜
             </mo>
            </mover>
            <mrow>
             <mo>
              (
             </mo>
             <mi>
              f
             </mi>
             <mo>
              )
             </mo>
            </mrow>
            <msup>
             <mover accent="true">
              <mi>
               s
              </mi>
              <mo>
               ˜
              </mo>
             </mover>
             <mo>
              *
             </mo>
            </msup>
            <msup>
             <mrow>
              <mrow>
               <mo>
                (
               </mo>
               <mi>
                f
               </mi>
               <mo>
                )
               </mo>
              </mrow>
              <mo>
               |
              </mo>
             </mrow>
             <mn>
              2
             </mn>
            </msup>
           </mrow>
           <mrow>
            <mrow>
             <mo>
              〈
             </mo>
             <mo>
              |
             </mo>
            </mrow>
            <mover accent="true">
             <mi>
              x
             </mi>
             <mo>
              ˜
             </mo>
            </mover>
            <msup>
             <mrow>
              <mrow>
               <mo>
                (
               </mo>
               <mi>
                f
               </mi>
               <mo>
                )
               </mo>
              </mrow>
              <mo>
               |
              </mo>
             </mrow>
             <mn>
              2
             </mn>
            </msup>
            <mrow>
             <mo>
              〉
             </mo>
             <mo>
              〈
             </mo>
             <mo>
              |
             </mo>
            </mrow>
            <mover accent="true">
             <mi>
              s
             </mi>
             <mo>
              ˜
             </mo>
            </mover>
            <mrow>
             <mo>
              (
             </mo>
             <mi>
              f
             </mi>
             <mo>
              )
             </mo>
            </mrow>
            <mrow>
             <msup>
              <mo>
               |
              </mo>
              <mn>
               2
              </mn>
             </msup>
             <mo>
              〉
             </mo>
            </mrow>
           </mrow>
          </mfrac>
         </mstyle>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     onde
     <math>
      <mrow>
       <mo>
        〈
       </mo>
       <mo>
        〉
       </mo>
      </mrow>
     </math>
     indicam médias feitas por diversas repetições e
     <math>
      <mrow>
       <mover accent="true">
        <mi>
         x
        </mi>
        <mo>
         ˜
        </mo>
       </mover>
       <mrow>
        <mo>
         (
        </mo>
        <mi>
         f
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     e
     <math>
      <mrow>
       <mover accent="true">
        <mi>
         s
        </mi>
        <mo>
         ˜
        </mo>
       </mover>
       <mrow>
        <mo>
         (
        </mo>
        <mi>
         f
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     são, respectivamente, as transformadas de Fourier do trem de disparos neuronal e do sinal, dadas por:
    </p>
    <div class="row formula" id="eeq20">
     <a name="eq20">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (20)
       </span>
       <math display="block" id="m20">
        <mrow>
         <mover accent="true">
          <mi>
           x
          </mi>
          <mo>
           ˜
          </mo>
         </mover>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           f
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <msubsup>
          <mo>
           ∫
          </mo>
          <mrow>
           <mn>
            0
           </mn>
          </mrow>
          <mi>
           T
          </mi>
         </msubsup>
         <mi>
          x
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <msup>
          <mi>
           e
          </mi>
          <mrow>
           <mn>
            2
           </mn>
           <mi>
            π
           </mi>
           <mi>
            i
           </mi>
           <mi>
            f
           </mi>
          </mrow>
         </msup>
         <mi>
          d
         </mi>
         <mi>
          t
         </mi>
         <mo>
          ,
         </mo>
         <mspace width="1em">
         </mspace>
         <mover accent="true">
          <mi>
           s
          </mi>
          <mo>
           ˜
          </mo>
         </mover>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           f
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <mo>
          =
         </mo>
         <msubsup>
          <mo>
           ∫
          </mo>
          <mrow>
           <mn>
            0
           </mn>
          </mrow>
          <mi>
           T
          </mi>
         </msubsup>
         <mi>
          s
         </mi>
         <mrow>
          <mo>
           (
          </mo>
          <mi>
           t
          </mi>
          <mo>
           )
          </mo>
         </mrow>
         <msup>
          <mi>
           e
          </mi>
          <mrow>
           <mn>
            2
           </mn>
           <mi>
            π
           </mi>
           <mi>
            i
           </mi>
           <mi>
            f
           </mi>
          </mrow>
         </msup>
         <mi>
          d
         </mi>
         <mi>
          t
         </mi>
         <mo>
          ,
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     em que
     <math>
      <mi>
       T
      </mi>
     </math>
     é a janela de tempo usada para integração.
    </p>
    <p>
     O limite inferior da informação mútua (
     <math>
      <msub>
       <mi>
        I
       </mi>
       <mrow>
        <mi>
         l
        </mi>
        <mi>
         b
        </mi>
       </mrow>
      </msub>
     </math>
     ) é dado pela
     <a class="open-asset-modal" data-target="#ModalSchemeeq21" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (21)
     </a>
     :
    </p>
    <div class="row formula" id="eeq21">
     <a name="eq21">
     </a>
     <div class="col-md-12">
      <div class="formula-container">
       <span class="label">
        (21)
       </span>
       <math display="block" id="m21">
        <mrow>
         <msub>
          <mi>
           I
          </mi>
          <mtext>
           lb
          </mtext>
         </msub>
         <mo>
          =
         </mo>
         <mo>
          −
         </mo>
         <msubsup>
          <mo>
           ∫
          </mo>
          <mrow>
           <mn>
            0
           </mn>
          </mrow>
          <msub>
           <mi>
            f
           </mi>
           <mi>
            c
           </mi>
          </msub>
         </msubsup>
         <msub>
          <mo form="prefix">
           log
          </mo>
          <mn>
           2
          </mn>
         </msub>
         <mrow>
          <mo>
           (
          </mo>
          <mn>
           1
          </mn>
          <mo>
           −
          </mo>
          <msub>
           <mi>
            C
           </mi>
           <mtext>
            xs
           </mtext>
          </msub>
          <mrow>
           <mo>
            (
           </mo>
           <mi>
            f
           </mi>
           <mo>
            )
           </mo>
          </mrow>
          <mo>
           )
          </mo>
         </mrow>
         <mi>
          d
         </mi>
         <mi>
          f
         </mi>
         <mo>
          .
         </mo>
        </mrow>
       </math>
      </div>
     </div>
    </div>
    <p>
     Esse limite inferior para a taxa de informação mútua (MIR, do inglês “
     <i>
      mutual information rate
     </i>
     ”) é valido somente para o caso em que a estatística do sinal de entrada
     <math>
      <mrow>
       <mi>
        s
       </mi>
       <mo>
        (
       </mo>
       <mi>
        t
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     é gaussiana.
    </p>
    <p>
     Um aspecto interessante do cálculo da informação utilizando a
     <a class="open-asset-modal" data-target="#ModalSchemeeq21" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equação (21)
     </a>
     é que o termo no interior da integral pode ser visto como a taxa de informação dependente da frequência. Assim, diferentemente do que acontece no método direto, em que não se sabe quais aspectos do sinal são representados pela resposta neuronal, aqui têm-se uma noção disso. Observe que plotando
     <math>
      <mrow>
       <mo>
        −
       </mo>
       <msub>
        <mo form="prefix">
         log
        </mo>
        <mn>
         2
        </mn>
       </msub>
       <mrow>
        <mo>
         (
        </mo>
        <mn>
         1
        </mn>
        <mo>
         −
        </mo>
        <msub>
         <mi>
          C
         </mi>
         <mtext>
          xs
         </mtext>
        </msub>
        <mrow>
         <mo>
          (
         </mo>
         <mi>
          f
         </mi>
         <mo>
          )
         </mo>
        </mrow>
        <mo>
         )
        </mo>
       </mrow>
      </mrow>
     </math>
     <i>
      versus
     </i>
     <math>
      <mi>
       f
      </mi>
     </math>
     é possível determinar quanta informação do sinal é processada em cada frequência.
    </p>
    <p>
     Para demonstrar esses conceitos, os modelos de neurônios descritos na seção 2 foram implementados Python e, a partir dos resultados obtidos, foram determinadas as curvas para a MIR e o valor total da informação
     <math>
      <msub>
       <mi>
        I
       </mi>
       <mtext>
        lb
       </mtext>
      </msub>
     </math>
     .
    </p>
    <p>
     Na
     <a class="open-asset-modal" data-target="#ModalFigf7" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 7
     </a>
     , pode-se observar que o neurônio Poisson transmite informação igualmente em todas as frequências, enquanto que o neurônio LIF estocástico carrega informação preferencialmente nas baixas frequências.
    </p>
    <div class="row fig" id="f7">
     <a name="f7">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf7" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 7
      </strong>
      <br/>
      Curvas da taxa de informação mútua dependente da frequência para os casos em que utilizou-se (i) o neurônio Poisson, (ii) o neurônio LIF com sinal fraco e (iii) o neurônio LIF com sinal forte. É possível ver que o neurônio Poisson processa informação igualmente em todas frequências, enquanto que o neurônio LIF processa preferencialmente nas baixas frequências. Além disso, o neurônio LIF é capaz de transmitir uma quantidade maior de informação (especialmente no caso com sinal forte, veja a discussão no texto).
      <br/>
     </div>
    </div>
    <p>
     A integração das curvas na
     <a class="open-asset-modal" data-target="#ModalFigf7" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 7
     </a>
     dá o valor total da MIR para cada um dos neurônios. A capacidade do LIF estocástico, tanto para o caso com sinal fraco quanto para o sinal forte (
     <math>
      <mrow>
       <mn>
        20
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        97
       </mn>
       <mo>
        ±
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        22
       </mn>
      </mrow>
     </math>
     bits/s e
     <math>
      <mrow>
       <mn>
        139
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        44
       </mn>
       <mo>
        ±
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        65
       </mn>
      </mrow>
     </math>
     bits/s respectivamente) é superior à do neurônio Poisson (
     <math>
      <mrow>
       <mn>
        3
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        33
       </mn>
       <mo>
        ±
       </mo>
       <mn>
        0
       </mn>
       <mo>
        ,
       </mo>
       <mn>
        08
       </mn>
      </mrow>
     </math>
     bits/s).
    </p>
    <p>
     Observe que apesar das medidas apresentadas nessa seção se restringirem a sinais de origem gaussiana, a sua implementação prática é mais viável que as do método direto.
    </p>
    <h1 class="articleSectionTitle">
     5. Determinação da conectividade de uma rede
    </h1>
    <p>
     Nesta seção, com o objetivo de determinar a matriz de adjacência de uma rede de neurônio binários, utiliza-se o método discutido na seção 4.1. Em particular, estima-se as probabilidades de disparo e de silêncio, dadas pelas
     <a class="open-asset-modal" data-target="#ModalSchemeeq17" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      equações (17
     </a>
     e
     <a class="open-asset-modal" data-target="#ModalSchemeeq18" data-toggle="modal" href="">
      <span class="sci-ico-fileFormula">
      </span>
      18
     </a>
     ), para calcular a entropia dos trens de disparos. Para determinar a informação mútua entre pares de neurônios, utiliza-se o algorítimo esquematizado na
     <a class="open-asset-modal" data-target="#ModalFigf5" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 5
     </a>
     .
    </p>
    <p>
     A atividade da rede descrita na seção 2.3 pode ser visualizada com um gráfico de rastreio como o da
     <a class="open-asset-modal" data-target="#ModalFigf8" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 8
     </a>
     . Neste gráfico, o eixo vertical representa os neurônios, indexados de 1 a
     <math>
      <mi>
       N
      </mi>
     </math>
     , e o eixo horizontal representa o tempo. Os disparos dos neurônios são representados por barras amarelas. Assim, cada linha horizontal no diagrama representa o trem de disparos de um neurônio de forma visual.
    </p>
    <div class="row fig" id="f8">
     <a name="f8">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf8" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 8
      </strong>
      <br/>
      Gráfico de rastreio dos trens de disparo da rede de autômatos. Cada linha horizontal representa o trem de disparos de um neurônio, onde cada traço amarelo corresponde a um disparo.
      <br/>
     </div>
    </div>
    <p>
     Utilizando o método apresentado na seção 4.1, foi possível recuperar a matriz de adjacência
     <math>
      <msup>
       <mi>
        M
       </mi>
       <mtext>
        adj
       </mtext>
      </msup>
     </math>
     original da rede. A
     <a class="open-asset-modal" data-target="#ModalFigf9" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 9
     </a>
     mostra a matriz original (
     <a class="open-asset-modal" data-target="#ModalFigf9" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 9A
     </a>
     ) ao lado da matriz recuperada (
     <a class="open-asset-modal" data-target="#ModalFigf9" data-toggle="modal" href="">
      <span class="sci-ico-fileFigure">
      </span>
      figura 9B
     </a>
     ). Observe que a segunda matriz apresenta os valores da informação mútua entre pares de neurônio pré- e pós-sinápticos.
    </p>
    <div class="row fig" id="f9">
     <a name="f9">
     </a>
     <div class="col-md-4 col-sm-4">
      <a data-target="#ModalFigf9" data-toggle="modal" href="">
       <div class="thumbOff">
        <div class="zoom">
         <span class="sci-ico-zoom">
         </span>
        </div>
       </div>
      </a>
     </div>
     <div class="col-md-8 col-sm-8">
      <strong>
       Figura 9
      </strong>
      <br/>
      Aplicação do cálculo da informação mútua para neurônios binários para a recuperação da matriz de adjacência original da rede. Em (A) é mostrada a matriz de adjacência original, onde cada traço amarelo é o valor de
      <math>
       <msub>
        <mi>
         G
        </mi>
        <mtext>
         e
        </mtext>
       </msub>
      </math>
      , como descrito na seção 2.3. Em (B) mostra-se os valores de informação mútua encontrados. As cores indicam a intensidade da
      <math>
       <mrow>
        <mi>
         M
        </mi>
        <mi>
         I
        </mi>
        <mo>
         (
        </mo>
        <mi>
         x
        </mi>
        <mo>
         ;
        </mo>
        <mi>
         y
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      . Nota-se que nos pontos onde existe conexão a
      <math>
       <mrow>
        <mi>
         M
        </mi>
        <mi>
         I
        </mi>
        <mo>
         (
        </mo>
        <mi>
         x
        </mi>
        <mo>
         ;
        </mo>
        <mi>
         y
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </math>
      é alta permitindo que
      <math>
       <msup>
        <mi>
         M
        </mi>
        <mtext>
         adj
        </mtext>
       </msup>
      </math>
      seja estimada.
      <br/>
     </div>
    </div>
    <p>
     Por intermédio da matriz de informação mútua é possível estimar
     <math>
      <msup>
       <mi>
        M
       </mi>
       <mtext>
        adj
       </mtext>
      </msup>
     </math>
     . Isto é feito estabelecendo-se um valor de corte
     <math>
      <mi>
       L
      </mi>
     </math>
     , e em seguida, atribuindo-se o valor 1 nas posições da matriz estimada
     <math>
      <msubsup>
       <mi>
        M
       </mi>
       <mtext>
        est
       </mtext>
       <mtext>
        adj
       </mtext>
      </msubsup>
     </math>
     em que os valores da matriz de informação mútua
     <math>
      <mrow>
       <mi>
        M
       </mi>
       <mi>
        I
       </mi>
       <mo>
        (
       </mo>
       <mi>
        x
       </mi>
       <mo>
        ;
       </mo>
       <mi>
        y
       </mi>
       <mo>
        )
       </mo>
      </mrow>
     </math>
     ultrapassarem
     <math>
      <mi>
       L
      </mi>
     </math>
     . Senão, atribui-se o valor 0. É importante ressaltar que neste caso utilizou-se
     <math>
      <mrow>
       <mi>
        τ
       </mi>
       <mo>
        =
       </mo>
       <mn>
        1
       </mn>
      </mrow>
     </math>
     , que é igual ao valor do passo de tempo
     <math>
      <mrow>
       <mi>
        Δ
       </mi>
       <mi>
        t
       </mi>
      </mrow>
     </math>
     utilizado, e como a probabilidade de disparo de um dado neurônio depende do estado dos neurônios pré-sinápticos no passo anterior, esse é o tempo que leva para a informação viajar de um neurônio ao outro.
    </p>
    <h1 class="articleSectionTitle">
     6. Conclusão
    </h1>
    <p>
     Os métodos apresentados neste texto ilustram maneiras de se aplicar ferramentas da teoria da informação a trens de disparos. Em particular, mostrou-se como determinar a informação para trens de disparos de neurônios Poisson e LIF estocástico utilizando o método direto e o método baseado na função de coerência. O segundo método não está sujeito ao problema da dimensionalidade, como o primeiro, e permite maior eficiência em aplicações práticas, embora esteja restrito ao caso de sinais gaussianos.
    </p>
    <p>
     Também foi mostrado como realizar o calculo da entropia para neurônios binários e da informação mútua entre dois trens de disparos gerados por esse tipo de neurônio. Como aplicação final desse método mostrou-se que é possível utilizá-lo para recuperar a conectividade de uma rede simples de autômatos celulares.
    </p>
    <p>
     Esses métodos ilustram a aplicabilidade das ferramentas da teoria da informação em neurociência. Além deles, medidas desenvolvidas recentemente como, por exemplo, as chamadas “entropia transferida”
     <span class="ref">
      <strong class="xref xrefblue">
       [36]
      </strong>
      <span class="refCtt closed">
       <span>
        [36] T. Schreiber, Phys. Rev. Lett. 85, 461 (2000).
       </span>
      </span>
     </span>
     e “campo de dinâmica da informação”
     <span class="ref">
      <strong class="xref xrefblue">
       [37]
      </strong>
      <span class="refCtt closed">
       <span>
        [37] M. Wibral, J.T. Lizier e V. Priesemann, arXiv:1412.0291 (2014).
       </span>
      </span>
     </span>
     , propõem novas grandezas que permitem determinar como a informação flui, é armazenada e é transformada pelos disparos dos neurônios de uma rede neural. Outra medida recente, chamada “informação mútua causal”, aperfeiçoa a metodologia utilizada aqui para não só inferir a topologia da rede, mas também a direção e a natureza das sinapses
     <span class="ref">
      <strong class="xref xrefblue">
       [38]
      </strong>
      <span class="refCtt closed">
       <span>
        [38] F.S. Borges, E.L. Lameu, K.C. Iarosz, P.R. Protachevicz, I.L. Caldas, R.L. Viana, E.E.N. Macau, A.M. Batista e M.D.S. Baptista, Phys. Rev. E. 97, 022303 (2018).
       </span>
      </span>
     </span>
     <span class="ref">
      <sup class="xref xrefblue">
      </sup>
      <span class="refCtt closed">
       <span>
        [39] M. Michele e M. Rosanna, Plos One 5, e36867 (2012).
       </span>
      </span>
     </span>
     .
    </p>
    <p>
     De modo geral, o presente artigo apresentou as grandezas fundamentais da teoria da informação para que o leitor interessado em se aprofundar tenha mais facilidade ao adentrar no estado da arte no campo da teoria da informação aplicada à neurociência.
    </p>
   </div>
   <div class="articleSection" data-anchor="Material suplementar">
    <h1 class="articleSectionTitle">
     Material suplementar
    </h1>
    <p>
     O seguinte material suplementar está disponível online:
    </p>
    <a href="https://minio.scielo.br/documentstore/1806-9126/WRdxNxhKsZXKR3HBY9Jzfks/c655373b7312f49969bb8666dd4b4431aa114375.pdf" target="_blank">
     Apêndice A.
    </a>
   </div>
   <div class="articleSection" data-anchor="Agradecimentos">
    <h1 class="articleSectionTitle">
     Agradecimentos
    </h1>
    <p>
     Esta pesquisa foi desenvolvida como parte das atividades do Centro de Pesquisa, Inovação e Difusão em Neuromatemática (CEPID NeuroMat) da Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP), proc. no. 2013/07699-0. VL recebe uma bolsa de mestrado da FAPESP, proc. no. 2017/05874-0. RFOP recebe uma bolsa de doutorado da FAPESP, proc. no. 2013/25667-8. CCC recebeu bolsa de doutorado da Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES). ROS recebe uma bolsa de doutorado da FAPESP, proc. no. 2017/07688-9. RFOP e ACR participam do Projeto Temático FAPESP No. 2015/50122-0. ACR recebe uma bolsa de pesquisador do Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq), proc. no. 306251/2014-0.
    </p>
   </div>
   <div class="articleSection">
    <div class="ref-list">
     <ul class="refList footnote">
      <li>
       <span class="xref big">
        1
       </span>
       <div>
        O primeiro a notar isso foi o neurofisiologista inglês Lord Adrian
        <span class="ref">
         <strong class="xref xrefblue">
          [1]
         </strong>
         <span class="refCtt closed">
          <span>
           [1] E.D. Adrian, J. Physiol. 47, 460 (1914).
          </span>
         </span>
        </span>
        .
       </div>
      </li>
      <li>
       <span class="xref big">
        2
       </span>
       <div>
        Alguns autores criticam a concepção de código neural. Ver, por exemplo,
        <span class="ref">
         <strong class="xref xrefblue">
          [3]
         </strong>
         <span class="refCtt closed">
          <span>
           [3] R. Brette, bioRxiv:168237 (2017).
          </span>
         </span>
        </span>
        .
       </div>
      </li>
      <li>
       <span class="xref big">
        3
       </span>
       <div>
        Uma versão integral desse artigo está disponível na url
        <a href="http://math.harvard.edu/%24%5Csim%24ctm/home/text/others/shannon/entropy/entropy.pdf" target="_blank">
         http://math.harvard.edu/$\sim$ctm/home/text/others/shannon/entropy/entropy.pdf
        </a>
       </div>
      </li>
      <li>
       <span class="xref big">
        4
       </span>
       <div>
        É dito que
        <i>
         X
        </i>
        é a entrada do canal e
        <i>
         Y
        </i>
        a saída.
       </div>
      </li>
      <li>
       <span class="xref big">
        5
       </span>
       <div>
        A entropia condicional é igual à entropia do ruído
        <math>
         <mrow>
          <mi>
           H
          </mi>
          <mo>
           (
          </mo>
          <mi>
           X
          </mi>
          <mo>
           |
          </mo>
          <mi>
           Y
          </mi>
          <mo>
           )
          </mo>
          <mo>
           =
          </mo>
          <mi>
           H
          </mi>
          <mo>
           (
          </mo>
          <mi>
           η
          </mi>
          <mo>
           )
          </mo>
         </mrow>
        </math>
        <span class="ref">
         <strong class="xref xrefblue">
          [25]
         </strong>
         <span class="refCtt closed">
          <span>
           [25] J.V. Stone, Information theory: a tutorial introduction (Sebtel Press, Sheffield, 2015), v. 2
          </span>
         </span>
        </span>
        .
       </div>
      </li>
     </ul>
    </div>
   </div>
   <div class="articleSection" data-anchor="Referências bibliográficas">
    <div class="ref-list">
     <ul class="refList">
      <li>
       <sup class="xref big">
        [1]
       </sup>
       <div>
        E.D. Adrian, J. Physiol.
        <b>
         47
        </b>
        , 460 (1914).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [2]
       </sup>
       <div>
        W. Gerstner, W.M. Kistler, R. Naud e L. Paninski,
        <i>
         Neuronal dynamics: From single neurons to networks and models of cognition
        </i>
        (Cambridge University Press, Cambridge, 2014).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [3]
       </sup>
       <div>
        R. Brette, bioRxiv:168237 (2017).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [4]
       </sup>
       <div>
        P. König, K.E. Andreas e W. Singer, Trends neurosci.
        <b>
         19
        </b>
        , 130 (1996).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [5]
       </sup>
       <div>
        B.B. Averbeck, E.L. Peter e A. Pouget, Nat. Rev. Neurosci.
        <b>
         7
        </b>
        , 358 (2006).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [6]
       </sup>
       <div>
        W. Bialek, F. Rieke, R.D.R. Van Steveninck e D. Warland, Science
        <b>
         252
        </b>
        , 1854 (1991).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [7]
       </sup>
       <div>
        J.J. Eggermont, Neurosci. Biobehav. R.
        <b>
         22
        </b>
        , 355 (1998).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [8]
       </sup>
       <div>
        F. Rieke, D. Warland, R.D.R. Van Steveninck e W. Bialek,
        <i>
         Spikes: exploring the neural code
        </i>
        (MIT press, Cambridge, 1999).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [9]
       </sup>
       <div>
        L. Paninski, Neural. Comput.
        <b>
         15
        </b>
        , 1191 (2003).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [10]
       </sup>
       <div>
        C. Laing e J.L. Gabriel,
        <i>
         Stochastic methods in neuroscience
        </i>
        (Oxford University Press, Oxford, 2010).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [11]
       </sup>
       <div>
        B. Naundorf, F. Wolf e M. Volgushev, Nature
        <b>
         440
        </b>
        , 1060 (2006).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [12]
       </sup>
       <div>
        F. David e S. Nelson, Science
        <b>
         270
        </b>
        , 756 (1995).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [13]
       </sup>
       <div>
        D.J. Foster e M.A. Wilson, Nature
        <b>
         440
        </b>
        , 680 (2006).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [14]
       </sup>
       <div>
        M.R. Mehta, A.K. Lee e M.A. Wilson, Nature
        <b>
         417
        </b>
        , 741 (2002).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [15]
       </sup>
       <div>
        C. Passaglia, F. Dodge, E. Herzog, S.Jackson e R. Barlow, P. Natl. Acad. Sci.
        <b>
         94
        </b>
        , 12649 (1997).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [16]
       </sup>
       <div>
        V.E. Abraira e D.D. Ginty, Neuron
        <b>
         79
        </b>
        , 618 (2013).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [17]
       </sup>
       <div>
        P. Dayan e L.F. Abbott,
        <i>
         Theoretical neuroscience
        </i>
        (MIT Press, Cambridge, 2001).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [18]
       </sup>
       <div>
        J. O'Keefe e M.L. Recce, Hippocampus.
        <b>
         3
        </b>
        , 317 (1993).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [19]
       </sup>
       <div>
        D. Bernardi e B. Lindner, J. Neurophysiol.
        <b>
         113
        </b>
        , 1342 (2015).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [20]
       </sup>
       <div>
        D.J. Higham, Siam. Rev.
        <b>
         43
        </b>
        , 525 (2001).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [21]
       </sup>
       <div>
        C.E. Shannon, Bell. Syst. Tech. J.
        <b>
         27
        </b>
        , 623 (1948).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [22]
       </sup>
       <div>
        J.R. Pierce,
        <i>
         An introduction to information theory: symbols, signals and noise
        </i>
        (Courier Corporation, Mineola, 2012).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [23]
       </sup>
       <div>
        S. Haykin,
        <i>
         Redes Neurais: Princípios e prática
        </i>
        (Artmed editora, São Paulo, 2008).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [24]
       </sup>
       <div>
        D.J.C. MacKay,
        <i>
         Information theory, inference and learning algorithms
        </i>
        (Cambridge University Press, Cambridge, 2003), v. 4
       </div>
      </li>
      <li>
       <sup class="xref big">
        [25]
       </sup>
       <div>
        J.V. Stone,
        <i>
         Information theory: a tutorial introduction
        </i>
        (Sebtel Press, Sheffield, 2015), v. 2
       </div>
      </li>
      <li>
       <sup class="xref big">
        [26]
       </sup>
       <div>
        F. Attneave, Psychol. Rev.
        <b>
         61
        </b>
        , 183 (1954).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [27]
       </sup>
       <div>
        H.B. Barlow,
        <i>
         Current problems in animal behaviour
        </i>
        (Cambridge University Press, Cambridge, 1961).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [28]
       </sup>
       <div>
        A.G. Dimitrov, A.A Lazar e J.D. Victor, J. Comput. Neurosci.
        <b>
         30
        </b>
        , 1 (2011).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [29]
       </sup>
       <div>
        D. MacKay e W.S. McCulloch, Bull. Math. Biophys.
        <b>
         14
        </b>
        , 127 (1952).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [30]
       </sup>
       <div>
        S.P. Strong, R. Koberle, R.R. de Ruyter van Steveninck, W. Bialek, Phys. Rev. Lett.
        <b>
         80
        </b>
        , 197 (1998).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [31]
       </sup>
       <div>
        J.G. Orlandi, O. Stetter, J. Soriano, T. Geisel e D. Battaglia, Plos One
        <b>
         9
        </b>
        , e98842 (2014).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [32]
       </sup>
       <div>
        S.A. Neymotin, K.M. Jacobs, A.A. Fenton e W.W. Lytton, J. Comput. Neurosci.
        <b>
         30
        </b>
        , 69 (2011)
       </div>
      </li>
      <li>
       <sup class="xref big">
        [33]
       </sup>
       <div>
        M. Wibral, B. Rahm, M. Rieder, M. Lindner, R. Vicente e J. Kaiser, Prog. Biophys. Mol. Bio.
        <b>
         105
        </b>
        , 80 (2011).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [34]
       </sup>
       <div>
        W. Liao, J. Ding, D. Marinazzo, Q. Xu, Z. Wang, C. Yuan, Z. Zhang, G. Lu e H.Chen, Neuroimage
        <b>
         54
        </b>
        , 2683 (2011).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [35]
       </sup>
       <div>
        A. Borst e F.E. Theunissen, Nat. Neurosci.
        <b>
         2
        </b>
        , 947 (1999).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [36]
       </sup>
       <div>
        T. Schreiber, Phys. Rev. Lett.
        <b>
         85
        </b>
        , 461 (2000).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [37]
       </sup>
       <div>
        M. Wibral, J.T. Lizier e V. Priesemann, arXiv:1412.0291 (2014).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [38]
       </sup>
       <div>
        F.S. Borges, E.L. Lameu, K.C. Iarosz, P.R. Protachevicz, I.L. Caldas, R.L. Viana, E.E.N. Macau, A.M. Batista e M.D.S. Baptista, Phys. Rev. E.
        <b>
         97
        </b>
        , 022303 (2018).
       </div>
      </li>
      <li>
       <sup class="xref big">
        [39]
       </sup>
       <div>
        M. Michele e M. Rosanna, Plos One
        <b>
         5
        </b>
        , e36867 (2012).
       </div>
      </li>
     </ul>
    </div>
   </div>
   <div class="articleSection" data-anchor="Disponibilidade de dados">
    <h1 class="articleSectionTitle">
     Disponibilidade de dados
    </h1>
   </div>
   <h2>
    Citações de dados
   </h2>
   <div class="row">
    <div class="col-md-12 col-sm-12">
     <p>
      R. Brette, bioRxiv:168237 (2017).
     </p>
    </div>
   </div>
   <div class="row">
    <div class="col-md-12 col-sm-12">
     <p>
      M. Wibral, J.T. Lizier e V. Priesemann, arXiv:1412.0291 (2014).
     </p>
    </div>
   </div>
   <div class="articleSection" data-anchor="Datas de Publicação ">
    <h1 class="articleSectionTitle">
     Datas de Publicação
    </h1>
    <div class="row">
     <div class="col-md-12 col-sm-12">
      <ul class="articleTimeline">
       <li>
        <strong>
         Publicação nesta coleção
        </strong>
        <br/>
        2019
       </li>
      </ul>
     </div>
    </div>
   </div>
   <div class="articleSection" data-anchor="Histórico">
    <h1 class="articleSectionTitle">
     Histórico
    </h1>
    <div class="row">
     <div class="col-md-12 col-sm-12">
      <ul class="articleTimeline">
       <li>
        <strong>
         Recebido
        </strong>
        <br/>
        03 Jul 2018
       </li>
       <li>
        <strong>
         Revisado
        </strong>
        <br/>
        10 Set 2018
       </li>
       <li>
        <strong>
         Aceito
        </strong>
        <br/>
        15 Set 2018
       </li>
      </ul>
     </div>
    </div>
   </div>
   <section class="documentLicense">
    <div class="container-license">
     <div class="row">
      <div class="col-sm-3 col-md-2">
       <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" title="">
        <img alt="Creative Common - by 4.0 " src="https://licensebuttons.net/l/by/4.0//88x31.png"/>
       </a>
      </div>
      <div class="col-sm-9 col-md-10">
       <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" title="">
        License information: This is an open-access article distributed under the terms of the Creative Commons Attribution License (type CC-BY), which permits unrestricted use, distribution and reproduction in any medium, provided the original article is properly cited.
       </a>
      </div>
     </div>
    </div>
   </section>
  </article>
 </div>
</div>